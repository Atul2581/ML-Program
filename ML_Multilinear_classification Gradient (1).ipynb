{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "f= open(\"E:/R/multinomila_patients_3label.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = f.readlines()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa,20,25,5.2,low\\n', 'bb,22,56,6.1,high\\n', 'cc,24,45,6.0,moderat\\n', 'dd,25,35,4.7,low\\n', 'ee,23,45,3.5,low\\n', 'ff,22,55,6.0,high\\n', 'gg,34,74,5.5,high\\n', 'hh,43,80,5.9,moderat\\n', 'ii,72,43,6.3,moderat\\n', 'jj,43,54,6.6,high\\n', 'kk,74,55,5.5,low\\n', 'll,76,54,4.5,low']\n"
     ]
    }
   ],
   "source": [
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "label=[]\n",
    "for line in lines:\n",
    "    w = line.strip().lower().split(\",\")\n",
    "    ins = [float(v) for v in w[1:-1]]\n",
    "    x.append(ins)\n",
    "    lab = w[-1]\n",
    "    label.append(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low\n"
     ]
    }
   ],
   "source": [
    "print(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low', 'high', 'moderat', 'low', 'low', 'high', 'high', 'moderat', 'moderat', 'high', 'low', 'low']\n"
     ]
    }
   ],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20.  25.   5.2]\n",
      " [22.  56.   6.1]\n",
      " [24.  45.   6. ]\n",
      " [25.  35.   4.7]\n",
      " [23.  45.   3.5]\n",
      " [22.  55.   6. ]\n",
      " [34.  74.   5.5]\n",
      " [43.  80.   5.9]\n",
      " [72.  43.   6.3]\n",
      " [43.  54.   6.6]\n",
      " [74.  55.   5.5]\n",
      " [76.  54.   4.5]]\n"
     ]
    }
   ],
   "source": [
    "print(np.c_[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low', 'high', 'moderat', 'low', 'low', 'high', 'high', 'moderat', 'moderat', 'high', 'low', 'low']\n"
     ]
    }
   ],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low', 'high', 'moderat', 'low', 'low', 'high', 'high', 'moderat', 'moderat', 'high', 'low', 'low']\n"
     ]
    }
   ],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assigning uniqe numerical label for string label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d ={label:i for i,label in enumerate(list(set(label)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'low': 0, 'moderat': 1, 'high': 2}\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert all string label into numeric label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [d[l] for l in label]                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 1, 0, 0, 2, 2, 1, 1, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform numeric label to into binarray array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[]\n",
    "for i in y:\n",
    "    barr = np.zeros(len(set(y)))\n",
    "    barr[i]=1\n",
    "    b.append(barr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jhfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dfdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 0., 0.]), array([0., 0., 1.]), array([0., 1., 0.]), array([1., 0., 0.]), array([1., 0., 0.]), array([0., 0., 1.]), array([0., 0., 1.]), array([0., 1., 0.]), array([0., 1., 0.]), array([0., 0., 1.]), array([1., 0., 0.]), array([1., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.c_[b]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 1, 0, 0, 2, 2, 1, 1, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y= np.array(b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "Y = np.c_[Y]\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sd(x):\n",
    "    num=((x-x.mean())**2).sum()\n",
    "    den = x.size-1\n",
    "    return (num/den)**0.5\n",
    "\n",
    "\n",
    "def scale(x):\n",
    "    return (x-x.mean())/sd(x)\n",
    "\n",
    "\n",
    "def scaleMatrix(x):\n",
    "    \n",
    "    for col in range(x.shape[1]):\n",
    "        x[:,col]= scale(x[:,col])\n",
    "    ones = np.ones(x.shape[0])\n",
    "    return np.c_[ones,x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20.  25.   5.2]\n",
      " [22.  56.   6.1]\n",
      " [24.  45.   6. ]\n",
      " [25.  35.   4.7]\n",
      " [23.  45.   3.5]\n",
      " [22.  55.   6. ]\n",
      " [34.  74.   5.5]\n",
      " [43.  80.   5.9]\n",
      " [72.  43.   6.3]\n",
      " [43.  54.   6.6]\n",
      " [74.  55.   5.5]\n",
      " [76.  54.   4.5]]\n"
     ]
    }
   ],
   "source": [
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = scaleMatrix(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -0.89951806 -1.77218042 -0.32050069]\n",
      " [ 1.         -0.80881036  0.28156138  0.69756033]\n",
      " [ 1.         -0.71810266 -0.44718571  0.58444244]\n",
      " [ 1.         -0.6727488  -1.10968307 -0.88609015]\n",
      " [ 1.         -0.76345651 -0.44718571 -2.24350484]\n",
      " [ 1.         -0.80881036  0.21531164  0.58444244]\n",
      " [ 1.         -0.26456414  1.47405661  0.01885298]\n",
      " [ 1.          0.14362053  1.87155502  0.47132455]\n",
      " [ 1.          1.45888224 -0.57968518  0.92379611]\n",
      " [ 1.          0.14362053  0.1490619   1.26314978]\n",
      " [ 1.          1.54958994  0.21531164  0.01885298]\n",
      " [ 1.          1.64029765  0.1490619  -1.11232593]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def losss(y,ycap):\n",
    "    return (y*np.log(ycap)).sum()*-1\n",
    "def loss(y,ycap):\n",
    "    return ((y-ycap)**2).mean()\n",
    "def derivative(x):\n",
    "    return x*(1-x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03279726  0.14133517 -0.94305155]\n",
      " [-0.65695669  0.37055396  0.66779373]\n",
      " [-0.38606756  0.78722616  0.44308772]\n",
      " [-0.62012209  0.10845518 -0.29573609]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(101)\n",
    "W = 2*np.random.random((X.shape[1],Y.shape[1]))-1\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current loss at 1 iteration 0.249441503801451\n",
      "current loss at 11 iteration 0.09355007049573877\n",
      "current loss at 21 iteration 0.08889631552491113\n",
      "current loss at 31 iteration 0.08685497001497629\n",
      "current loss at 41 iteration 0.08569155699430445\n",
      "current loss at 51 iteration 0.08493561320311611\n",
      "current loss at 61 iteration 0.08440236290101737\n",
      "current loss at 71 iteration 0.08400388834377\n",
      "current loss at 81 iteration 0.08369278225607733\n",
      "current loss at 91 iteration 0.08344101519319586\n",
      "current loss at 101 iteration 0.08323071260457399\n",
      "current loss at 111 iteration 0.08304963786759251\n",
      "current loss at 121 iteration 0.08288874418584918\n",
      "current loss at 131 iteration 0.08274069896128519\n",
      "current loss at 141 iteration 0.0825988773290763\n",
      "current loss at 151 iteration 0.08245658528618746\n",
      "current loss at 161 iteration 0.08230642916651972\n",
      "current loss at 171 iteration 0.08213989606908227\n",
      "current loss at 181 iteration 0.08194738283834822\n",
      "current loss at 191 iteration 0.08171903788891012\n",
      "current loss at 201 iteration 0.08144663357183801\n",
      "current loss at 211 iteration 0.08112604985513722\n",
      "current loss at 221 iteration 0.08075912494981276\n",
      "current loss at 231 iteration 0.08035357645513516\n",
      "current loss at 241 iteration 0.07992087993244429\n",
      "current loss at 251 iteration 0.07947333787510961\n",
      "current loss at 261 iteration 0.07902176195563555\n",
      "current loss at 271 iteration 0.0785743733959205\n",
      "current loss at 281 iteration 0.07813672033934937\n",
      "current loss at 291 iteration 0.07771212927696167\n",
      "current loss at 301 iteration 0.07730230104974416\n",
      "current loss at 311 iteration 0.07690784767412129\n",
      "current loss at 321 iteration 0.07652870304759075\n",
      "current loss at 331 iteration 0.0761644093323628\n",
      "current loss at 341 iteration 0.07581430469535291\n",
      "current loss at 351 iteration 0.07547764056016501\n",
      "current loss at 361 iteration 0.0751536514753874\n",
      "current loss at 371 iteration 0.07484159440937029\n",
      "current loss at 381 iteration 0.0745407689469423\n",
      "current loss at 391 iteration 0.0742505259253979\n",
      "current loss at 401 iteration 0.07397026933050083\n",
      "current loss at 411 iteration 0.07369945446946682\n",
      "current loss at 421 iteration 0.07343758426851593\n",
      "current loss at 431 iteration 0.0731842047973793\n",
      "current loss at 441 iteration 0.07293890065519454\n",
      "current loss at 451 iteration 0.07270129056284781\n",
      "current loss at 461 iteration 0.07247102333119199\n",
      "current loss at 471 iteration 0.07224777427066939\n",
      "current loss at 481 iteration 0.07203124204869354\n",
      "current loss at 491 iteration 0.07182114596947967\n",
      "current loss at 501 iteration 0.0716172236359535\n",
      "current loss at 511 iteration 0.07141922894806008\n",
      "current loss at 521 iteration 0.07122693039198504\n",
      "current loss at 531 iteration 0.07104010957787729\n",
      "current loss at 541 iteration 0.07085855998805915\n",
      "current loss at 551 iteration 0.07068208590251367\n",
      "current loss at 561 iteration 0.07051050147313803\n",
      "current loss at 571 iteration 0.07034362992258739\n",
      "current loss at 581 iteration 0.07018130284738969\n",
      "current loss at 591 iteration 0.07002335960836127\n",
      "current loss at 601 iteration 0.06986964679421341\n",
      "current loss at 611 iteration 0.0697200177466561\n",
      "current loss at 621 iteration 0.06957433213732452\n",
      "current loss at 631 iteration 0.0694324555885349\n",
      "current loss at 641 iteration 0.06929425933126643\n",
      "current loss at 651 iteration 0.06915961989491144\n",
      "current loss at 661 iteration 0.06902841882428143\n",
      "current loss at 671 iteration 0.06890054242012762\n",
      "current loss at 681 iteration 0.06877588150007297\n",
      "current loss at 691 iteration 0.06865433117737152\n",
      "current loss at 701 iteration 0.0685357906553383\n",
      "current loss at 711 iteration 0.0684201630356433\n",
      "current loss at 721 iteration 0.06830735513894984\n",
      "current loss at 731 iteration 0.06819727733661392\n",
      "current loss at 741 iteration 0.06808984339235483\n",
      "current loss at 751 iteration 0.06798497031296732\n",
      "current loss at 761 iteration 0.0678825782072777\n",
      "current loss at 771 iteration 0.06778259015265514\n",
      "current loss at 781 iteration 0.06768493206847981\n",
      "current loss at 791 iteration 0.06758953259604566\n",
      "current loss at 801 iteration 0.06749632298443786\n",
      "current loss at 811 iteration 0.06740523698197724\n",
      "current loss at 821 iteration 0.06731621073287013\n",
      "current loss at 831 iteration 0.06722918267873723\n",
      "current loss at 841 iteration 0.06714409346472934\n",
      "current loss at 851 iteration 0.06706088584996246\n",
      "current loss at 861 iteration 0.06697950462203082\n",
      "current loss at 871 iteration 0.06689989651537497\n",
      "current loss at 881 iteration 0.06682201013329915\n",
      "current loss at 891 iteration 0.06674579587344961\n",
      "current loss at 901 iteration 0.06667120585657696\n",
      "current loss at 911 iteration 0.06659819385841868\n",
      "current loss at 921 iteration 0.06652671524454869\n",
      "current loss at 931 iteration 0.06645672690804959\n",
      "current loss at 941 iteration 0.06638818720987343\n",
      "current loss at 951 iteration 0.06632105592176238\n",
      "current loss at 961 iteration 0.06625529417161097\n",
      "current loss at 971 iteration 0.06619086439115444\n",
      "current loss at 981 iteration 0.06612773026587786\n",
      "current loss at 991 iteration 0.06606585668704229\n",
      "current loss at 1001 iteration 0.06600520970573318\n",
      "current loss at 1011 iteration 0.06594575648883792\n",
      "current loss at 1021 iteration 0.06588746527686629\n",
      "current loss at 1031 iteration 0.06583030534353068\n",
      "current loss at 1041 iteration 0.06577424695700725\n",
      "current loss at 1051 iteration 0.06571926134280298\n",
      "current loss at 1061 iteration 0.06566532064815744\n",
      "current loss at 1071 iteration 0.06561239790791065\n",
      "current loss at 1081 iteration 0.0655604670117727\n",
      "current loss at 1091 iteration 0.06550950267293307\n",
      "current loss at 1101 iteration 0.06545948039795076\n",
      "current loss at 1111 iteration 0.06541037645786879\n",
      "current loss at 1121 iteration 0.06536216786049973\n",
      "current loss at 1131 iteration 0.06531483232383119\n",
      "current loss at 1141 iteration 0.0652683482505017\n",
      "current loss at 1151 iteration 0.06522269470330182\n",
      "current loss at 1161 iteration 0.06517785138165433\n",
      "current loss at 1171 iteration 0.06513379859903273\n",
      "current loss at 1181 iteration 0.06509051726127629\n",
      "current loss at 1191 iteration 0.06504798884576385\n",
      "current loss at 1201 iteration 0.0650061953814091\n",
      "current loss at 1211 iteration 0.06496511942944211\n",
      "current loss at 1221 iteration 0.06492474406494371\n",
      "current loss at 1231 iteration 0.06488505285910062\n",
      "current loss at 1241 iteration 0.06484602986215024\n",
      "current loss at 1251 iteration 0.06480765958698656\n",
      "current loss at 1261 iteration 0.06476992699339863\n",
      "current loss at 1271 iteration 0.06473281747291508\n",
      "current loss at 1281 iteration 0.0646963168342295\n",
      "current loss at 1291 iteration 0.06466041128918164\n",
      "current loss at 1301 iteration 0.064625087439272\n",
      "current loss at 1311 iteration 0.0645903322626869\n",
      "current loss at 1321 iteration 0.06455613310181305\n",
      "current loss at 1331 iteration 0.06452247765122146\n",
      "current loss at 1341 iteration 0.06448935394610109\n",
      "current loss at 1351 iteration 0.06445675035112376\n",
      "current loss at 1361 iteration 0.0644246555497226\n",
      "current loss at 1371 iteration 0.06439305853376708\n",
      "current loss at 1381 iteration 0.06436194859361871\n",
      "current loss at 1391 iteration 0.0643313153085514\n",
      "current loss at 1401 iteration 0.06430114853752197\n",
      "current loss at 1411 iteration 0.06427143841027702\n",
      "current loss at 1421 iteration 0.06424217531878174\n",
      "current loss at 1431 iteration 0.06421334990895863\n",
      "current loss at 1441 iteration 0.06418495307272315\n",
      "current loss at 1451 iteration 0.0641569759403046\n",
      "current loss at 1461 iteration 0.06412940987284116\n",
      "current loss at 1471 iteration 0.06410224645523771\n",
      "current loss at 1481 iteration 0.0640754774892766\n",
      "current loss at 1491 iteration 0.06404909498697088\n",
      "current loss at 1501 iteration 0.06402309116415114\n",
      "current loss at 1511 iteration 0.06399745843427618\n",
      "current loss at 1521 iteration 0.06397218940245913\n",
      "current loss at 1531 iteration 0.06394727685970075\n",
      "current loss at 1541 iteration 0.06392271377732167\n",
      "current loss at 1551 iteration 0.06389849330158603\n",
      "current loss at 1561 iteration 0.06387460874850912\n",
      "current loss at 1571 iteration 0.06385105359884216\n",
      "current loss at 1581 iteration 0.06382782149322722\n",
      "current loss at 1591 iteration 0.06380490622751582\n",
      "current loss at 1601 iteration 0.0637823017482456\n",
      "current loss at 1611 iteration 0.0637600021482681\n",
      "current loss at 1621 iteration 0.0637380016625228\n",
      "current loss at 1631 iteration 0.0637162946639518\n",
      "current loss at 1641 iteration 0.06369487565954952\n",
      "current loss at 1651 iteration 0.0636737392865428\n",
      "current loss at 1661 iteration 0.06365288030869641\n",
      "current loss at 1671 iteration 0.06363229361273956\n",
      "current loss at 1681 iteration 0.06361197420490843\n",
      "current loss at 1691 iteration 0.06359191720760123\n",
      "current loss at 1701 iteration 0.06357211785614104\n",
      "current loss at 1711 iteration 0.06355257149564303\n",
      "current loss at 1721 iteration 0.0635332735779817\n",
      "current loss at 1731 iteration 0.0635142196588553\n",
      "current loss at 1741 iteration 0.06349540539494303\n",
      "current loss at 1751 iteration 0.06347682654115273\n",
      "current loss at 1761 iteration 0.06345847894795492\n",
      "current loss at 1771 iteration 0.06344035855880062\n",
      "current loss at 1781 iteration 0.06342246140761987\n",
      "current loss at 1791 iteration 0.06340478361639827\n",
      "current loss at 1801 iteration 0.06338732139282836\n",
      "current loss at 1811 iteration 0.06337007102803374\n",
      "current loss at 1821 iteration 0.06335302889436313\n",
      "current loss at 1831 iteration 0.06333619144325191\n",
      "current loss at 1841 iteration 0.06331955520314907\n",
      "current loss at 1851 iteration 0.06330311677750672\n",
      "current loss at 1861 iteration 0.06328687284283113\n",
      "current loss at 1871 iteration 0.06327082014679175\n",
      "current loss at 1881 iteration 0.0632549555063874\n",
      "current loss at 1891 iteration 0.06323927580616731\n",
      "current loss at 1901 iteration 0.06322377799650493\n",
      "current loss at 1911 iteration 0.06320845909192292\n",
      "current loss at 1921 iteration 0.06319331616946797\n",
      "current loss at 1931 iteration 0.06317834636713276\n",
      "current loss at 1941 iteration 0.06316354688232495\n",
      "current loss at 1951 iteration 0.06314891497038021\n",
      "current loss at 1961 iteration 0.06313444794311884\n",
      "current loss at 1971 iteration 0.063120143167444\n",
      "current loss at 1981 iteration 0.06310599806398051\n",
      "current loss at 1991 iteration 0.06309201010575265\n",
      "current loss at 2001 iteration 0.06307817681689976\n",
      "current loss at 2011 iteration 0.06306449577142857\n",
      "current loss at 2021 iteration 0.063050964592001\n",
      "current loss at 2031 iteration 0.06303758094875583\n",
      "current loss at 2041 iteration 0.0630243425581643\n",
      "current loss at 2051 iteration 0.06301124718191695\n",
      "current loss at 2061 iteration 0.06299829262584243\n",
      "current loss at 2071 iteration 0.06298547673885573\n",
      "current loss at 2081 iteration 0.06297279741193594\n",
      "current loss at 2091 iteration 0.06296025257713221\n",
      "current loss at 2101 iteration 0.06294784020659702\n",
      "current loss at 2111 iteration 0.06293555831164604\n",
      "current loss at 2121 iteration 0.06292340494184345\n",
      "current loss at 2131 iteration 0.06291137818411255\n",
      "current loss at 2141 iteration 0.06289947616186997\n",
      "current loss at 2151 iteration 0.0628876970341838\n",
      "current loss at 2161 iteration 0.06287603899495388\n",
      "current loss at 2171 iteration 0.06286450027211445\n",
      "current loss at 2181 iteration 0.06285307912685781\n",
      "current loss at 2191 iteration 0.06284177385287885\n",
      "current loss at 2201 iteration 0.06283058277563944\n",
      "current loss at 2211 iteration 0.06281950425165232\n",
      "current loss at 2221 iteration 0.06280853666778381\n",
      "current loss at 2231 iteration 0.06279767844057484\n",
      "current loss at 2241 iteration 0.06278692801557959\n",
      "current loss at 2251 iteration 0.06277628386672149\n",
      "current loss at 2261 iteration 0.0627657444956658\n",
      "current loss at 2271 iteration 0.06275530843120844\n",
      "current loss at 2281 iteration 0.06274497422868058\n",
      "current loss at 2291 iteration 0.06273474046936839\n",
      "current loss at 2301 iteration 0.06272460575994766\n",
      "current loss at 2311 iteration 0.06271456873193289\n",
      "current loss at 2321 iteration 0.06270462804114026\n",
      "current loss at 2331 iteration 0.06269478236716418\n",
      "current loss at 2341 iteration 0.06268503041286703\n",
      "current loss at 2351 iteration 0.06267537090388173\n",
      "current loss at 2361 iteration 0.06266580258812665\n",
      "current loss at 2371 iteration 0.06265632423533268\n",
      "current loss at 2381 iteration 0.0626469346365822\n",
      "current loss at 2391 iteration 0.06263763260385898\n",
      "current loss at 2401 iteration 0.06262841696960969\n",
      "current loss at 2411 iteration 0.0626192865863158\n",
      "current loss at 2421 iteration 0.06261024032607626\n",
      "current loss at 2431 iteration 0.06260127708020018\n",
      "current loss at 2441 iteration 0.0625923957588092\n",
      "current loss at 2451 iteration 0.06258359529045009\n",
      "current loss at 2461 iteration 0.06257487462171601\n",
      "current loss at 2471 iteration 0.06256623271687739\n",
      "current loss at 2481 iteration 0.06255766855752136\n",
      "current loss at 2491 iteration 0.0625491811421998\n",
      "current loss at 2501 iteration 0.06254076948608608\n",
      "current loss at 2511 iteration 0.0625324326206396\n",
      "current loss at 2521 iteration 0.06252416959327822\n",
      "current loss at 2531 iteration 0.06251597946705881\n",
      "current loss at 2541 iteration 0.06250786132036476\n",
      "current loss at 2551 iteration 0.062499814246601386\n",
      "current loss at 2561 iteration 0.062491837353897864\n",
      "current loss at 2571 iteration 0.062483929764816475\n",
      "current loss at 2581 iteration 0.06247609061606845\n",
      "current loss at 2591 iteration 0.06246831905823638\n",
      "current loss at 2601 iteration 0.06246061425550301\n",
      "current loss at 2611 iteration 0.062452975385386224\n",
      "current loss at 2621 iteration 0.062445401638480114\n",
      "current loss at 2631 iteration 0.062437892218202\n",
      "current loss at 2641 iteration 0.06243044634054515\n",
      "current loss at 2651 iteration 0.06242306323383687\n",
      "current loss at 2661 iteration 0.06241574213850254\n",
      "current loss at 2671 iteration 0.06240848230683446\n",
      "current loss at 2681 iteration 0.0624012830027662\n",
      "current loss at 2691 iteration 0.062394143501651705\n",
      "current loss at 2701 iteration 0.062387063090049724\n",
      "current loss at 2711 iteration 0.0623800410655125\n",
      "current loss at 2721 iteration 0.06237307673637961\n",
      "current loss at 2731 iteration 0.062366169421575995\n",
      "current loss at 2741 iteration 0.06235931845041465\n",
      "current loss at 2751 iteration 0.0623525231624035\n",
      "current loss at 2761 iteration 0.06234578290705664\n",
      "current loss at 2771 iteration 0.06233909704370937\n",
      "current loss at 2781 iteration 0.062332464941337734\n",
      "current loss at 2791 iteration 0.0623258859783814\n",
      "current loss at 2801 iteration 0.062319359542570875\n",
      "current loss at 2811 iteration 0.062312885030757936\n",
      "current loss at 2821 iteration 0.06230646184875019\n",
      "current loss at 2831 iteration 0.06230008941114878\n",
      "current loss at 2841 iteration 0.06229376714118959\n",
      "current loss at 2851 iteration 0.06228749447058815\n",
      "current loss at 2861 iteration 0.06228127083938738\n",
      "current loss at 2871 iteration 0.062275095695808726\n",
      "current loss at 2881 iteration 0.06226896849610661\n",
      "current loss at 2891 iteration 0.06226288870442557\n",
      "current loss at 2901 iteration 0.062256855792660666\n",
      "current loss at 2911 iteration 0.062250869240320576\n",
      "current loss at 2921 iteration 0.06224492853439385\n",
      "current loss at 2931 iteration 0.06223903316921743\n",
      "current loss at 2941 iteration 0.06223318264634852\n",
      "current loss at 2951 iteration 0.06222737647443846\n",
      "current loss at 2961 iteration 0.062221614169109554\n",
      "current loss at 2971 iteration 0.06221589525283444\n",
      "current loss at 2981 iteration 0.062210219254817636\n",
      "current loss at 2991 iteration 0.062204585710879785\n",
      "current loss at 3001 iteration 0.062198994163344096\n",
      "current loss at 3011 iteration 0.06219344416092501\n",
      "current loss at 3021 iteration 0.06218793525861938\n",
      "current loss at 3031 iteration 0.062182467017599455\n",
      "current loss at 3041 iteration 0.06217703900510834\n",
      "current loss at 3051 iteration 0.06217165079435738\n",
      "current loss at 3061 iteration 0.06216630196442552\n",
      "current loss at 3071 iteration 0.06216099210016086\n",
      "current loss at 3081 iteration 0.062155720792083965\n",
      "current loss at 3091 iteration 0.062150487636293245\n",
      "current loss at 3101 iteration 0.06214529223437196\n",
      "current loss at 3111 iteration 0.062140134193297385\n",
      "current loss at 3121 iteration 0.062135013125351364\n",
      "current loss at 3131 iteration 0.062129928648033\n",
      "current loss at 3141 iteration 0.062124880383972694\n",
      "current loss at 3151 iteration 0.062119867960848035\n",
      "current loss at 3161 iteration 0.06211489101130127\n",
      "current loss at 3171 iteration 0.06210994917285847\n",
      "current loss at 3181 iteration 0.06210504208784992\n",
      "current loss at 3191 iteration 0.06210016940333255\n",
      "current loss at 3201 iteration 0.06209533077101332\n",
      "current loss at 3211 iteration 0.062090525847174415\n",
      "current loss at 3221 iteration 0.06208575429259963\n",
      "current loss at 3231 iteration 0.06208101577250247\n",
      "current loss at 3241 iteration 0.06207630995645519\n",
      "current loss at 3251 iteration 0.0620716365183195\n",
      "current loss at 3261 iteration 0.06206699513617833\n",
      "current loss at 3271 iteration 0.0620623854922692\n",
      "current loss at 3281 iteration 0.06205780727291837\n",
      "current loss at 3291 iteration 0.06205326016847663\n",
      "current loss at 3301 iteration 0.06204874387325608\n",
      "current loss at 3311 iteration 0.06204425808546813\n",
      "current loss at 3321 iteration 0.06203980250716255\n",
      "current loss at 3331 iteration 0.062035376844167885\n",
      "current loss at 3341 iteration 0.06203098080603258\n",
      "current loss at 3351 iteration 0.06202661410596772\n",
      "current loss at 3361 iteration 0.062022276460790136\n",
      "current loss at 3371 iteration 0.06201796759086715\n",
      "current loss at 3381 iteration 0.06201368722006203\n",
      "current loss at 3391 iteration 0.0620094350756804\n",
      "current loss at 3401 iteration 0.06200521088841782\n",
      "current loss at 3411 iteration 0.06200101439230816\n",
      "current loss at 3421 iteration 0.06199684532467282\n",
      "current loss at 3431 iteration 0.06199270342607114\n",
      "current loss at 3441 iteration 0.06198858844025141\n",
      "current loss at 3451 iteration 0.06198450011410285\n",
      "current loss at 3461 iteration 0.06198043819760858\n",
      "current loss at 3471 iteration 0.06197640244379913\n",
      "current loss at 3481 iteration 0.061972392608707114\n",
      "current loss at 3491 iteration 0.061968408451322426\n",
      "current loss at 3501 iteration 0.06196444973354837\n",
      "current loss at 3511 iteration 0.061960516220158475\n",
      "current loss at 3521 iteration 0.06195660767875407\n",
      "current loss at 3531 iteration 0.061952723879722824\n",
      "current loss at 3541 iteration 0.06194886459619758\n",
      "current loss at 3551 iteration 0.061945029604016315\n",
      "current loss at 3561 iteration 0.06194121868168244\n",
      "current loss at 3571 iteration 0.061937431610326214\n",
      "current loss at 3581 iteration 0.061933668173666204\n",
      "current loss at 3591 iteration 0.061929928157972186\n",
      "current loss at 3601 iteration 0.06192621135202793\n",
      "current loss at 3611 iteration 0.06192251754709515\n",
      "current loss at 3621 iteration 0.061918846536877764\n",
      "current loss at 3631 iteration 0.06191519811748688\n",
      "current loss at 3641 iteration 0.06191157208740647\n",
      "current loss at 3651 iteration 0.06190796824745935\n",
      "current loss at 3661 iteration 0.06190438640077404\n",
      "current loss at 3671 iteration 0.06190082635275197\n",
      "current loss at 3681 iteration 0.06189728791103529\n",
      "current loss at 3691 iteration 0.06189377088547529\n",
      "current loss at 3701 iteration 0.06189027508810127\n",
      "current loss at 3711 iteration 0.06188680033308988\n",
      "current loss at 3721 iteration 0.06188334643673515\n",
      "current loss at 3731 iteration 0.06187991321741885\n",
      "current loss at 3741 iteration 0.061876500495581355\n",
      "current loss at 3751 iteration 0.06187310809369306\n",
      "current loss at 3761 iteration 0.061869735836226264\n",
      "current loss at 3771 iteration 0.061866383549627334\n",
      "current loss at 3781 iteration 0.061863051062289696\n",
      "current loss at 3791 iteration 0.0618597382045269\n",
      "current loss at 3801 iteration 0.06185644480854618\n",
      "current loss at 3811 iteration 0.06185317070842279\n",
      "current loss at 3821 iteration 0.061849915740074195\n",
      "current loss at 3831 iteration 0.06184667974123516\n",
      "current loss at 3841 iteration 0.06184346255143301\n",
      "current loss at 3851 iteration 0.061840264011963325\n",
      "current loss at 3861 iteration 0.061837083965865985\n",
      "current loss at 3871 iteration 0.06183392225790169\n",
      "current loss at 3881 iteration 0.06183077873452888\n",
      "current loss at 3891 iteration 0.06182765324388076\n",
      "current loss at 3901 iteration 0.06182454563574307\n",
      "current loss at 3911 iteration 0.0618214557615319\n",
      "current loss at 3921 iteration 0.06181838347427203\n",
      "current loss at 3931 iteration 0.061815328628575465\n",
      "current loss at 3941 iteration 0.06181229108062051\n",
      "current loss at 3951 iteration 0.06180927068813101\n",
      "current loss at 3961 iteration 0.06180626731035588\n",
      "current loss at 3971 iteration 0.061803280808049185\n",
      "current loss at 3981 iteration 0.06180031104345021\n",
      "current loss at 3991 iteration 0.06179735788026418\n",
      "current loss at 4001 iteration 0.061794421183643006\n",
      "current loss at 4011 iteration 0.06179150082016643\n",
      "current loss at 4021 iteration 0.06178859665782345\n",
      "current loss at 4031 iteration 0.06178570856599414\n",
      "current loss at 4041 iteration 0.06178283641543155\n",
      "current loss at 4051 iteration 0.061779980078244\n",
      "current loss at 4061 iteration 0.06177713942787758\n",
      "current loss at 4071 iteration 0.061774314339099176\n",
      "current loss at 4081 iteration 0.06177150468797924\n",
      "current loss at 4091 iteration 0.06176871035187533\n",
      "current loss at 4101 iteration 0.06176593120941562\n",
      "current loss at 4111 iteration 0.06176316714048275\n",
      "current loss at 4121 iteration 0.06176041802619787\n",
      "current loss at 4131 iteration 0.06175768374890497\n",
      "current loss at 4141 iteration 0.061754964192155454\n",
      "current loss at 4151 iteration 0.06175225924069282\n",
      "current loss at 4161 iteration 0.06174956878043784\n",
      "current loss at 4171 iteration 0.06174689269847359\n",
      "current loss at 4181 iteration 0.061744230883031034\n",
      "current loss at 4191 iteration 0.061741583223474644\n",
      "current loss at 4201 iteration 0.061738949610288266\n",
      "current loss at 4211 iteration 0.0617363299350612\n",
      "current loss at 4221 iteration 0.06173372409047452\n",
      "current loss at 4231 iteration 0.06173113197028753\n",
      "current loss at 4241 iteration 0.06172855346932445\n",
      "current loss at 4251 iteration 0.061725988483461364\n",
      "current loss at 4261 iteration 0.06172343690961314\n",
      "current loss at 4271 iteration 0.06172089864572086\n",
      "current loss at 4281 iteration 0.061718373590739145\n",
      "current loss at 4291 iteration 0.0617158616446239\n",
      "current loss at 4301 iteration 0.06171336270832\n",
      "current loss at 4311 iteration 0.06171087668374928\n",
      "current loss at 4321 iteration 0.0617084034737988\n",
      "current loss at 4331 iteration 0.06170594298230909\n",
      "current loss at 4341 iteration 0.06170349511406259\n",
      "current loss at 4351 iteration 0.06170105977477242\n",
      "current loss at 4361 iteration 0.06169863687107116\n",
      "current loss at 4371 iteration 0.06169622631049972\n",
      "current loss at 4381 iteration 0.061693828001496635\n",
      "current loss at 4391 iteration 0.06169144185338725\n",
      "current loss at 4401 iteration 0.061689067776373174\n",
      "current loss at 4411 iteration 0.06168670568152195\n",
      "current loss at 4421 iteration 0.061684355480756646\n",
      "current loss at 4431 iteration 0.06168201708684576\n",
      "current loss at 4441 iteration 0.06167969041339342\n",
      "current loss at 4451 iteration 0.06167737537482928\n",
      "current loss at 4461 iteration 0.06167507188639901\n",
      "current loss at 4471 iteration 0.06167277986415461\n",
      "current loss at 4481 iteration 0.061670499224945066\n",
      "current loss at 4491 iteration 0.06166822988640698\n",
      "current loss at 4501 iteration 0.061665971766955416\n",
      "current loss at 4511 iteration 0.06166372478577483\n",
      "current loss at 4521 iteration 0.0616614888628102\n",
      "current loss at 4531 iteration 0.0616592639187581\n",
      "current loss at 4541 iteration 0.06165704987505824\n",
      "current loss at 4551 iteration 0.06165484665388468\n",
      "current loss at 4561 iteration 0.061652654178137395\n",
      "current loss at 4571 iteration 0.06165047237143414\n",
      "current loss at 4581 iteration 0.061648301158102076\n",
      "current loss at 4591 iteration 0.061646140463169644\n",
      "current loss at 4601 iteration 0.061643990212358654\n",
      "current loss at 4611 iteration 0.061641850332076364\n",
      "current loss at 4621 iteration 0.0616397207494077\n",
      "current loss at 4631 iteration 0.0616376013921075\n",
      "current loss at 4641 iteration 0.06163549218859313\n",
      "current loss at 4651 iteration 0.06163339306793672\n",
      "current loss at 4661 iteration 0.06163130395985815\n",
      "current loss at 4671 iteration 0.061629224794717455\n",
      "current loss at 4681 iteration 0.06162715550350777\n",
      "current loss at 4691 iteration 0.06162509601784833\n",
      "current loss at 4701 iteration 0.06162304626997736\n",
      "current loss at 4711 iteration 0.061621006192745194\n",
      "current loss at 4721 iteration 0.061618975719607505\n",
      "current loss at 4731 iteration 0.06161695478461861\n",
      "current loss at 4741 iteration 0.06161494332242476\n",
      "current loss at 4751 iteration 0.0616129412682577\n",
      "current loss at 4761 iteration 0.0616109485579281\n",
      "current loss at 4771 iteration 0.0616089651278193\n",
      "current loss at 4781 iteration 0.06160699091488094\n",
      "current loss at 4791 iteration 0.06160502585662275\n",
      "current loss at 4801 iteration 0.06160306989110852\n",
      "current loss at 4811 iteration 0.061601122956949944\n",
      "current loss at 4821 iteration 0.061599184993300714\n",
      "current loss at 4831 iteration 0.06159725593985056\n",
      "current loss at 4841 iteration 0.06159533573681954\n",
      "current loss at 4851 iteration 0.06159342432495222\n",
      "current loss at 4861 iteration 0.0615915216455121\n",
      "current loss at 4871 iteration 0.061589627640275785\n",
      "current loss at 4881 iteration 0.06158774225152779\n",
      "current loss at 4891 iteration 0.06158586542205488\n",
      "current loss at 4901 iteration 0.06158399709514067\n",
      "current loss at 4911 iteration 0.061582137214560455\n",
      "current loss at 4921 iteration 0.06158028572457584\n",
      "current loss at 4931 iteration 0.06157844256992959\n",
      "current loss at 4941 iteration 0.061576607695840564\n",
      "current loss at 4951 iteration 0.06157478104799858\n",
      "current loss at 4961 iteration 0.06157296257255946\n",
      "current loss at 4971 iteration 0.061571152216140144\n",
      "current loss at 4981 iteration 0.06156934992581385\n",
      "current loss at 4991 iteration 0.061567555649105134\n",
      "current loss at 5001 iteration 0.061565769333985276\n",
      "current loss at 5011 iteration 0.06156399092886755\n",
      "current loss at 5021 iteration 0.06156222038260257\n",
      "current loss at 5031 iteration 0.06156045764447381\n",
      "current loss at 5041 iteration 0.06155870266419297\n",
      "current loss at 5051 iteration 0.061556955391895686\n",
      "current loss at 5061 iteration 0.061555215778136926\n",
      "current loss at 5071 iteration 0.061553483773886836\n",
      "current loss at 5081 iteration 0.06155175933052629\n",
      "current loss at 5091 iteration 0.06155004239984277\n",
      "current loss at 5101 iteration 0.06154833293402606\n",
      "current loss at 5111 iteration 0.061546630885664304\n",
      "current loss at 5121 iteration 0.06154493620773961\n",
      "current loss at 5131 iteration 0.06154324885362439\n",
      "current loss at 5141 iteration 0.061541568777077\n",
      "current loss at 5151 iteration 0.06153989593223811\n",
      "current loss at 5161 iteration 0.061538230273626565\n",
      "current loss at 5171 iteration 0.061536571756135756\n",
      "current loss at 5181 iteration 0.06153492033502964\n",
      "current loss at 5191 iteration 0.0615332759659391\n",
      "current loss at 5201 iteration 0.06153163860485819\n",
      "current loss at 5211 iteration 0.0615300082081405\n",
      "current loss at 5221 iteration 0.06152838473249548\n",
      "current loss at 5231 iteration 0.061526768134984956\n",
      "current loss at 5241 iteration 0.06152515837301948\n",
      "current loss at 5251 iteration 0.06152355540435498\n",
      "current loss at 5261 iteration 0.06152195918708915\n",
      "current loss at 5271 iteration 0.06152036967965819\n",
      "current loss at 5281 iteration 0.061518786840833325\n",
      "current loss at 5291 iteration 0.0615172106297175\n",
      "current loss at 5301 iteration 0.06151564100574215\n",
      "current loss at 5311 iteration 0.06151407792866389\n",
      "current loss at 5321 iteration 0.061512521358561334\n",
      "current loss at 5331 iteration 0.0615109712558319\n",
      "current loss at 5341 iteration 0.06150942758118864\n",
      "current loss at 5351 iteration 0.0615078902956572\n",
      "current loss at 5361 iteration 0.061506359360572756\n",
      "current loss at 5371 iteration 0.061504834737576904\n",
      "current loss at 5381 iteration 0.06150331638861474\n",
      "current loss at 5391 iteration 0.061501804275931854\n",
      "current loss at 5401 iteration 0.06150029836207152\n",
      "current loss at 5411 iteration 0.06149879860987159\n",
      "current loss at 5421 iteration 0.06149730498246183\n",
      "current loss at 5431 iteration 0.061495817443260964\n",
      "current loss at 5441 iteration 0.061494335955974054\n",
      "current loss at 5451 iteration 0.061492860484589434\n",
      "current loss at 5461 iteration 0.06149139099337634\n",
      "current loss at 5471 iteration 0.06148992744688199\n",
      "current loss at 5481 iteration 0.06148846980992892\n",
      "current loss at 5491 iteration 0.06148701804761244\n",
      "current loss at 5501 iteration 0.061485572125297955\n",
      "current loss at 5511 iteration 0.061484132008618415\n",
      "current loss at 5521 iteration 0.061482697663471794\n",
      "current loss at 5531 iteration 0.06148126905601851\n",
      "current loss at 5541 iteration 0.06147984615267896\n",
      "current loss at 5551 iteration 0.061478428920131084\n",
      "current loss at 5561 iteration 0.06147701732530789\n",
      "current loss at 5571 iteration 0.06147561133539502\n",
      "current loss at 5581 iteration 0.0614742109178285\n",
      "current loss at 5591 iteration 0.06147281604029223\n",
      "current loss at 5601 iteration 0.06147142667071573\n",
      "current loss at 5611 iteration 0.06147004277727185\n",
      "current loss at 5621 iteration 0.06146866432837444\n",
      "current loss at 5631 iteration 0.06146729129267617\n",
      "current loss at 5641 iteration 0.061465923639066194\n",
      "current loss at 5651 iteration 0.06146456133666811\n",
      "current loss at 5661 iteration 0.06146320435483764\n",
      "current loss at 5671 iteration 0.06146185266316051\n",
      "current loss at 5681 iteration 0.06146050623145037\n",
      "current loss at 5691 iteration 0.06145916502974664\n",
      "current loss at 5701 iteration 0.06145782902831244\n",
      "current loss at 5711 iteration 0.061456498197632534\n",
      "current loss at 5721 iteration 0.06145517250841129\n",
      "current loss at 5731 iteration 0.061453851931570616\n",
      "current loss at 5741 iteration 0.061452536438248134\n",
      "current loss at 5751 iteration 0.06145122599979494\n",
      "current loss at 5761 iteration 0.06144992058777391\n",
      "current loss at 5771 iteration 0.061448620173957615\n",
      "current loss at 5781 iteration 0.061447324730326464\n",
      "current loss at 5791 iteration 0.06144603422906677\n",
      "current loss at 5801 iteration 0.06144474864256902\n",
      "current loss at 5811 iteration 0.0614434679434258\n",
      "current loss at 5821 iteration 0.06144219210443022\n",
      "current loss at 5831 iteration 0.06144092109857392\n",
      "current loss at 5841 iteration 0.06143965489904538\n",
      "current loss at 5851 iteration 0.06143839347922805\n",
      "current loss at 5861 iteration 0.061437136812698676\n",
      "current loss at 5871 iteration 0.06143588487322565\n",
      "current loss at 5881 iteration 0.0614346376347671\n",
      "current loss at 5891 iteration 0.06143339507146934\n",
      "current loss at 5901 iteration 0.06143215715766511\n",
      "current loss at 5911 iteration 0.06143092386787204\n",
      "current loss at 5921 iteration 0.06142969517679083\n",
      "current loss at 5931 iteration 0.06142847105930376\n",
      "current loss at 5941 iteration 0.061427251490473074\n",
      "current loss at 5951 iteration 0.06142603644553927\n",
      "current loss at 5961 iteration 0.06142482589991971\n",
      "current loss at 5971 iteration 0.06142361982920692\n",
      "current loss at 5981 iteration 0.06142241820916704\n",
      "current loss at 5991 iteration 0.06142122101573847\n",
      "current loss at 6001 iteration 0.061420028225030135\n",
      "current loss at 6011 iteration 0.06141883981332013\n",
      "current loss at 6021 iteration 0.061417655757054254\n",
      "current loss at 6031 iteration 0.061416476032844475\n",
      "current loss at 6041 iteration 0.061415300617467486\n",
      "current loss at 6051 iteration 0.06141412948786335\n",
      "current loss at 6061 iteration 0.06141296262113397\n",
      "current loss at 6071 iteration 0.06141179999454181\n",
      "current loss at 6081 iteration 0.061410641585508415\n",
      "current loss at 6091 iteration 0.06140948737161299\n",
      "current loss at 6101 iteration 0.06140833733059129\n",
      "current loss at 6111 iteration 0.06140719144033392\n",
      "current loss at 6121 iteration 0.06140604967888531\n",
      "current loss at 6131 iteration 0.06140491202444217\n",
      "current loss at 6141 iteration 0.06140377845535231\n",
      "current loss at 6151 iteration 0.061402648950113346\n",
      "current loss at 6161 iteration 0.06140152348737138\n",
      "current loss at 6171 iteration 0.061400402045919726\n",
      "current loss at 6181 iteration 0.06139928460469769\n",
      "current loss at 6191 iteration 0.06139817114278935\n",
      "current loss at 6201 iteration 0.061397061639422205\n",
      "current loss at 6211 iteration 0.06139595607396616\n",
      "current loss at 6221 iteration 0.06139485442593215\n",
      "current loss at 6231 iteration 0.06139375667497098\n",
      "current loss at 6241 iteration 0.061392662800872265\n",
      "current loss at 6251 iteration 0.06139157278356306\n",
      "current loss at 6261 iteration 0.06139048660310691\n",
      "current loss at 6271 iteration 0.06138940423970259\n",
      "current loss at 6281 iteration 0.06138832567368297\n",
      "current loss at 6291 iteration 0.061387250885513917\n",
      "current loss at 6301 iteration 0.0613861798557932\n",
      "current loss at 6311 iteration 0.06138511256524938\n",
      "current loss at 6321 iteration 0.0613840489947407\n",
      "current loss at 6331 iteration 0.06138298912525402\n",
      "current loss at 6341 iteration 0.06138193293790376\n",
      "current loss at 6351 iteration 0.061380880413930895\n",
      "current loss at 6361 iteration 0.06137983153470174\n",
      "current loss at 6371 iteration 0.06137878628170706\n",
      "current loss at 6381 iteration 0.06137774463656108\n",
      "current loss at 6391 iteration 0.061376706581000286\n",
      "current loss at 6401 iteration 0.061375672096882626\n",
      "current loss at 6411 iteration 0.06137464116618637\n",
      "current loss at 6421 iteration 0.06137361377100915\n",
      "current loss at 6431 iteration 0.06137258989356706\n",
      "current loss at 6441 iteration 0.06137156951619359\n",
      "current loss at 6451 iteration 0.06137055262133871\n",
      "current loss at 6461 iteration 0.061369539191567984\n",
      "current loss at 6471 iteration 0.06136852920956147\n",
      "current loss at 6481 iteration 0.061367522658112995\n",
      "current loss at 6491 iteration 0.061366519520129045\n",
      "current loss at 6501 iteration 0.06136551977862795\n",
      "current loss at 6511 iteration 0.06136452341673901\n",
      "current loss at 6521 iteration 0.06136353041770151\n",
      "current loss at 6531 iteration 0.0613625407648639\n",
      "current loss at 6541 iteration 0.0613615544416829\n",
      "current loss at 6551 iteration 0.06136057143172256\n",
      "current loss at 6561 iteration 0.06135959171865356\n",
      "current loss at 6571 iteration 0.06135861528625222\n",
      "current loss at 6581 iteration 0.0613576421183997\n",
      "current loss at 6591 iteration 0.06135667219908114\n",
      "current loss at 6601 iteration 0.06135570551238488\n",
      "current loss at 6611 iteration 0.06135474204250159\n",
      "current loss at 6621 iteration 0.06135378177372348\n",
      "current loss at 6631 iteration 0.06135282469044348\n",
      "current loss at 6641 iteration 0.06135187077715449\n",
      "current loss at 6651 iteration 0.06135092001844852\n",
      "current loss at 6661 iteration 0.061349972399015934\n",
      "current loss at 6671 iteration 0.061349027903644715\n",
      "current loss at 6681 iteration 0.06134808651721962\n",
      "current loss at 6691 iteration 0.061347148224721454\n",
      "current loss at 6701 iteration 0.061346213011226385\n",
      "current loss at 6711 iteration 0.06134528086190506\n",
      "current loss at 6721 iteration 0.061344351762021944\n",
      "current loss at 6731 iteration 0.061343425696934656\n",
      "current loss at 6741 iteration 0.06134250265209308\n",
      "current loss at 6751 iteration 0.06134158261303874\n",
      "current loss at 6761 iteration 0.06134066556540415\n",
      "current loss at 6771 iteration 0.061339751494911984\n",
      "current loss at 6781 iteration 0.061338840387374395\n",
      "current loss at 6791 iteration 0.06133793222869244\n",
      "current loss at 6801 iteration 0.06133702700485523\n",
      "current loss at 6811 iteration 0.061336124701939373\n",
      "current loss at 6821 iteration 0.061335225306108195\n",
      "current loss at 6831 iteration 0.06133432880361124\n",
      "current loss at 6841 iteration 0.061333435180783394\n",
      "current loss at 6851 iteration 0.06133254442404437\n",
      "current loss at 6861 iteration 0.061331656519898\n",
      "current loss at 6871 iteration 0.061330771454931675\n",
      "current loss at 6881 iteration 0.06132988921581554\n",
      "current loss at 6891 iteration 0.061329009789302065\n",
      "current loss at 6901 iteration 0.06132813316222527\n",
      "current loss at 6911 iteration 0.061327259321500106\n",
      "current loss at 6921 iteration 0.06132638825412198\n",
      "current loss at 6931 iteration 0.061325519947166\n",
      "current loss at 6941 iteration 0.0613246543877864\n",
      "current loss at 6951 iteration 0.06132379156321599\n",
      "current loss at 6961 iteration 0.061322931460765534\n",
      "current loss at 6971 iteration 0.06132207406782313\n",
      "current loss at 6981 iteration 0.061321219371853676\n",
      "current loss at 6991 iteration 0.061320367360398316\n",
      "current loss at 7001 iteration 0.06131951802107375\n",
      "current loss at 7011 iteration 0.06131867134157179\n",
      "current loss at 7021 iteration 0.06131782730965872\n",
      "current loss at 7031 iteration 0.06131698591317482\n",
      "current loss at 7041 iteration 0.061316147140033715\n",
      "current loss at 7051 iteration 0.06131531097822186\n",
      "current loss at 7061 iteration 0.06131447741579806\n",
      "current loss at 7071 iteration 0.06131364644089286\n",
      "current loss at 7081 iteration 0.06131281804170804\n",
      "current loss at 7091 iteration 0.061311992206516036\n",
      "current loss at 7101 iteration 0.061311168923659544\n",
      "current loss at 7111 iteration 0.06131034818155085\n",
      "current loss at 7121 iteration 0.061309529968671436\n",
      "current loss at 7131 iteration 0.06130871427357137\n",
      "current loss at 7141 iteration 0.06130790108486888\n",
      "current loss at 7151 iteration 0.06130709039124979\n",
      "current loss at 7161 iteration 0.061306282181467116\n",
      "current loss at 7171 iteration 0.061305476444340444\n",
      "current loss at 7181 iteration 0.06130467316875554\n",
      "current loss at 7191 iteration 0.06130387234366386\n",
      "current loss at 7201 iteration 0.06130307395808199\n",
      "current loss at 7211 iteration 0.061302278001091226\n",
      "current loss at 7221 iteration 0.061301484461837204\n",
      "current loss at 7231 iteration 0.061300693329529206\n",
      "current loss at 7241 iteration 0.06129990459343991\n",
      "current loss at 7251 iteration 0.06129911824290478\n",
      "current loss at 7261 iteration 0.06129833426732174\n",
      "current loss at 7271 iteration 0.061297552656150604\n",
      "current loss at 7281 iteration 0.06129677339891267\n",
      "current loss at 7291 iteration 0.0612959964851904\n",
      "current loss at 7301 iteration 0.06129522190462669\n",
      "current loss at 7311 iteration 0.06129444964692475\n",
      "current loss at 7321 iteration 0.0612936797018475\n",
      "current loss at 7331 iteration 0.06129291205921715\n",
      "current loss at 7341 iteration 0.06129214670891477\n",
      "current loss at 7351 iteration 0.061291383640880014\n",
      "current loss at 7361 iteration 0.06129062284511046\n",
      "current loss at 7371 iteration 0.061289864311661425\n",
      "current loss at 7381 iteration 0.06128910803064534\n",
      "current loss at 7391 iteration 0.06128835399223157\n",
      "current loss at 7401 iteration 0.06128760218664581\n",
      "current loss at 7411 iteration 0.06128685260416981\n",
      "current loss at 7421 iteration 0.061286105235140935\n",
      "current loss at 7431 iteration 0.06128536006995174\n",
      "current loss at 7441 iteration 0.06128461709904968\n",
      "current loss at 7451 iteration 0.061283876312936574\n",
      "current loss at 7461 iteration 0.061283137702168373\n",
      "current loss at 7471 iteration 0.06128240125735465\n",
      "current loss at 7481 iteration 0.06128166696915834\n",
      "current loss at 7491 iteration 0.06128093482829527\n",
      "current loss at 7501 iteration 0.061280204825533834\n",
      "current loss at 7511 iteration 0.06127947695169465\n",
      "current loss at 7521 iteration 0.061278751197650076\n",
      "current loss at 7531 iteration 0.06127802755432404\n",
      "current loss at 7541 iteration 0.06127730601269151\n",
      "current loss at 7551 iteration 0.0612765865637782\n",
      "current loss at 7561 iteration 0.06127586919866024\n",
      "current loss at 7571 iteration 0.061275153908463845\n",
      "current loss at 7581 iteration 0.06127444068436484\n",
      "current loss at 7591 iteration 0.061273729517588485\n",
      "current loss at 7601 iteration 0.061273020399408996\n",
      "current loss at 7611 iteration 0.06127231332114932\n",
      "current loss at 7621 iteration 0.06127160827418069\n",
      "current loss at 7631 iteration 0.061270905249922375\n",
      "current loss at 7641 iteration 0.06127020423984128\n",
      "current loss at 7651 iteration 0.06126950523545175\n",
      "current loss at 7661 iteration 0.061268808228315055\n",
      "current loss at 7671 iteration 0.06126811321003919\n",
      "current loss at 7681 iteration 0.06126742017227855\n",
      "current loss at 7691 iteration 0.061266729106733586\n",
      "current loss at 7701 iteration 0.061266040005150506\n",
      "current loss at 7711 iteration 0.06126535285932095\n",
      "current loss at 7721 iteration 0.06126466766108164\n",
      "current loss at 7731 iteration 0.061263984402314224\n",
      "current loss at 7741 iteration 0.061263303074944736\n",
      "current loss at 7751 iteration 0.061262623670943546\n",
      "current loss at 7761 iteration 0.06126194618232485\n",
      "current loss at 7771 iteration 0.0612612706011465\n",
      "current loss at 7781 iteration 0.06126059691950966\n",
      "current loss at 7791 iteration 0.06125992512955853\n",
      "current loss at 7801 iteration 0.061259255223480066\n",
      "current loss at 7811 iteration 0.06125858719350363\n",
      "current loss at 7821 iteration 0.0612579210319008\n",
      "current loss at 7831 iteration 0.06125725673098502\n",
      "current loss at 7841 iteration 0.06125659428311134\n",
      "current loss at 7851 iteration 0.06125593368067612\n",
      "current loss at 7861 iteration 0.06125527491611677\n",
      "current loss at 7871 iteration 0.06125461798191148\n",
      "current loss at 7881 iteration 0.061253962870578986\n",
      "current loss at 7891 iteration 0.061253309574678165\n",
      "current loss at 7901 iteration 0.06125265808680792\n",
      "current loss at 7911 iteration 0.06125200839960686\n",
      "current loss at 7921 iteration 0.06125136050575301\n",
      "current loss at 7931 iteration 0.06125071439796354\n",
      "current loss at 7941 iteration 0.0612500700689946\n",
      "current loss at 7951 iteration 0.061249427511640926\n",
      "current loss at 7961 iteration 0.06124878671873571\n",
      "current loss at 7971 iteration 0.061248147683150296\n",
      "current loss at 7981 iteration 0.06124751039779386\n",
      "current loss at 7991 iteration 0.061246874855613295\n",
      "current loss at 8001 iteration 0.061246241049592864\n",
      "current loss at 8011 iteration 0.061245608972753995\n",
      "current loss at 8021 iteration 0.06124497861815503\n",
      "current loss at 8031 iteration 0.06124434997889096\n",
      "current loss at 8041 iteration 0.06124372304809321\n",
      "current loss at 8051 iteration 0.06124309781892945\n",
      "current loss at 8061 iteration 0.06124247428460323\n",
      "current loss at 8071 iteration 0.06124185243835388\n",
      "current loss at 8081 iteration 0.06124123227345615\n",
      "current loss at 8091 iteration 0.061240613783220196\n",
      "current loss at 8101 iteration 0.061239996960991014\n",
      "current loss at 8111 iteration 0.06123938180014854\n",
      "current loss at 8121 iteration 0.061238768294107224\n",
      "current loss at 8131 iteration 0.061238156436315916\n",
      "current loss at 8141 iteration 0.06123754622025753\n",
      "current loss at 8151 iteration 0.061236937639449006\n",
      "current loss at 8161 iteration 0.06123633068744086\n",
      "current loss at 8171 iteration 0.06123572535781715\n",
      "current loss at 8181 iteration 0.06123512164419519\n",
      "current loss at 8191 iteration 0.061234519540225324\n",
      "current loss at 8201 iteration 0.061233919039590745\n",
      "current loss at 8211 iteration 0.0612333201360073\n",
      "current loss at 8221 iteration 0.0612327228232232\n",
      "current loss at 8231 iteration 0.06123212709501892\n",
      "current loss at 8241 iteration 0.06123153294520689\n",
      "current loss at 8251 iteration 0.06123094036763141\n",
      "current loss at 8261 iteration 0.06123034935616831\n",
      "current loss at 8271 iteration 0.061229759904724866\n",
      "current loss at 8281 iteration 0.06122917200723953\n",
      "current loss at 8291 iteration 0.061228585657681744\n",
      "current loss at 8301 iteration 0.06122800085005179\n",
      "current loss at 8311 iteration 0.0612274175783806\n",
      "current loss at 8321 iteration 0.061226835836729415\n",
      "current loss at 8331 iteration 0.061226255619189755\n",
      "current loss at 8341 iteration 0.061225676919883204\n",
      "current loss at 8351 iteration 0.06122509973296117\n",
      "current loss at 8361 iteration 0.06122452405260469\n",
      "current loss at 8371 iteration 0.06122394987302434\n",
      "current loss at 8381 iteration 0.061223377188459924\n",
      "current loss at 8391 iteration 0.061222805993180356\n",
      "current loss at 8401 iteration 0.06122223628148355\n",
      "current loss at 8411 iteration 0.06122166804769602\n",
      "current loss at 8421 iteration 0.061221101286172974\n",
      "current loss at 8431 iteration 0.06122053599129794\n",
      "current loss at 8441 iteration 0.061219972157482684\n",
      "current loss at 8451 iteration 0.061219409779166956\n",
      "current loss at 8461 iteration 0.06121884885081842\n",
      "current loss at 8471 iteration 0.061218289366932396\n",
      "current loss at 8481 iteration 0.06121773132203174\n",
      "current loss at 8491 iteration 0.0612171747106666\n",
      "current loss at 8501 iteration 0.06121661952741438\n",
      "current loss at 8511 iteration 0.061216065766879386\n",
      "current loss at 8521 iteration 0.06121551342369288\n",
      "current loss at 8531 iteration 0.061214962492512716\n",
      "current loss at 8541 iteration 0.06121441296802329\n",
      "current loss at 8551 iteration 0.06121386484493536\n",
      "current loss at 8561 iteration 0.06121331811798582\n",
      "current loss at 8571 iteration 0.061212772781937645\n",
      "current loss at 8581 iteration 0.06121222883157965\n",
      "current loss at 8591 iteration 0.061211686261726364\n",
      "current loss at 8601 iteration 0.06121114506721786\n",
      "current loss at 8611 iteration 0.06121060524291961\n",
      "current loss at 8621 iteration 0.061210066783722344\n",
      "current loss at 8631 iteration 0.06120952968454188\n",
      "current loss at 8641 iteration 0.061208993940318895\n",
      "current loss at 8651 iteration 0.06120845954601896\n",
      "current loss at 8661 iteration 0.06120792649663223\n",
      "current loss at 8671 iteration 0.0612073947871733\n",
      "current loss at 8681 iteration 0.061206864412681194\n",
      "current loss at 8691 iteration 0.06120633536821903\n",
      "current loss at 8701 iteration 0.06120580764887404\n",
      "current loss at 8711 iteration 0.061205281249757326\n",
      "current loss at 8721 iteration 0.06120475616600371\n",
      "current loss at 8731 iteration 0.06120423239277168\n",
      "current loss at 8741 iteration 0.06120370992524317\n",
      "current loss at 8751 iteration 0.061203188758623434\n",
      "current loss at 8761 iteration 0.0612026688881409\n",
      "current loss at 8771 iteration 0.061202150309047125\n",
      "current loss at 8781 iteration 0.06120163301661647\n",
      "current loss at 8791 iteration 0.06120111700614612\n",
      "current loss at 8801 iteration 0.06120060227295591\n",
      "current loss at 8811 iteration 0.061200088812388156\n",
      "current loss at 8821 iteration 0.06119957661980757\n",
      "current loss at 8831 iteration 0.06119906569060106\n",
      "current loss at 8841 iteration 0.061198556020177666\n",
      "current loss at 8851 iteration 0.061198047603968395\n",
      "current loss at 8861 iteration 0.06119754043742612\n",
      "current loss at 8871 iteration 0.06119703451602538\n",
      "current loss at 8881 iteration 0.06119652983526233\n",
      "current loss at 8891 iteration 0.06119602639065458\n",
      "current loss at 8901 iteration 0.061195524177741066\n",
      "current loss at 8911 iteration 0.061195023192081935\n",
      "current loss at 8921 iteration 0.0611945234292584\n",
      "current loss at 8931 iteration 0.06119402488487266\n",
      "current loss at 8941 iteration 0.06119352755454772\n",
      "current loss at 8951 iteration 0.06119303143392729\n",
      "current loss at 8961 iteration 0.0611925365186757\n",
      "current loss at 8971 iteration 0.06119204280447775\n",
      "current loss at 8981 iteration 0.061191550287038536\n",
      "current loss at 8991 iteration 0.06119105896208343\n",
      "current loss at 9001 iteration 0.061190568825357956\n",
      "current loss at 9011 iteration 0.06119007987262753\n",
      "current loss at 9021 iteration 0.061189592099677524\n",
      "current loss at 9031 iteration 0.06118910550231306\n",
      "current loss at 9041 iteration 0.0611886200763589\n",
      "current loss at 9051 iteration 0.06118813581765931\n",
      "current loss at 9061 iteration 0.06118765272207806\n",
      "current loss at 9071 iteration 0.061187170785498145\n",
      "current loss at 9081 iteration 0.06118669000382182\n",
      "current loss at 9091 iteration 0.061186210372970354\n",
      "current loss at 9101 iteration 0.061185731888884085\n",
      "current loss at 9111 iteration 0.061185254547522154\n",
      "current loss at 9121 iteration 0.06118477834486243\n",
      "current loss at 9131 iteration 0.061184303276901546\n",
      "current loss at 9141 iteration 0.06118382933965459\n",
      "current loss at 9151 iteration 0.0611833565291551\n",
      "current loss at 9161 iteration 0.061182884841454954\n",
      "current loss at 9171 iteration 0.06118241427262425\n",
      "current loss at 9181 iteration 0.06118194481875123\n",
      "current loss at 9191 iteration 0.06118147647594214\n",
      "current loss at 9201 iteration 0.06118100924032113\n",
      "current loss at 9211 iteration 0.06118054310803018\n",
      "current loss at 9221 iteration 0.06118007807522896\n",
      "current loss at 9231 iteration 0.06117961413809479\n",
      "current loss at 9241 iteration 0.0611791512928225\n",
      "current loss at 9251 iteration 0.061178689535624255\n",
      "current loss at 9261 iteration 0.061178228862729646\n",
      "current loss at 9271 iteration 0.0611777692703854\n",
      "current loss at 9281 iteration 0.061177310754855406\n",
      "current loss at 9291 iteration 0.06117685331242052\n",
      "current loss at 9301 iteration 0.0611763969393786\n",
      "current loss at 9311 iteration 0.06117594163204424\n",
      "current loss at 9321 iteration 0.06117548738674889\n",
      "current loss at 9331 iteration 0.06117503419984052\n",
      "current loss at 9341 iteration 0.061174582067683704\n",
      "current loss at 9351 iteration 0.0611741309866595\n",
      "current loss at 9361 iteration 0.061173680953165256\n",
      "current loss at 9371 iteration 0.06117323196361467\n",
      "current loss at 9381 iteration 0.06117278401443755\n",
      "current loss at 9391 iteration 0.061172337102079824\n",
      "current loss at 9401 iteration 0.06117189122300345\n",
      "current loss at 9411 iteration 0.06117144637368621\n",
      "current loss at 9421 iteration 0.061171002550621814\n",
      "current loss at 9431 iteration 0.061170559750319625\n",
      "current loss at 9441 iteration 0.06117011796930468\n",
      "current loss at 9451 iteration 0.061169677204117576\n",
      "current loss at 9461 iteration 0.061169237451314404\n",
      "current loss at 9471 iteration 0.06116879870746658\n",
      "current loss at 9481 iteration 0.06116836096916091\n",
      "current loss at 9491 iteration 0.06116792423299933\n",
      "current loss at 9501 iteration 0.06116748849559896\n",
      "current loss at 9511 iteration 0.06116705375359196\n",
      "current loss at 9521 iteration 0.06116662000362546\n",
      "current loss at 9531 iteration 0.06116618724236145\n",
      "current loss at 9541 iteration 0.06116575546647678\n",
      "current loss at 9551 iteration 0.061165324672662925\n",
      "current loss at 9561 iteration 0.061164894857626106\n",
      "current loss at 9571 iteration 0.06116446601808705\n",
      "current loss at 9581 iteration 0.06116403815078095\n",
      "current loss at 9591 iteration 0.061163611252457444\n",
      "current loss at 9601 iteration 0.061163185319880436\n",
      "current loss at 9611 iteration 0.06116276034982812\n",
      "current loss at 9621 iteration 0.061162336339092876\n",
      "current loss at 9631 iteration 0.06116191328448109\n",
      "current loss at 9641 iteration 0.061161491182813235\n",
      "current loss at 9651 iteration 0.0611610700309237\n",
      "current loss at 9661 iteration 0.061160649825660746\n",
      "current loss at 9671 iteration 0.06116023056388636\n",
      "current loss at 9681 iteration 0.061159812242476304\n",
      "current loss at 9691 iteration 0.06115939485831997\n",
      "current loss at 9701 iteration 0.06115897840832031\n",
      "current loss at 9711 iteration 0.061158562889393725\n",
      "current loss at 9721 iteration 0.06115814829847008\n",
      "current loss at 9731 iteration 0.06115773463249256\n",
      "current loss at 9741 iteration 0.06115732188841763\n",
      "current loss at 9751 iteration 0.06115691006321496\n",
      "current loss at 9761 iteration 0.06115649915386735\n",
      "current loss at 9771 iteration 0.061156089157370634\n",
      "current loss at 9781 iteration 0.0611556800707337\n",
      "current loss at 9791 iteration 0.06115527189097828\n",
      "current loss at 9801 iteration 0.061154864615139\n",
      "current loss at 9811 iteration 0.061154458240263256\n",
      "current loss at 9821 iteration 0.06115405276341114\n",
      "current loss at 9831 iteration 0.061153648181655446\n",
      "current loss at 9841 iteration 0.06115324449208151\n",
      "current loss at 9851 iteration 0.06115284169178714\n",
      "current loss at 9861 iteration 0.06115243977788268\n",
      "current loss at 9871 iteration 0.061152038747490756\n",
      "current loss at 9881 iteration 0.06115163859774637\n",
      "current loss at 9891 iteration 0.061151239325796775\n",
      "current loss at 9901 iteration 0.06115084092880136\n",
      "current loss at 9911 iteration 0.061150443403931685\n",
      "current loss at 9921 iteration 0.06115004674837131\n",
      "current loss at 9931 iteration 0.06114965095931583\n",
      "current loss at 9941 iteration 0.06114925603397276\n",
      "current loss at 9951 iteration 0.06114886196956146\n",
      "current loss at 9961 iteration 0.06114846876331311\n",
      "current loss at 9971 iteration 0.061148076412470616\n",
      "current loss at 9981 iteration 0.06114768491428858\n",
      "current loss at 9991 iteration 0.06114729426603321\n",
      "current loss at 10001 iteration 0.06114690446498225\n",
      "current loss at 10011 iteration 0.061146515508424995\n",
      "current loss at 10021 iteration 0.061146127393662125\n",
      "current loss at 10031 iteration 0.06114574011800571\n",
      "current loss at 10041 iteration 0.06114535367877916\n",
      "current loss at 10051 iteration 0.061144968073317076\n",
      "current loss at 10061 iteration 0.06114458329896534\n",
      "current loss at 10071 iteration 0.06114419935308092\n",
      "current loss at 10081 iteration 0.06114381623303189\n",
      "current loss at 10091 iteration 0.06114343393619733\n",
      "current loss at 10101 iteration 0.061143052459967304\n",
      "current loss at 10111 iteration 0.06114267180174279\n",
      "current loss at 10121 iteration 0.061142291958935585\n",
      "current loss at 10131 iteration 0.061141912928968346\n",
      "current loss at 10141 iteration 0.061141534709274414\n",
      "current loss at 10151 iteration 0.06114115729729783\n",
      "current loss at 10161 iteration 0.0611407806904933\n",
      "current loss at 10171 iteration 0.06114040488632604\n",
      "current loss at 10181 iteration 0.061140029882271865\n",
      "current loss at 10191 iteration 0.061139655675817\n",
      "current loss at 10201 iteration 0.06113928226445813\n",
      "current loss at 10211 iteration 0.06113890964570226\n",
      "current loss at 10221 iteration 0.0611385378170667\n",
      "current loss at 10231 iteration 0.06113816677607905\n",
      "current loss at 10241 iteration 0.061137796520277114\n",
      "current loss at 10251 iteration 0.06113742704720879\n",
      "current loss at 10261 iteration 0.06113705835443214\n",
      "current loss at 10271 iteration 0.06113669043951522\n",
      "current loss at 10281 iteration 0.06113632330003614\n",
      "current loss at 10291 iteration 0.06113595693358287\n",
      "current loss at 10301 iteration 0.061135591337753356\n",
      "current loss at 10311 iteration 0.061135226510155335\n",
      "current loss at 10321 iteration 0.06113486244840638\n",
      "current loss at 10331 iteration 0.06113449915013377\n",
      "current loss at 10341 iteration 0.06113413661297448\n",
      "current loss at 10351 iteration 0.06113377483457516\n",
      "current loss at 10361 iteration 0.061133413812592026\n",
      "current loss at 10371 iteration 0.06113305354469088\n",
      "current loss at 10381 iteration 0.061132694028546965\n",
      "current loss at 10391 iteration 0.06113233526184504\n",
      "current loss at 10401 iteration 0.06113197724227921\n",
      "current loss at 10411 iteration 0.06113161996755298\n",
      "current loss at 10421 iteration 0.06113126343537914\n",
      "current loss at 10431 iteration 0.061130907643479754\n",
      "current loss at 10441 iteration 0.06113055258958611\n",
      "current loss at 10451 iteration 0.061130198271438646\n",
      "current loss at 10461 iteration 0.061129844686786915\n",
      "current loss at 10471 iteration 0.061129491833389595\n",
      "current loss at 10481 iteration 0.061129139709014374\n",
      "current loss at 10491 iteration 0.0611287883114379\n",
      "current loss at 10501 iteration 0.061128437638445804\n",
      "current loss at 10511 iteration 0.06112808768783259\n",
      "current loss at 10521 iteration 0.06112773845740163\n",
      "current loss at 10531 iteration 0.06112738994496509\n",
      "current loss at 10541 iteration 0.06112704214834394\n",
      "current loss at 10551 iteration 0.06112669506536782\n",
      "current loss at 10561 iteration 0.061126348693875086\n",
      "current loss at 10571 iteration 0.061126003031712735\n",
      "current loss at 10581 iteration 0.06112565807673631\n",
      "current loss at 10591 iteration 0.06112531382680997\n",
      "current loss at 10601 iteration 0.06112497027980633\n",
      "current loss at 10611 iteration 0.0611246274336065\n",
      "current loss at 10621 iteration 0.061124285286100005\n",
      "current loss at 10631 iteration 0.061123943835184776\n",
      "current loss at 10641 iteration 0.06112360307876704\n",
      "current loss at 10651 iteration 0.061123263014761356\n",
      "current loss at 10661 iteration 0.061122923641090544\n",
      "current loss at 10671 iteration 0.06112258495568564\n",
      "current loss at 10681 iteration 0.061122246956485854\n",
      "current loss at 10691 iteration 0.06112190964143855\n",
      "current loss at 10701 iteration 0.06112157300849915\n",
      "current loss at 10711 iteration 0.0611212370556312\n",
      "current loss at 10721 iteration 0.06112090178080624\n",
      "current loss at 10731 iteration 0.061120567182003746\n",
      "current loss at 10741 iteration 0.0611202332572112\n",
      "current loss at 10751 iteration 0.06111990000442396\n",
      "current loss at 10761 iteration 0.06111956742164526\n",
      "current loss at 10771 iteration 0.061119235506886124\n",
      "current loss at 10781 iteration 0.06111890425816543\n",
      "current loss at 10791 iteration 0.061118573673509737\n",
      "current loss at 10801 iteration 0.06111824375095338\n",
      "current loss at 10811 iteration 0.061117914488538316\n",
      "current loss at 10821 iteration 0.06111758588431418\n",
      "current loss at 10831 iteration 0.06111725793633819\n",
      "current loss at 10841 iteration 0.06111693064267514\n",
      "current loss at 10851 iteration 0.06111660400139735\n",
      "current loss at 10861 iteration 0.06111627801058461\n",
      "current loss at 10871 iteration 0.061115952668324196\n",
      "current loss at 10881 iteration 0.0611156279727108\n",
      "current loss at 10891 iteration 0.06111530392184646\n",
      "current loss at 10901 iteration 0.06111498051384065\n",
      "current loss at 10911 iteration 0.06111465774681007\n",
      "current loss at 10921 iteration 0.061114335618878725\n",
      "current loss at 10931 iteration 0.061114014128177886\n",
      "current loss at 10941 iteration 0.06111369327284602\n",
      "current loss at 10951 iteration 0.06111337305102874\n",
      "current loss at 10961 iteration 0.061113053460878856\n",
      "current loss at 10971 iteration 0.061112734500556244\n",
      "current loss at 10981 iteration 0.06111241616822786\n",
      "current loss at 10991 iteration 0.0611120984620677\n",
      "current loss at 11001 iteration 0.061111781380256774\n",
      "current loss at 11011 iteration 0.061111464920983044\n",
      "current loss at 11021 iteration 0.061111149082441446\n",
      "current loss at 11031 iteration 0.061110833862833766\n",
      "current loss at 11041 iteration 0.06111051926036873\n",
      "current loss at 11051 iteration 0.06111020527326183\n",
      "current loss at 11061 iteration 0.06110989189973545\n",
      "current loss at 11071 iteration 0.06110957913801867\n",
      "current loss at 11081 iteration 0.06110926698634738\n",
      "current loss at 11091 iteration 0.06110895544296413\n",
      "current loss at 11101 iteration 0.06110864450611818\n",
      "current loss at 11111 iteration 0.061108334174065436\n",
      "current loss at 11121 iteration 0.06110802444506839\n",
      "current loss at 11131 iteration 0.06110771531739618\n",
      "current loss at 11141 iteration 0.061107406789324456\n",
      "current loss at 11151 iteration 0.06110709885913538\n",
      "current loss at 11161 iteration 0.061106791525117665\n",
      "current loss at 11171 iteration 0.06110648478556643\n",
      "current loss at 11181 iteration 0.06110617863878325\n",
      "current loss at 11191 iteration 0.06110587308307612\n",
      "current loss at 11201 iteration 0.061105568116759365\n",
      "current loss at 11211 iteration 0.06110526373815371\n",
      "current loss at 11221 iteration 0.06110495994558615\n",
      "current loss at 11231 iteration 0.06110465673738996\n",
      "current loss at 11241 iteration 0.061104354111904735\n",
      "current loss at 11251 iteration 0.061104052067476204\n",
      "current loss at 11261 iteration 0.06110375060245633\n",
      "current loss at 11271 iteration 0.061103449715203303\n",
      "current loss at 11281 iteration 0.06110314940408136\n",
      "current loss at 11291 iteration 0.06110284966746091\n",
      "current loss at 11301 iteration 0.06110255050371841\n",
      "current loss at 11311 iteration 0.0611022519112364\n",
      "current loss at 11321 iteration 0.061101953888403435\n",
      "current loss at 11331 iteration 0.061101656433614045\n",
      "current loss at 11341 iteration 0.0611013595452688\n",
      "current loss at 11351 iteration 0.06110106322177411\n",
      "current loss at 11361 iteration 0.06110076746154241\n",
      "current loss at 11371 iteration 0.06110047226299193\n",
      "current loss at 11381 iteration 0.061100177624546816\n",
      "current loss at 11391 iteration 0.06109988354463707\n",
      "current loss at 11401 iteration 0.061099590021698416\n",
      "current loss at 11411 iteration 0.06109929705417245\n",
      "current loss at 11421 iteration 0.06109900464050649\n",
      "current loss at 11431 iteration 0.06109871277915354\n",
      "current loss at 11441 iteration 0.06109842146857236\n",
      "current loss at 11451 iteration 0.06109813070722739\n",
      "current loss at 11461 iteration 0.06109784049358867\n",
      "current loss at 11471 iteration 0.06109755082613191\n",
      "current loss at 11481 iteration 0.061097261703338415\n",
      "current loss at 11491 iteration 0.06109697312369503\n",
      "current loss at 11501 iteration 0.061096685085694186\n",
      "current loss at 11511 iteration 0.06109639758783382\n",
      "current loss at 11521 iteration 0.061096110628617355\n",
      "current loss at 11531 iteration 0.06109582420655371\n",
      "current loss at 11541 iteration 0.06109553832015725\n",
      "current loss at 11551 iteration 0.06109525296794774\n",
      "current loss at 11561 iteration 0.06109496814845034\n",
      "current loss at 11571 iteration 0.06109468386019565\n",
      "current loss at 11581 iteration 0.06109440010171954\n",
      "current loss at 11591 iteration 0.06109411687156322\n",
      "current loss at 11601 iteration 0.06109383416827327\n",
      "current loss at 11611 iteration 0.061093551990401465\n",
      "current loss at 11621 iteration 0.06109327033650488\n",
      "current loss at 11631 iteration 0.06109298920514578\n",
      "current loss at 11641 iteration 0.06109270859489169\n",
      "current loss at 11651 iteration 0.061092428504315274\n",
      "current loss at 11661 iteration 0.061092148931994404\n",
      "current loss at 11671 iteration 0.06109186987651202\n",
      "current loss at 11681 iteration 0.06109159133645625\n",
      "current loss at 11691 iteration 0.06109131331042027\n",
      "current loss at 11701 iteration 0.06109103579700232\n",
      "current loss at 11711 iteration 0.06109075879480571\n",
      "current loss at 11721 iteration 0.061090482302438756\n",
      "current loss at 11731 iteration 0.06109020631851478\n",
      "current loss at 11741 iteration 0.06108993084165209\n",
      "current loss at 11751 iteration 0.06108965587047394\n",
      "current loss at 11761 iteration 0.061089381403608534\n",
      "current loss at 11771 iteration 0.06108910743968895\n",
      "current loss at 11781 iteration 0.061088833977353194\n",
      "current loss at 11791 iteration 0.06108856101524413\n",
      "current loss at 11801 iteration 0.06108828855200949\n",
      "current loss at 11811 iteration 0.06108801658630178\n",
      "current loss at 11821 iteration 0.06108774511677834\n",
      "current loss at 11831 iteration 0.061087474142101326\n",
      "current loss at 11841 iteration 0.061087203660937596\n",
      "current loss at 11851 iteration 0.06108693367195878\n",
      "current loss at 11861 iteration 0.061086664173841246\n",
      "current loss at 11871 iteration 0.061086395165266004\n",
      "current loss at 11881 iteration 0.06108612664491881\n",
      "current loss at 11891 iteration 0.061085858611490025\n",
      "current loss at 11901 iteration 0.06108559106367469\n",
      "current loss at 11911 iteration 0.06108532400017243\n",
      "current loss at 11921 iteration 0.06108505741968747\n",
      "current loss at 11931 iteration 0.06108479132092862\n",
      "current loss at 11941 iteration 0.06108452570260928\n",
      "current loss at 11951 iteration 0.061084260563447325\n",
      "current loss at 11961 iteration 0.061083995902165164\n",
      "current loss at 11971 iteration 0.06108373171748974\n",
      "current loss at 11981 iteration 0.06108346800815245\n",
      "current loss at 11991 iteration 0.06108320477288912\n",
      "current loss at 12001 iteration 0.06108294201044007\n",
      "current loss at 12011 iteration 0.06108267971955\n",
      "current loss at 12021 iteration 0.061082417898968004\n",
      "current loss at 12031 iteration 0.06108215654744759\n",
      "current loss at 12041 iteration 0.0610818956637466\n",
      "current loss at 12051 iteration 0.061081635246627256\n",
      "current loss at 12061 iteration 0.06108137529485605\n",
      "current loss at 12071 iteration 0.0610811158072038\n",
      "current loss at 12081 iteration 0.06108085678244563\n",
      "current loss at 12091 iteration 0.061080598219360946\n",
      "current loss at 12101 iteration 0.06108034011673333\n",
      "current loss at 12111 iteration 0.061080082473350665\n",
      "current loss at 12121 iteration 0.06107982528800501\n",
      "current loss at 12131 iteration 0.06107956855949267\n",
      "current loss at 12141 iteration 0.06107931228661404\n",
      "current loss at 12151 iteration 0.06107905646817376\n",
      "current loss at 12161 iteration 0.06107880110298055\n",
      "current loss at 12171 iteration 0.06107854618984729\n",
      "current loss at 12181 iteration 0.06107829172759095\n",
      "current loss at 12191 iteration 0.06107803771503258\n",
      "current loss at 12201 iteration 0.06107778415099732\n",
      "current loss at 12211 iteration 0.06107753103431435\n",
      "current loss at 12221 iteration 0.06107727836381687\n",
      "current loss at 12231 iteration 0.06107702613834214\n",
      "current loss at 12241 iteration 0.06107677435673141\n",
      "current loss at 12251 iteration 0.06107652301782987\n",
      "current loss at 12261 iteration 0.061076272120486705\n",
      "current loss at 12271 iteration 0.061076021663555066\n",
      "current loss at 12281 iteration 0.06107577164589203\n",
      "current loss at 12291 iteration 0.06107552206635858\n",
      "current loss at 12301 iteration 0.06107527292381958\n",
      "current loss at 12311 iteration 0.061075024217143864\n",
      "current loss at 12321 iteration 0.061074775945203975\n",
      "current loss at 12331 iteration 0.06107452810687649\n",
      "current loss at 12341 iteration 0.06107428070104166\n",
      "current loss at 12351 iteration 0.06107403372658366\n",
      "current loss at 12361 iteration 0.06107378718239044\n",
      "current loss at 12371 iteration 0.061073541067353694\n",
      "current loss at 12381 iteration 0.06107329538036894\n",
      "current loss at 12391 iteration 0.06107305012033542\n",
      "current loss at 12401 iteration 0.06107280528615612\n",
      "current loss at 12411 iteration 0.06107256087673775\n",
      "current loss at 12421 iteration 0.06107231689099071\n",
      "current loss at 12431 iteration 0.06107207332782914\n",
      "current loss at 12441 iteration 0.061071830186170795\n",
      "current loss at 12451 iteration 0.06107158746493711\n",
      "current loss at 12461 iteration 0.0610713451630532\n",
      "current loss at 12471 iteration 0.06107110327944777\n",
      "current loss at 12481 iteration 0.06107086181305313\n",
      "current loss at 12491 iteration 0.061070620762805214\n",
      "current loss at 12501 iteration 0.06107038012764354\n",
      "current loss at 12511 iteration 0.061070139906511195\n",
      "current loss at 12521 iteration 0.06106990009835479\n",
      "current loss at 12531 iteration 0.06106966070212452\n",
      "current loss at 12541 iteration 0.06106942171677408\n",
      "current loss at 12551 iteration 0.06106918314126067\n",
      "current loss at 12561 iteration 0.061068944974544986\n",
      "current loss at 12571 iteration 0.06106870721559124\n",
      "current loss at 12581 iteration 0.06106846986336703\n",
      "current loss at 12591 iteration 0.06106823291684349\n",
      "current loss at 12601 iteration 0.06106799637499516\n",
      "current loss at 12611 iteration 0.06106776023679999\n",
      "current loss at 12621 iteration 0.06106752450123936\n",
      "current loss at 12631 iteration 0.06106728916729801\n",
      "current loss at 12641 iteration 0.061067054233964115\n",
      "current loss at 12651 iteration 0.06106681970022917\n",
      "current loss at 12661 iteration 0.06106658556508805\n",
      "current loss at 12671 iteration 0.06106635182753893\n",
      "current loss at 12681 iteration 0.06106611848658337\n",
      "current loss at 12691 iteration 0.06106588554122621\n",
      "current loss at 12701 iteration 0.06106565299047557\n",
      "current loss at 12711 iteration 0.061065420833342876\n",
      "current loss at 12721 iteration 0.06106518906884284\n",
      "current loss at 12731 iteration 0.06106495769599339\n",
      "current loss at 12741 iteration 0.06106472671381572\n",
      "current loss at 12751 iteration 0.06106449612133428\n",
      "current loss at 12761 iteration 0.061064265917576695\n",
      "current loss at 12771 iteration 0.06106403610157382\n",
      "current loss at 12781 iteration 0.061063806672359694\n",
      "current loss at 12791 iteration 0.06106357762897152\n",
      "current loss at 12801 iteration 0.0610633489704497\n",
      "current loss at 12811 iteration 0.06106312069583775\n",
      "current loss at 12821 iteration 0.061062892804182346\n",
      "current loss at 12831 iteration 0.06106266529453333\n",
      "current loss at 12841 iteration 0.061062438165943554\n",
      "current loss at 12851 iteration 0.06106221141746907\n",
      "current loss at 12861 iteration 0.06106198504816898\n",
      "current loss at 12871 iteration 0.06106175905710546\n",
      "current loss at 12881 iteration 0.06106153344334375\n",
      "current loss at 12891 iteration 0.06106130820595216\n",
      "current loss at 12901 iteration 0.061061083344002\n",
      "current loss at 12911 iteration 0.06106085885656766\n",
      "current loss at 12921 iteration 0.061060634742726494\n",
      "current loss at 12931 iteration 0.06106041100155887\n",
      "current loss at 12941 iteration 0.06106018763214819\n",
      "current loss at 12951 iteration 0.06105996463358079\n",
      "current loss at 12961 iteration 0.06105974200494597\n",
      "current loss at 12971 iteration 0.061059519745335994\n",
      "current loss at 12981 iteration 0.06105929785384608\n",
      "current loss at 12991 iteration 0.06105907632957435\n",
      "current loss at 13001 iteration 0.06105885517162187\n",
      "current loss at 13011 iteration 0.06105863437909261\n",
      "current loss at 13021 iteration 0.06105841395109341\n",
      "current loss at 13031 iteration 0.06105819388673403\n",
      "current loss at 13041 iteration 0.06105797418512709\n",
      "current loss at 13051 iteration 0.06105775484538805\n",
      "current loss at 13061 iteration 0.061057535866635204\n",
      "current loss at 13071 iteration 0.06105731724798977\n",
      "current loss at 13081 iteration 0.061057098988575684\n",
      "current loss at 13091 iteration 0.06105688108751978\n",
      "current loss at 13101 iteration 0.06105666354395166\n",
      "current loss at 13111 iteration 0.061056446357003705\n",
      "current loss at 13121 iteration 0.061056229525811107\n",
      "current loss at 13131 iteration 0.06105601304951183\n",
      "current loss at 13141 iteration 0.06105579692724653\n",
      "current loss at 13151 iteration 0.06105558115815874\n",
      "current loss at 13161 iteration 0.0610553657413946\n",
      "current loss at 13171 iteration 0.061055150676103044\n",
      "current loss at 13181 iteration 0.06105493596143571\n",
      "current loss at 13191 iteration 0.061054721596546964\n",
      "current loss at 13201 iteration 0.061054507580593796\n",
      "current loss at 13211 iteration 0.061054293912735966\n",
      "current loss at 13221 iteration 0.06105408059213586\n",
      "current loss at 13231 iteration 0.06105386761795853\n",
      "current loss at 13241 iteration 0.06105365498937169\n",
      "current loss at 13251 iteration 0.06105344270554566\n",
      "current loss at 13261 iteration 0.06105323076565345\n",
      "current loss at 13271 iteration 0.061053019168870674\n",
      "current loss at 13281 iteration 0.06105280791437552\n",
      "current loss at 13291 iteration 0.061052597001348796\n",
      "current loss at 13301 iteration 0.061052386428973926\n",
      "current loss at 13311 iteration 0.061052176196436866\n",
      "current loss at 13321 iteration 0.06105196630292621\n",
      "current loss at 13331 iteration 0.06105175674763303\n",
      "current loss at 13341 iteration 0.061051547529751\n",
      "current loss at 13351 iteration 0.06105133864847631\n",
      "current loss at 13361 iteration 0.06105113010300773\n",
      "current loss at 13371 iteration 0.06105092189254646\n",
      "current loss at 13381 iteration 0.0610507140162963\n",
      "current loss at 13391 iteration 0.06105050647346349\n",
      "current loss at 13401 iteration 0.06105029926325678\n",
      "current loss at 13411 iteration 0.06105009238488742\n",
      "current loss at 13421 iteration 0.061049885837569096\n",
      "current loss at 13431 iteration 0.06104967962051798\n",
      "current loss at 13441 iteration 0.0610494737329527\n",
      "current loss at 13451 iteration 0.061049268174094284\n",
      "current loss at 13461 iteration 0.061049062943166255\n",
      "current loss at 13471 iteration 0.0610488580393945\n",
      "current loss at 13481 iteration 0.061048653462007385\n",
      "current loss at 13491 iteration 0.061048449210235604\n",
      "current loss at 13501 iteration 0.06104824528331233\n",
      "current loss at 13511 iteration 0.061048041680473036\n",
      "current loss at 13521 iteration 0.06104783840095566\n",
      "current loss at 13531 iteration 0.06104763544400043\n",
      "current loss at 13541 iteration 0.06104743280884997\n",
      "current loss at 13551 iteration 0.06104723049474926\n",
      "current loss at 13561 iteration 0.061047028500945594\n",
      "current loss at 13571 iteration 0.06104682682668863\n",
      "current loss at 13581 iteration 0.06104662547123033\n",
      "current loss at 13591 iteration 0.06104642443382496\n",
      "current loss at 13601 iteration 0.061046223713729114\n",
      "current loss at 13611 iteration 0.06104602331020167\n",
      "current loss at 13621 iteration 0.06104582322250377\n",
      "current loss at 13631 iteration 0.06104562344989888\n",
      "current loss at 13641 iteration 0.06104542399165273\n",
      "current loss at 13651 iteration 0.06104522484703326\n",
      "current loss at 13661 iteration 0.06104502601531071\n",
      "current loss at 13671 iteration 0.06104482749575754\n",
      "current loss at 13681 iteration 0.0610446292876485\n",
      "current loss at 13691 iteration 0.06104443139026045\n",
      "current loss at 13701 iteration 0.061044233802872605\n",
      "current loss at 13711 iteration 0.06104403652476628\n",
      "current loss at 13721 iteration 0.06104383955522508\n",
      "current loss at 13731 iteration 0.061043642893534714\n",
      "current loss at 13741 iteration 0.06104344653898313\n",
      "current loss at 13751 iteration 0.06104325049086047\n",
      "current loss at 13761 iteration 0.06104305474845898\n",
      "current loss at 13771 iteration 0.061042859311073144\n",
      "current loss at 13781 iteration 0.061042664177999514\n",
      "current loss at 13791 iteration 0.06104246934853687\n",
      "current loss at 13801 iteration 0.061042274821986035\n",
      "current loss at 13811 iteration 0.06104208059765005\n",
      "current loss at 13821 iteration 0.06104188667483401\n",
      "current loss at 13831 iteration 0.061041693052845174\n",
      "current loss at 13841 iteration 0.061041499730992856\n",
      "current loss at 13851 iteration 0.06104130670858849\n",
      "current loss at 13861 iteration 0.06104111398494559\n",
      "current loss at 13871 iteration 0.06104092155937974\n",
      "current loss at 13881 iteration 0.061040729431208626\n",
      "current loss at 13891 iteration 0.06104053759975198\n",
      "current loss at 13901 iteration 0.061040346064331584\n",
      "current loss at 13911 iteration 0.061040154824271275\n",
      "current loss at 13921 iteration 0.06103996387889694\n",
      "current loss at 13931 iteration 0.061039773227536474\n",
      "current loss at 13941 iteration 0.06103958286951981\n",
      "current loss at 13951 iteration 0.061039392804178944\n",
      "current loss at 13961 iteration 0.06103920303084778\n",
      "current loss at 13971 iteration 0.061039013548862334\n",
      "current loss at 13981 iteration 0.06103882435756055\n",
      "current loss at 13991 iteration 0.06103863545628236\n",
      "current loss at 14001 iteration 0.06103844684436974\n",
      "current loss at 14011 iteration 0.06103825852116656\n",
      "current loss at 14021 iteration 0.06103807048601871\n",
      "current loss at 14031 iteration 0.061037882738274006\n",
      "current loss at 14041 iteration 0.061037695277282235\n",
      "current loss at 14051 iteration 0.06103750810239512\n",
      "current loss at 14061 iteration 0.06103732121296632\n",
      "current loss at 14071 iteration 0.06103713460835144\n",
      "current loss at 14081 iteration 0.06103694828790797\n",
      "current loss at 14091 iteration 0.06103676225099535\n",
      "current loss at 14101 iteration 0.06103657649697495\n",
      "current loss at 14111 iteration 0.061036391025209955\n",
      "current loss at 14121 iteration 0.06103620583506551\n",
      "current loss at 14131 iteration 0.06103602092590868\n",
      "current loss at 14141 iteration 0.06103583629710829\n",
      "current loss at 14151 iteration 0.06103565194803519\n",
      "current loss at 14161 iteration 0.061035467878061946\n",
      "current loss at 14171 iteration 0.06103528408656313\n",
      "current loss at 14181 iteration 0.06103510057291503\n",
      "current loss at 14191 iteration 0.061034917336495895\n",
      "current loss at 14201 iteration 0.06103473437668572\n",
      "current loss at 14211 iteration 0.06103455169286639\n",
      "current loss at 14221 iteration 0.061034369284421616\n",
      "current loss at 14231 iteration 0.06103418715073688\n",
      "current loss at 14241 iteration 0.06103400529119954\n",
      "current loss at 14251 iteration 0.061033823705198706\n",
      "current loss at 14261 iteration 0.06103364239212531\n",
      "current loss at 14271 iteration 0.06103346135137208\n",
      "current loss at 14281 iteration 0.06103328058233352\n",
      "current loss at 14291 iteration 0.06103310008440593\n",
      "current loss at 14301 iteration 0.06103291985698735\n",
      "current loss at 14311 iteration 0.06103273989947763\n",
      "current loss at 14321 iteration 0.06103256021127833\n",
      "current loss at 14331 iteration 0.0610323807917928\n",
      "current loss at 14341 iteration 0.061032201640426144\n",
      "current loss at 14351 iteration 0.061032022756585175\n",
      "current loss at 14361 iteration 0.06103184413967847\n",
      "current loss at 14371 iteration 0.06103166578911629\n",
      "current loss at 14381 iteration 0.061031487704310675\n",
      "current loss at 14391 iteration 0.06103130988467534\n",
      "current loss at 14401 iteration 0.061031132329625724\n",
      "current loss at 14411 iteration 0.06103095503857898\n",
      "current loss at 14421 iteration 0.06103077801095394\n",
      "current loss at 14431 iteration 0.061030601246171115\n",
      "current loss at 14441 iteration 0.06103042474365275\n",
      "current loss at 14451 iteration 0.06103024850282271\n",
      "current loss at 14461 iteration 0.06103007252310656\n",
      "current loss at 14471 iteration 0.06102989680393156\n",
      "current loss at 14481 iteration 0.06102972134472658\n",
      "current loss at 14491 iteration 0.06102954614492219\n",
      "current loss at 14501 iteration 0.06102937120395054\n",
      "current loss at 14511 iteration 0.06102919652124553\n",
      "current loss at 14521 iteration 0.06102902209624258\n",
      "current loss at 14531 iteration 0.061028847928378815\n",
      "current loss at 14541 iteration 0.061028674017092985\n",
      "current loss at 14551 iteration 0.06102850036182544\n",
      "current loss at 14561 iteration 0.06102832696201811\n",
      "current loss at 14571 iteration 0.061028153817114586\n",
      "current loss at 14581 iteration 0.06102798092656006\n",
      "current loss at 14591 iteration 0.06102780828980129\n",
      "current loss at 14601 iteration 0.06102763590628662\n",
      "current loss at 14611 iteration 0.061027463775466036\n",
      "current loss at 14621 iteration 0.061027291896791014\n",
      "current loss at 14631 iteration 0.061027120269714685\n",
      "current loss at 14641 iteration 0.06102694889369171\n",
      "current loss at 14651 iteration 0.06102677776817832\n",
      "current loss at 14661 iteration 0.0610266068926323\n",
      "current loss at 14671 iteration 0.061026436266512975\n",
      "current loss at 14681 iteration 0.06102626588928125\n",
      "current loss at 14691 iteration 0.061026095760399524\n",
      "current loss at 14701 iteration 0.06102592587933178\n",
      "current loss at 14711 iteration 0.06102575624554349\n",
      "current loss at 14721 iteration 0.061025586858501665\n",
      "current loss at 14731 iteration 0.061025417717674856\n",
      "current loss at 14741 iteration 0.06102524882253308\n",
      "current loss at 14751 iteration 0.0610250801725479\n",
      "current loss at 14761 iteration 0.061024911767192366\n",
      "current loss at 14771 iteration 0.06102474360594104\n",
      "current loss at 14781 iteration 0.06102457568826996\n",
      "current loss at 14791 iteration 0.06102440801365664\n",
      "current loss at 14801 iteration 0.06102424058158012\n",
      "current loss at 14811 iteration 0.06102407339152086\n",
      "current loss at 14821 iteration 0.06102390644296085\n",
      "current loss at 14831 iteration 0.06102373973538348\n",
      "current loss at 14841 iteration 0.06102357326827365\n",
      "current loss at 14851 iteration 0.061023407041117715\n",
      "current loss at 14861 iteration 0.06102324105340343\n",
      "current loss at 14871 iteration 0.06102307530462008\n",
      "current loss at 14881 iteration 0.0610229097942583\n",
      "current loss at 14891 iteration 0.061022744521810224\n",
      "current loss at 14901 iteration 0.06102257948676938\n",
      "current loss at 14911 iteration 0.06102241468863076\n",
      "current loss at 14921 iteration 0.06102225012689073\n",
      "current loss at 14931 iteration 0.06102208580104709\n",
      "current loss at 14941 iteration 0.06102192171059906\n",
      "current loss at 14951 iteration 0.06102175785504729\n",
      "current loss at 14961 iteration 0.061021594233893736\n",
      "current loss at 14971 iteration 0.061021430846641876\n",
      "current loss at 14981 iteration 0.06102126769279648\n",
      "current loss at 14991 iteration 0.06102110477186373\n",
      "current loss at 15001 iteration 0.06102094208335123\n",
      "current loss at 15011 iteration 0.06102077962676792\n",
      "current loss at 15021 iteration 0.061020617401624136\n",
      "current loss at 15031 iteration 0.061020455407431534\n",
      "current loss at 15041 iteration 0.06102029364370317\n",
      "current loss at 15051 iteration 0.06102013210995346\n",
      "current loss at 15061 iteration 0.061019970805698175\n",
      "current loss at 15071 iteration 0.0610198097304544\n",
      "current loss at 15081 iteration 0.06101964888374061\n",
      "current loss at 15091 iteration 0.061019488265076575\n",
      "current loss at 15101 iteration 0.06101932787398342\n",
      "current loss at 15111 iteration 0.06101916770998363\n",
      "current loss at 15121 iteration 0.06101900777260093\n",
      "current loss at 15131 iteration 0.06101884806136046\n",
      "current loss at 15141 iteration 0.06101868857578861\n",
      "current loss at 15151 iteration 0.061018529315413135\n",
      "current loss at 15161 iteration 0.06101837027976304\n",
      "current loss at 15171 iteration 0.06101821146836867\n",
      "current loss at 15181 iteration 0.06101805288076165\n",
      "current loss at 15191 iteration 0.06101789451647493\n",
      "current loss at 15201 iteration 0.061017736375042694\n",
      "current loss at 15211 iteration 0.06101757845600045\n",
      "current loss at 15221 iteration 0.06101742075888498\n",
      "current loss at 15231 iteration 0.06101726328323435\n",
      "current loss at 15241 iteration 0.06101710602858786\n",
      "current loss at 15251 iteration 0.06101694899448613\n",
      "current loss at 15261 iteration 0.06101679218047099\n",
      "current loss at 15271 iteration 0.06101663558608558\n",
      "current loss at 15281 iteration 0.06101647921087425\n",
      "current loss at 15291 iteration 0.06101632305438262\n",
      "current loss at 15301 iteration 0.06101616711615757\n",
      "current loss at 15311 iteration 0.061016011395747166\n",
      "current loss at 15321 iteration 0.06101585589270079\n",
      "current loss at 15331 iteration 0.061015700606568996\n",
      "current loss at 15341 iteration 0.061015545536903595\n",
      "current loss at 15351 iteration 0.0610153906832576\n",
      "current loss at 15361 iteration 0.06101523604518525\n",
      "current loss at 15371 iteration 0.06101508162224207\n",
      "current loss at 15381 iteration 0.061014927413984656\n",
      "current loss at 15391 iteration 0.06101477341997094\n",
      "current loss at 15401 iteration 0.06101461963976001\n",
      "current loss at 15411 iteration 0.06101446607291213\n",
      "current loss at 15421 iteration 0.06101431271898879\n",
      "current loss at 15431 iteration 0.061014159577552664\n",
      "current loss at 15441 iteration 0.06101400664816763\n",
      "current loss at 15451 iteration 0.06101385393039874\n",
      "current loss at 15461 iteration 0.061013701423812176\n",
      "current loss at 15471 iteration 0.061013549127975374\n",
      "current loss at 15481 iteration 0.06101339704245688\n",
      "current loss at 15491 iteration 0.06101324516682648\n",
      "current loss at 15501 iteration 0.06101309350065506\n",
      "current loss at 15511 iteration 0.06101294204351466\n",
      "current loss at 15521 iteration 0.06101279079497852\n",
      "current loss at 15531 iteration 0.061012639754621\n",
      "current loss at 15541 iteration 0.06101248892201765\n",
      "current loss at 15551 iteration 0.06101233829674513\n",
      "current loss at 15561 iteration 0.061012187878381204\n",
      "current loss at 15571 iteration 0.061012037666504856\n",
      "current loss at 15581 iteration 0.06101188766069616\n",
      "current loss at 15591 iteration 0.06101173786053631\n",
      "current loss at 15601 iteration 0.061011588265607625\n",
      "current loss at 15611 iteration 0.0610114388754936\n",
      "current loss at 15621 iteration 0.06101128968977876\n",
      "current loss at 15631 iteration 0.06101114070804882\n",
      "current loss at 15641 iteration 0.06101099192989054\n",
      "current loss at 15651 iteration 0.061010843354891854\n",
      "current loss at 15661 iteration 0.061010694982641764\n",
      "current loss at 15671 iteration 0.06101054681273036\n",
      "current loss at 15681 iteration 0.06101039884474887\n",
      "current loss at 15691 iteration 0.06101025107828955\n",
      "current loss at 15701 iteration 0.06101010351294581\n",
      "current loss at 15711 iteration 0.06100995614831209\n",
      "current loss at 15721 iteration 0.06100980898398397\n",
      "current loss at 15731 iteration 0.06100966201955803\n",
      "current loss at 15741 iteration 0.06100951525463202\n",
      "current loss at 15751 iteration 0.061009368688804674\n",
      "current loss at 15761 iteration 0.06100922232167583\n",
      "current loss at 15771 iteration 0.061009076152846425\n",
      "current loss at 15781 iteration 0.061008930181918365\n",
      "current loss at 15791 iteration 0.06100878440849473\n",
      "current loss at 15801 iteration 0.061008638832179524\n",
      "current loss at 15811 iteration 0.06100849345257791\n",
      "current loss at 15821 iteration 0.061008348269296055\n",
      "current loss at 15831 iteration 0.061008203281941155\n",
      "current loss at 15841 iteration 0.06100805849012145\n",
      "current loss at 15851 iteration 0.06100791389344624\n",
      "current loss at 15861 iteration 0.06100776949152583\n",
      "current loss at 15871 iteration 0.06100762528397157\n",
      "current loss at 15881 iteration 0.06100748127039582\n",
      "current loss at 15891 iteration 0.061007337450411994\n",
      "current loss at 15901 iteration 0.061007193823634495\n",
      "current loss at 15911 iteration 0.06100705038967875\n",
      "current loss at 15921 iteration 0.061006907148161184\n",
      "current loss at 15931 iteration 0.06100676409869928\n",
      "current loss at 15941 iteration 0.06100662124091146\n",
      "current loss at 15951 iteration 0.06100647857441721\n",
      "current loss at 15961 iteration 0.061006336098836954\n",
      "current loss at 15971 iteration 0.06100619381379218\n",
      "current loss at 15981 iteration 0.0610060517189053\n",
      "current loss at 15991 iteration 0.06100590981379978\n",
      "current loss at 16001 iteration 0.061005768098100016\n",
      "current loss at 16011 iteration 0.061005626571431416\n",
      "current loss at 16021 iteration 0.061005485233420385\n",
      "current loss at 16031 iteration 0.06100534408369424\n",
      "current loss at 16041 iteration 0.06100520312188137\n",
      "current loss at 16051 iteration 0.06100506234761102\n",
      "current loss at 16061 iteration 0.0610049217605135\n",
      "current loss at 16071 iteration 0.06100478136022004\n",
      "current loss at 16081 iteration 0.06100464114636284\n",
      "current loss at 16091 iteration 0.06100450111857503\n",
      "current loss at 16101 iteration 0.06100436127649072\n",
      "current loss at 16111 iteration 0.061004221619744996\n",
      "current loss at 16121 iteration 0.06100408214797385\n",
      "current loss at 16131 iteration 0.06100394286081423\n",
      "current loss at 16141 iteration 0.06100380375790404\n",
      "current loss at 16151 iteration 0.06100366483888212\n",
      "current loss at 16161 iteration 0.061003526103388245\n",
      "current loss at 16171 iteration 0.0610033875510631\n",
      "current loss at 16181 iteration 0.06100324918154833\n",
      "current loss at 16191 iteration 0.061003110994486485\n",
      "current loss at 16201 iteration 0.06100297298952109\n",
      "current loss at 16211 iteration 0.06100283516629652\n",
      "current loss at 16221 iteration 0.06100269752445811\n",
      "current loss at 16231 iteration 0.06100256006365209\n",
      "current loss at 16241 iteration 0.061002422783525634\n",
      "current loss at 16251 iteration 0.0610022856837268\n",
      "current loss at 16261 iteration 0.06100214876390456\n",
      "current loss at 16271 iteration 0.06100201202370877\n",
      "current loss at 16281 iteration 0.061001875462790234\n",
      "current loss at 16291 iteration 0.06100173908080061\n",
      "current loss at 16301 iteration 0.06100160287739246\n",
      "current loss at 16311 iteration 0.061001466852219265\n",
      "current loss at 16321 iteration 0.06100133100493537\n",
      "current loss at 16331 iteration 0.061001195335195976\n",
      "current loss at 16341 iteration 0.06100105984265726\n",
      "current loss at 16351 iteration 0.061000924526976176\n",
      "current loss at 16361 iteration 0.06100078938781063\n",
      "current loss at 16371 iteration 0.061000654424819375\n",
      "current loss at 16381 iteration 0.06100051963766204\n",
      "current loss at 16391 iteration 0.06100038502599911\n",
      "current loss at 16401 iteration 0.061000250589491954\n",
      "current loss at 16411 iteration 0.06100011632780279\n",
      "current loss at 16421 iteration 0.06099998224059474\n",
      "current loss at 16431 iteration 0.060999848327531726\n",
      "current loss at 16441 iteration 0.06099971458827856\n",
      "current loss at 16451 iteration 0.06099958102250089\n",
      "current loss at 16461 iteration 0.06099944762986526\n",
      "current loss at 16471 iteration 0.06099931441003898\n",
      "current loss at 16481 iteration 0.06099918136269029\n",
      "current loss at 16491 iteration 0.06099904848748821\n",
      "current loss at 16501 iteration 0.06099891578410263\n",
      "current loss at 16511 iteration 0.060998783252204286\n",
      "current loss at 16521 iteration 0.06099865089146472\n",
      "current loss at 16531 iteration 0.06099851870155634\n",
      "current loss at 16541 iteration 0.06099838668215234\n",
      "current loss at 16551 iteration 0.060998254832926785\n",
      "current loss at 16561 iteration 0.06099812315355454\n",
      "current loss at 16571 iteration 0.06099799164371129\n",
      "current loss at 16581 iteration 0.06099786030307355\n",
      "current loss at 16591 iteration 0.06099772913131864\n",
      "current loss at 16601 iteration 0.06099759812812469\n",
      "current loss at 16611 iteration 0.06099746729317067\n",
      "current loss at 16621 iteration 0.06099733662613635\n",
      "current loss at 16631 iteration 0.06099720612670229\n",
      "current loss at 16641 iteration 0.06099707579454986\n",
      "current loss at 16651 iteration 0.06099694562936123\n",
      "current loss at 16661 iteration 0.06099681563081935\n",
      "current loss at 16671 iteration 0.06099668579860803\n",
      "current loss at 16681 iteration 0.06099655613241179\n",
      "current loss at 16691 iteration 0.06099642663191602\n",
      "current loss at 16701 iteration 0.06099629729680683\n",
      "current loss at 16711 iteration 0.06099616812677116\n",
      "current loss at 16721 iteration 0.060996039121496724\n",
      "current loss at 16731 iteration 0.06099591028067201\n",
      "current loss at 16741 iteration 0.0609957816039863\n",
      "current loss at 16751 iteration 0.06099565309112963\n",
      "current loss at 16761 iteration 0.06099552474179283\n",
      "current loss at 16771 iteration 0.060995396555667485\n",
      "current loss at 16781 iteration 0.06099526853244598\n",
      "current loss at 16791 iteration 0.06099514067182142\n",
      "current loss at 16801 iteration 0.06099501297348772\n",
      "current loss at 16811 iteration 0.06099488543713955\n",
      "current loss at 16821 iteration 0.06099475806247231\n",
      "current loss at 16831 iteration 0.060994630849182156\n",
      "current loss at 16841 iteration 0.06099450379696606\n",
      "current loss at 16851 iteration 0.060994376905521674\n",
      "current loss at 16861 iteration 0.06099425017454745\n",
      "current loss at 16871 iteration 0.06099412360374258\n",
      "current loss at 16881 iteration 0.06099399719280696\n",
      "current loss at 16891 iteration 0.06099387094144129\n",
      "current loss at 16901 iteration 0.06099374484934698\n",
      "current loss at 16911 iteration 0.06099361891622617\n",
      "current loss at 16921 iteration 0.06099349314178177\n",
      "current loss at 16931 iteration 0.06099336752571741\n",
      "current loss at 16941 iteration 0.0609932420677374\n",
      "current loss at 16951 iteration 0.06099311676754685\n",
      "current loss at 16961 iteration 0.06099299162485161\n",
      "current loss at 16971 iteration 0.0609928666393582\n",
      "current loss at 16981 iteration 0.060992741810773854\n",
      "current loss at 16991 iteration 0.060992617138806576\n",
      "current loss at 17001 iteration 0.0609924926231651\n",
      "current loss at 17011 iteration 0.06099236826355878\n",
      "current loss at 17021 iteration 0.06099224405969783\n",
      "current loss at 17031 iteration 0.06099212001129303\n",
      "current loss at 17041 iteration 0.060991996118055974\n",
      "current loss at 17051 iteration 0.06099187237969893\n",
      "current loss at 17061 iteration 0.060991748795934844\n",
      "current loss at 17071 iteration 0.060991625366477425\n",
      "current loss at 17081 iteration 0.06099150209104101\n",
      "current loss at 17091 iteration 0.06099137896934069\n",
      "current loss at 17101 iteration 0.060991256001092266\n",
      "current loss at 17111 iteration 0.060991133186012174\n",
      "current loss at 17121 iteration 0.0609910105238176\n",
      "current loss at 17131 iteration 0.06099088801422636\n",
      "current loss at 17141 iteration 0.06099076565695705\n",
      "current loss at 17151 iteration 0.06099064345172884\n",
      "current loss at 17161 iteration 0.06099052139826168\n",
      "current loss at 17171 iteration 0.06099039949627616\n",
      "current loss at 17181 iteration 0.060990277745493544\n",
      "current loss at 17191 iteration 0.060990156145635806\n",
      "current loss at 17201 iteration 0.06099003469642555\n",
      "current loss at 17211 iteration 0.06098991339758611\n",
      "current loss at 17221 iteration 0.060989792248841455\n",
      "current loss at 17231 iteration 0.060989671249916215\n",
      "current loss at 17241 iteration 0.060989550400535726\n",
      "current loss at 17251 iteration 0.06098942970042598\n",
      "current loss at 17261 iteration 0.06098930914931359\n",
      "current loss at 17271 iteration 0.06098918874692589\n",
      "current loss at 17281 iteration 0.06098906849299086\n",
      "current loss at 17291 iteration 0.060988948387237105\n",
      "current loss at 17301 iteration 0.06098882842939392\n",
      "current loss at 17311 iteration 0.06098870861919123\n",
      "current loss at 17321 iteration 0.06098858895635964\n",
      "current loss at 17331 iteration 0.0609884694406304\n",
      "current loss at 17341 iteration 0.060988350071735385\n",
      "current loss at 17351 iteration 0.060988230849407125\n",
      "current loss at 17361 iteration 0.0609881117733788\n",
      "current loss at 17371 iteration 0.06098799284338426\n",
      "current loss at 17381 iteration 0.060987874059157954\n",
      "current loss at 17391 iteration 0.06098775542043498\n",
      "current loss at 17401 iteration 0.06098763692695108\n",
      "current loss at 17411 iteration 0.060987518578442645\n",
      "current loss at 17421 iteration 0.06098740037464666\n",
      "current loss at 17431 iteration 0.06098728231530077\n",
      "current loss at 17441 iteration 0.06098716440014326\n",
      "current loss at 17451 iteration 0.060987046628913015\n",
      "current loss at 17461 iteration 0.060986929001349534\n",
      "current loss at 17471 iteration 0.060986811517193\n",
      "current loss at 17481 iteration 0.06098669417618416\n",
      "current loss at 17491 iteration 0.06098657697806441\n",
      "current loss at 17501 iteration 0.06098645992257574\n",
      "current loss at 17511 iteration 0.060986343009460785\n",
      "current loss at 17521 iteration 0.06098622623846279\n",
      "current loss at 17531 iteration 0.06098610960932557\n",
      "current loss at 17541 iteration 0.06098599312179363\n",
      "current loss at 17551 iteration 0.06098587677561202\n",
      "current loss at 17561 iteration 0.060985760570526404\n",
      "current loss at 17571 iteration 0.06098564450628308\n",
      "current loss at 17581 iteration 0.06098552858262892\n",
      "current loss at 17591 iteration 0.060985412799311405\n",
      "current loss at 17601 iteration 0.06098529715607865\n",
      "current loss at 17611 iteration 0.06098518165267933\n",
      "current loss at 17621 iteration 0.060985066288862716\n",
      "current loss at 17631 iteration 0.06098495106437867\n",
      "current loss at 17641 iteration 0.06098483597897771\n",
      "current loss at 17651 iteration 0.060984721032410864\n",
      "current loss at 17661 iteration 0.06098460622442979\n",
      "current loss at 17671 iteration 0.06098449155478672\n",
      "current loss at 17681 iteration 0.06098437702323448\n",
      "current loss at 17691 iteration 0.0609842626295265\n",
      "current loss at 17701 iteration 0.06098414837341675\n",
      "current loss at 17711 iteration 0.060984034254659814\n",
      "current loss at 17721 iteration 0.06098392027301083\n",
      "current loss at 17731 iteration 0.06098380642822554\n",
      "current loss at 17741 iteration 0.06098369272006024\n",
      "current loss at 17751 iteration 0.06098357914827181\n",
      "current loss at 17761 iteration 0.060983465712617715\n",
      "current loss at 17771 iteration 0.06098335241285597\n",
      "current loss at 17781 iteration 0.06098323924874517\n",
      "current loss at 17791 iteration 0.06098312622004446\n",
      "current loss at 17801 iteration 0.06098301332651358\n",
      "current loss at 17811 iteration 0.060982900567912814\n",
      "current loss at 17821 iteration 0.06098278794400301\n",
      "current loss at 17831 iteration 0.060982675454545596\n",
      "current loss at 17841 iteration 0.060982563099302535\n",
      "current loss at 17851 iteration 0.060982450878036346\n",
      "current loss at 17861 iteration 0.060982338790510134\n",
      "current loss at 17871 iteration 0.06098222683648751\n",
      "current loss at 17881 iteration 0.06098211501573271\n",
      "current loss at 17891 iteration 0.06098200332801043\n",
      "current loss at 17901 iteration 0.060981891773085994\n",
      "current loss at 17911 iteration 0.06098178035072525\n",
      "current loss at 17921 iteration 0.06098166906069455\n",
      "current loss at 17931 iteration 0.06098155790276086\n",
      "current loss at 17941 iteration 0.06098144687669164\n",
      "current loss at 17951 iteration 0.06098133598225489\n",
      "current loss at 17961 iteration 0.060981225219219226\n",
      "current loss at 17971 iteration 0.06098111458735368\n",
      "current loss at 17981 iteration 0.0609810040864279\n",
      "current loss at 17991 iteration 0.0609808937162121\n",
      "current loss at 18001 iteration 0.0609807834764769\n",
      "current loss at 18011 iteration 0.060980673366993615\n",
      "current loss at 18021 iteration 0.06098056338753396\n",
      "current loss at 18031 iteration 0.060980453537870245\n",
      "current loss at 18041 iteration 0.0609803438177753\n",
      "current loss at 18051 iteration 0.060980234227022444\n",
      "current loss at 18061 iteration 0.06098012476538558\n",
      "current loss at 18071 iteration 0.06098001543263909\n",
      "current loss at 18081 iteration 0.0609799062285579\n",
      "current loss at 18091 iteration 0.06097979715291743\n",
      "current loss at 18101 iteration 0.06097968820549366\n",
      "current loss at 18111 iteration 0.060979579386063056\n",
      "current loss at 18121 iteration 0.060979470694402585\n",
      "current loss at 18131 iteration 0.06097936213028979\n",
      "current loss at 18141 iteration 0.060979253693502655\n",
      "current loss at 18151 iteration 0.060979145383819704\n",
      "current loss at 18161 iteration 0.06097903720102001\n",
      "current loss at 18171 iteration 0.06097892914488307\n",
      "current loss at 18181 iteration 0.06097882121518899\n",
      "current loss at 18191 iteration 0.060978713411718295\n",
      "current loss at 18201 iteration 0.06097860573425203\n",
      "current loss at 18211 iteration 0.060978498182571804\n",
      "current loss at 18221 iteration 0.060978390756459645\n",
      "current loss at 18231 iteration 0.06097828345569815\n",
      "current loss at 18241 iteration 0.06097817628007036\n",
      "current loss at 18251 iteration 0.060978069229359826\n",
      "current loss at 18261 iteration 0.06097796230335063\n",
      "current loss at 18271 iteration 0.0609778555018273\n",
      "current loss at 18281 iteration 0.0609777488245749\n",
      "current loss at 18291 iteration 0.06097764227137895\n",
      "current loss at 18301 iteration 0.060977535842025476\n",
      "current loss at 18311 iteration 0.060977429536300996\n",
      "current loss at 18321 iteration 0.06097732335399248\n",
      "current loss at 18331 iteration 0.06097721729488744\n",
      "current loss at 18341 iteration 0.06097711135877385\n",
      "current loss at 18351 iteration 0.06097700554544015\n",
      "current loss at 18361 iteration 0.060976899854675276\n",
      "current loss at 18371 iteration 0.06097679428626863\n",
      "current loss at 18381 iteration 0.060976688840010125\n",
      "current loss at 18391 iteration 0.0609765835156901\n",
      "current loss at 18401 iteration 0.06097647831309944\n",
      "current loss at 18411 iteration 0.06097637323202944\n",
      "current loss at 18421 iteration 0.0609762682722719\n",
      "current loss at 18431 iteration 0.060976163433619085\n",
      "current loss at 18441 iteration 0.06097605871586373\n",
      "current loss at 18451 iteration 0.060975954118799046\n",
      "current loss at 18461 iteration 0.06097584964221872\n",
      "current loss at 18471 iteration 0.06097574528591687\n",
      "current loss at 18481 iteration 0.06097564104968811\n",
      "current loss at 18491 iteration 0.060975536933327516\n",
      "current loss at 18501 iteration 0.06097543293663062\n",
      "current loss at 18511 iteration 0.060975329059393416\n",
      "current loss at 18521 iteration 0.060975225301412354\n",
      "current loss at 18531 iteration 0.06097512166248437\n",
      "current loss at 18541 iteration 0.060975018142406814\n",
      "current loss at 18551 iteration 0.06097491474097753\n",
      "current loss at 18561 iteration 0.0609748114579948\n",
      "current loss at 18571 iteration 0.06097470829325734\n",
      "current loss at 18581 iteration 0.06097460524656437\n",
      "current loss at 18591 iteration 0.06097450231771552\n",
      "current loss at 18601 iteration 0.060974399506510873\n",
      "current loss at 18611 iteration 0.060974296812751\n",
      "current loss at 18621 iteration 0.06097419423623686\n",
      "current loss at 18631 iteration 0.060974091776769886\n",
      "current loss at 18641 iteration 0.06097398943415197\n",
      "current loss at 18651 iteration 0.060973887208185445\n",
      "current loss at 18661 iteration 0.06097378509867305\n",
      "current loss at 18671 iteration 0.060973683105418015\n",
      "current loss at 18681 iteration 0.06097358122822399\n",
      "current loss at 18691 iteration 0.06097347946689505\n",
      "current loss at 18701 iteration 0.060973377821235714\n",
      "current loss at 18711 iteration 0.060973276291050954\n",
      "current loss at 18721 iteration 0.060973174876146174\n",
      "current loss at 18731 iteration 0.06097307357632718\n",
      "current loss at 18741 iteration 0.06097297239140027\n",
      "current loss at 18751 iteration 0.060972871321172094\n",
      "current loss at 18761 iteration 0.06097277036544982\n",
      "current loss at 18771 iteration 0.06097266952404098\n",
      "current loss at 18781 iteration 0.06097256879675355\n",
      "current loss at 18791 iteration 0.06097246818339595\n",
      "current loss at 18801 iteration 0.06097236768377702\n",
      "current loss at 18811 iteration 0.06097226729770602\n",
      "current loss at 18821 iteration 0.06097216702499261\n",
      "current loss at 18831 iteration 0.060972066865446935\n",
      "current loss at 18841 iteration 0.060971966818879494\n",
      "current loss at 18851 iteration 0.06097186688510124\n",
      "current loss at 18861 iteration 0.06097176706392352\n",
      "current loss at 18871 iteration 0.06097166735515816\n",
      "current loss at 18881 iteration 0.06097156775861731\n",
      "current loss at 18891 iteration 0.06097146827411362\n",
      "current loss at 18901 iteration 0.06097136890146009\n",
      "current loss at 18911 iteration 0.0609712696404702\n",
      "current loss at 18921 iteration 0.060971170490957766\n",
      "current loss at 18931 iteration 0.06097107145273707\n",
      "current loss at 18941 iteration 0.06097097252562275\n",
      "current loss at 18951 iteration 0.06097087370942997\n",
      "current loss at 18961 iteration 0.06097077500397414\n",
      "current loss at 18971 iteration 0.060970676409071195\n",
      "current loss at 18981 iteration 0.0609705779245374\n",
      "current loss at 18991 iteration 0.06097047955018953\n",
      "current loss at 19001 iteration 0.060970381285844616\n",
      "current loss at 19011 iteration 0.0609702831313202\n",
      "current loss at 19021 iteration 0.06097018508643421\n",
      "current loss at 19031 iteration 0.06097008715100492\n",
      "current loss at 19041 iteration 0.06096998932485106\n",
      "current loss at 19051 iteration 0.060969891607791746\n",
      "current loss at 19061 iteration 0.06096979399964645\n",
      "current loss at 19071 iteration 0.06096969650023509\n",
      "current loss at 19081 iteration 0.060969599109377964\n",
      "current loss at 19091 iteration 0.06096950182689573\n",
      "current loss at 19101 iteration 0.0609694046526095\n",
      "current loss at 19111 iteration 0.060969307586340706\n",
      "current loss at 19121 iteration 0.06096921062791124\n",
      "current loss at 19131 iteration 0.06096911377714333\n",
      "current loss at 19141 iteration 0.06096901703385961\n",
      "current loss at 19151 iteration 0.060968920397883104\n",
      "current loss at 19161 iteration 0.06096882386903722\n",
      "current loss at 19171 iteration 0.06096872744714575\n",
      "current loss at 19181 iteration 0.060968631132032866\n",
      "current loss at 19191 iteration 0.06096853492352314\n",
      "current loss at 19201 iteration 0.060968438821441505\n",
      "current loss at 19211 iteration 0.06096834282561327\n",
      "current loss at 19221 iteration 0.06096824693586414\n",
      "current loss at 19231 iteration 0.06096815115202018\n",
      "current loss at 19241 iteration 0.060968055473907885\n",
      "current loss at 19251 iteration 0.06096795990135406\n",
      "current loss at 19261 iteration 0.06096786443418592\n",
      "current loss at 19271 iteration 0.06096776907223103\n",
      "current loss at 19281 iteration 0.06096767381531736\n",
      "current loss at 19291 iteration 0.06096757866327324\n",
      "current loss at 19301 iteration 0.060967483615927374\n",
      "current loss at 19311 iteration 0.060967388673108816\n",
      "current loss at 19321 iteration 0.06096729383464702\n",
      "current loss at 19331 iteration 0.06096719910037175\n",
      "current loss at 19341 iteration 0.060967104470113255\n",
      "current loss at 19351 iteration 0.06096700994370203\n",
      "current loss at 19361 iteration 0.06096691552096897\n",
      "current loss at 19371 iteration 0.06096682120174538\n",
      "current loss at 19381 iteration 0.06096672698586288\n",
      "current loss at 19391 iteration 0.060966632873153466\n",
      "current loss at 19401 iteration 0.06096653886344949\n",
      "current loss at 19411 iteration 0.0609664449565837\n",
      "current loss at 19421 iteration 0.06096635115238913\n",
      "current loss at 19431 iteration 0.06096625745069925\n",
      "current loss at 19441 iteration 0.06096616385134785\n",
      "current loss at 19451 iteration 0.06096607035416907\n",
      "current loss at 19461 iteration 0.06096597695899745\n",
      "current loss at 19471 iteration 0.06096588366566781\n",
      "current loss at 19481 iteration 0.06096579047401539\n",
      "current loss at 19491 iteration 0.06096569738387576\n",
      "current loss at 19501 iteration 0.06096560439508483\n",
      "current loss at 19511 iteration 0.06096551150747888\n",
      "current loss at 19521 iteration 0.06096541872089453\n",
      "current loss at 19531 iteration 0.06096532603516877\n",
      "current loss at 19541 iteration 0.060965233450138885\n",
      "current loss at 19551 iteration 0.060965140965642574\n",
      "current loss at 19561 iteration 0.06096504858151783\n",
      "current loss at 19571 iteration 0.06096495629760304\n",
      "current loss at 19581 iteration 0.06096486411373687\n",
      "current loss at 19591 iteration 0.0609647720297584\n",
      "current loss at 19601 iteration 0.060964680045507\n",
      "current loss at 19611 iteration 0.060964588160822425\n",
      "current loss at 19621 iteration 0.06096449637554471\n",
      "current loss at 19631 iteration 0.060964404689514314\n",
      "current loss at 19641 iteration 0.06096431310257197\n",
      "current loss at 19651 iteration 0.06096422161455876\n",
      "current loss at 19661 iteration 0.06096413022531612\n",
      "current loss at 19671 iteration 0.06096403893468581\n",
      "current loss at 19681 iteration 0.060963947742509954\n",
      "current loss at 19691 iteration 0.06096385664863096\n",
      "current loss at 19701 iteration 0.0609637656528916\n",
      "current loss at 19711 iteration 0.06096367475513498\n",
      "current loss at 19721 iteration 0.06096358395520454\n",
      "current loss at 19731 iteration 0.06096349325294405\n",
      "current loss at 19741 iteration 0.06096340264819758\n",
      "current loss at 19751 iteration 0.06096331214080957\n",
      "current loss at 19761 iteration 0.06096322173062479\n",
      "current loss at 19771 iteration 0.0609631314174883\n",
      "current loss at 19781 iteration 0.06096304120124553\n",
      "current loss at 19791 iteration 0.06096295108174216\n",
      "current loss at 19801 iteration 0.06096286105882433\n",
      "current loss at 19811 iteration 0.060962771132338354\n",
      "current loss at 19821 iteration 0.06096268130213097\n",
      "current loss at 19831 iteration 0.06096259156804921\n",
      "current loss at 19841 iteration 0.060962501929940426\n",
      "current loss at 19851 iteration 0.060962412387652286\n",
      "current loss at 19861 iteration 0.060962322941032765\n",
      "current loss at 19871 iteration 0.0609622335899302\n",
      "current loss at 19881 iteration 0.06096214433419321\n",
      "current loss at 19891 iteration 0.06096205517367074\n",
      "current loss at 19901 iteration 0.060961966108212064\n",
      "current loss at 19911 iteration 0.060961877137666765\n",
      "current loss at 19921 iteration 0.06096178826188471\n",
      "current loss at 19931 iteration 0.06096169948071614\n",
      "current loss at 19941 iteration 0.06096161079401157\n",
      "current loss at 19951 iteration 0.06096152220162184\n",
      "current loss at 19961 iteration 0.06096143370339809\n",
      "current loss at 19971 iteration 0.06096134529919176\n",
      "current loss at 19981 iteration 0.06096125698885467\n",
      "current loss at 19991 iteration 0.06096116877223886\n",
      "current loss at 20001 iteration 0.06096108064919671\n",
      "current loss at 20011 iteration 0.060960992619580955\n",
      "current loss at 20021 iteration 0.06096090468324454\n",
      "current loss at 20031 iteration 0.06096081684004083\n",
      "current loss at 20041 iteration 0.06096072908982339\n",
      "current loss at 20051 iteration 0.060960641432446186\n",
      "current loss at 20061 iteration 0.06096055386776341\n",
      "current loss at 20071 iteration 0.060960466395629576\n",
      "current loss at 20081 iteration 0.06096037901589954\n",
      "current loss at 20091 iteration 0.060960291728428415\n",
      "current loss at 20101 iteration 0.06096020453307164\n",
      "current loss at 20111 iteration 0.06096011742968494\n",
      "current loss at 20121 iteration 0.06096003041812433\n",
      "current loss at 20131 iteration 0.06095994349824617\n",
      "current loss at 20141 iteration 0.060959856669907046\n",
      "current loss at 20151 iteration 0.06095976993296391\n",
      "current loss at 20161 iteration 0.06095968328727394\n",
      "current loss at 20171 iteration 0.060959596732694715\n",
      "current loss at 20181 iteration 0.060959510269083986\n",
      "current loss at 20191 iteration 0.06095942389629986\n",
      "current loss at 20201 iteration 0.06095933761420077\n",
      "current loss at 20211 iteration 0.06095925142264534\n",
      "current loss at 20221 iteration 0.06095916532149262\n",
      "current loss at 20231 iteration 0.060959079310601814\n",
      "current loss at 20241 iteration 0.06095899338983252\n",
      "current loss at 20251 iteration 0.060958907559044574\n",
      "current loss at 20261 iteration 0.06095882181809812\n",
      "current loss at 20271 iteration 0.06095873616685358\n",
      "current loss at 20281 iteration 0.060958650605171666\n",
      "current loss at 20291 iteration 0.060958565132913374\n",
      "current loss at 20301 iteration 0.06095847974993999\n",
      "current loss at 20311 iteration 0.06095839445611308\n",
      "current loss at 20321 iteration 0.06095830925129448\n",
      "current loss at 20331 iteration 0.060958224135346364\n",
      "current loss at 20341 iteration 0.06095813910813113\n",
      "current loss at 20351 iteration 0.06095805416951147\n",
      "current loss at 20361 iteration 0.060957969319350394\n",
      "current loss at 20371 iteration 0.06095788455751115\n",
      "current loss at 20381 iteration 0.060957799883857265\n",
      "current loss at 20391 iteration 0.06095771529825259\n",
      "current loss at 20401 iteration 0.060957630800561206\n",
      "current loss at 20411 iteration 0.060957546390647495\n",
      "current loss at 20421 iteration 0.06095746206837611\n",
      "current loss at 20431 iteration 0.06095737783361199\n",
      "current loss at 20441 iteration 0.06095729368622035\n",
      "current loss at 20451 iteration 0.06095720962606664\n",
      "current loss at 20461 iteration 0.06095712565301665\n",
      "current loss at 20471 iteration 0.06095704176693639\n",
      "current loss at 20481 iteration 0.06095695796769218\n",
      "current loss at 20491 iteration 0.06095687425515059\n",
      "current loss at 20501 iteration 0.06095679062917844\n",
      "current loss at 20511 iteration 0.06095670708964288\n",
      "current loss at 20521 iteration 0.060956623636411286\n",
      "current loss at 20531 iteration 0.060956540269351316\n",
      "current loss at 20541 iteration 0.06095645698833089\n",
      "current loss at 20551 iteration 0.060956373793218215\n",
      "current loss at 20561 iteration 0.060956290683881725\n",
      "current loss at 20571 iteration 0.060956207660190165\n",
      "current loss at 20581 iteration 0.060956124722012534\n",
      "current loss at 20591 iteration 0.06095604186921809\n",
      "current loss at 20601 iteration 0.06095595910167633\n",
      "current loss at 20611 iteration 0.06095587641925707\n",
      "current loss at 20621 iteration 0.060955793821830376\n",
      "current loss at 20631 iteration 0.060955711309266504\n",
      "current loss at 20641 iteration 0.06095562888143607\n",
      "current loss at 20651 iteration 0.06095554653820992\n",
      "current loss at 20661 iteration 0.060955464279459116\n",
      "current loss at 20671 iteration 0.06095538210505505\n",
      "current loss at 20681 iteration 0.06095530001486931\n",
      "current loss at 20691 iteration 0.06095521800877379\n",
      "current loss at 20701 iteration 0.06095513608664063\n",
      "current loss at 20711 iteration 0.060955054248342196\n",
      "current loss at 20721 iteration 0.06095497249375116\n",
      "current loss at 20731 iteration 0.06095489082274043\n",
      "current loss at 20741 iteration 0.06095480923518312\n",
      "current loss at 20751 iteration 0.0609547277309527\n",
      "current loss at 20761 iteration 0.060954646309922804\n",
      "current loss at 20771 iteration 0.0609545649719674\n",
      "current loss at 20781 iteration 0.0609544837169606\n",
      "current loss at 20791 iteration 0.060954402544776874\n",
      "current loss at 20801 iteration 0.06095432145529088\n",
      "current loss at 20811 iteration 0.06095424044837757\n",
      "current loss at 20821 iteration 0.060954159523912116\n",
      "current loss at 20831 iteration 0.06095407868176993\n",
      "current loss at 20841 iteration 0.06095399792182673\n",
      "current loss at 20851 iteration 0.06095391724395841\n",
      "current loss at 20861 iteration 0.06095383664804117\n",
      "current loss at 20871 iteration 0.060953756133951424\n",
      "current loss at 20881 iteration 0.060953675701565846\n",
      "current loss at 20891 iteration 0.06095359535076136\n",
      "current loss at 20901 iteration 0.0609535150814151\n",
      "current loss at 20911 iteration 0.06095343489340451\n",
      "current loss at 20921 iteration 0.06095335478660721\n",
      "current loss at 20931 iteration 0.06095327476090113\n",
      "current loss at 20941 iteration 0.06095319481616438\n",
      "current loss at 20951 iteration 0.06095311495227535\n",
      "current loss at 20961 iteration 0.060953035169112685\n",
      "current loss at 20971 iteration 0.0609529554665552\n",
      "current loss at 20981 iteration 0.06095287584448204\n",
      "current loss at 20991 iteration 0.06095279630277254\n",
      "current loss at 21001 iteration 0.060952716841306286\n",
      "current loss at 21011 iteration 0.06095263745996309\n",
      "current loss at 21021 iteration 0.06095255815862303\n",
      "current loss at 21031 iteration 0.060952478937166404\n",
      "current loss at 21041 iteration 0.06095239979547376\n",
      "current loss at 21051 iteration 0.06095232073342584\n",
      "current loss at 21061 iteration 0.06095224175090366\n",
      "current loss at 21071 iteration 0.06095216284778848\n",
      "current loss at 21081 iteration 0.06095208402396178\n",
      "current loss at 21091 iteration 0.06095200527930527\n",
      "current loss at 21101 iteration 0.06095192661370089\n",
      "current loss at 21111 iteration 0.06095184802703083\n",
      "current loss at 21121 iteration 0.0609517695191775\n",
      "current loss at 21131 iteration 0.060951691090023546\n",
      "current loss at 21141 iteration 0.06095161273945185\n",
      "current loss at 21151 iteration 0.060951534467345504\n",
      "current loss at 21161 iteration 0.06095145627358786\n",
      "current loss at 21171 iteration 0.06095137815806248\n",
      "current loss at 21181 iteration 0.06095130012065316\n",
      "current loss at 21191 iteration 0.060951222161243934\n",
      "current loss at 21201 iteration 0.06095114427971905\n",
      "current loss at 21211 iteration 0.06095106647596297\n",
      "current loss at 21221 iteration 0.06095098874986044\n",
      "current loss at 21231 iteration 0.06095091110129636\n",
      "current loss at 21241 iteration 0.06095083353015591\n",
      "current loss at 21251 iteration 0.060950756036324466\n",
      "current loss at 21261 iteration 0.06095067861968764\n",
      "current loss at 21271 iteration 0.06095060128013127\n",
      "current loss at 21281 iteration 0.060950524017541405\n",
      "current loss at 21291 iteration 0.06095044683180431\n",
      "current loss at 21301 iteration 0.06095036972280654\n",
      "current loss at 21311 iteration 0.06095029269043476\n",
      "current loss at 21321 iteration 0.06095021573457596\n",
      "current loss at 21331 iteration 0.06095013885511729\n",
      "current loss at 21341 iteration 0.060950062051946134\n",
      "current loss at 21351 iteration 0.06094998532495012\n",
      "current loss at 21361 iteration 0.060949908674017034\n",
      "current loss at 21371 iteration 0.06094983209903496\n",
      "current loss at 21381 iteration 0.06094975559989216\n",
      "current loss at 21391 iteration 0.06094967917647709\n",
      "current loss at 21401 iteration 0.060949602828678465\n",
      "current loss at 21411 iteration 0.060949526556385206\n",
      "current loss at 21421 iteration 0.06094945035948643\n",
      "current loss at 21431 iteration 0.06094937423787147\n",
      "current loss at 21441 iteration 0.06094929819142995\n",
      "current loss at 21451 iteration 0.06094922222005158\n",
      "current loss at 21461 iteration 0.06094914632362637\n",
      "current loss at 21471 iteration 0.06094907050204454\n",
      "current loss at 21481 iteration 0.060948994755196516\n",
      "current loss at 21491 iteration 0.06094891908297289\n",
      "current loss at 21501 iteration 0.06094884348526451\n",
      "current loss at 21511 iteration 0.06094876796196247\n",
      "current loss at 21521 iteration 0.060948692512957994\n",
      "current loss at 21531 iteration 0.06094861713814256\n",
      "current loss at 21541 iteration 0.06094854183740788\n",
      "current loss at 21551 iteration 0.06094846661064582\n",
      "current loss at 21561 iteration 0.06094839145774849\n",
      "current loss at 21571 iteration 0.0609483163786082\n",
      "current loss at 21581 iteration 0.06094824137311748\n",
      "current loss at 21591 iteration 0.06094816644116906\n",
      "current loss at 21601 iteration 0.06094809158265584\n",
      "current loss at 21611 iteration 0.060948016797471\n",
      "current loss at 21621 iteration 0.06094794208550786\n",
      "current loss at 21631 iteration 0.060947867446659997\n",
      "current loss at 21641 iteration 0.06094779288082113\n",
      "current loss at 21651 iteration 0.06094771838788525\n",
      "current loss at 21661 iteration 0.06094764396774652\n",
      "current loss at 21671 iteration 0.0609475696202993\n",
      "current loss at 21681 iteration 0.06094749534543815\n",
      "current loss at 21691 iteration 0.06094742114305786\n",
      "current loss at 21701 iteration 0.060947347013053405\n",
      "current loss at 21711 iteration 0.06094727295531996\n",
      "current loss at 21721 iteration 0.0609471989697529\n",
      "current loss at 21731 iteration 0.0609471250562478\n",
      "current loss at 21741 iteration 0.060947051214700454\n",
      "current loss at 21751 iteration 0.06094697744500685\n",
      "current loss at 21761 iteration 0.06094690374706313\n",
      "current loss at 21771 iteration 0.06094683012076569\n",
      "current loss at 21781 iteration 0.060946756566011104\n",
      "current loss at 21791 iteration 0.06094668308269614\n",
      "current loss at 21801 iteration 0.06094660967071779\n",
      "current loss at 21811 iteration 0.06094653632997319\n",
      "current loss at 21821 iteration 0.06094646306035973\n",
      "current loss at 21831 iteration 0.06094638986177495\n",
      "current loss at 21841 iteration 0.06094631673411661\n",
      "current loss at 21851 iteration 0.06094624367728266\n",
      "current loss at 21861 iteration 0.060946170691171225\n",
      "current loss at 21871 iteration 0.060946097775680684\n",
      "current loss at 21881 iteration 0.060946024930709525\n",
      "current loss at 21891 iteration 0.060945952156156516\n",
      "current loss at 21901 iteration 0.06094587945192052\n",
      "current loss at 21911 iteration 0.06094580681790071\n",
      "current loss at 21921 iteration 0.06094573425399633\n",
      "current loss at 21931 iteration 0.06094566176010691\n",
      "current loss at 21941 iteration 0.06094558933613211\n",
      "current loss at 21951 iteration 0.060945516981971815\n",
      "current loss at 21961 iteration 0.06094544469752609\n",
      "current loss at 21971 iteration 0.06094537248269517\n",
      "current loss at 21981 iteration 0.060945300337379514\n",
      "current loss at 21991 iteration 0.06094522826147976\n",
      "current loss at 22001 iteration 0.06094515625489669\n",
      "current loss at 22011 iteration 0.060945084317531366\n",
      "current loss at 22021 iteration 0.06094501244928493\n",
      "current loss at 22031 iteration 0.06094494065005878\n",
      "current loss at 22041 iteration 0.060944868919754494\n",
      "current loss at 22051 iteration 0.06094479725827381\n",
      "current loss at 22061 iteration 0.06094472566551867\n",
      "current loss at 22071 iteration 0.06094465414139121\n",
      "current loss at 22081 iteration 0.060944582685793725\n",
      "current loss at 22091 iteration 0.060944511298628705\n",
      "current loss at 22101 iteration 0.06094443997979883\n",
      "current loss at 22111 iteration 0.06094436872920697\n",
      "current loss at 22121 iteration 0.060944297546756135\n",
      "current loss at 22131 iteration 0.060944226432349584\n",
      "current loss at 22141 iteration 0.06094415538589071\n",
      "current loss at 22151 iteration 0.0609440844072831\n",
      "current loss at 22161 iteration 0.06094401349643051\n",
      "current loss at 22171 iteration 0.06094394265323692\n",
      "current loss at 22181 iteration 0.06094387187760645\n",
      "current loss at 22191 iteration 0.06094380116944339\n",
      "current loss at 22201 iteration 0.06094373052865224\n",
      "current loss at 22211 iteration 0.060943659955137695\n",
      "current loss at 22221 iteration 0.060943589448804576\n",
      "current loss at 22231 iteration 0.060943519009557905\n",
      "current loss at 22241 iteration 0.06094344863730291\n",
      "current loss at 22251 iteration 0.06094337833194495\n",
      "current loss at 22261 iteration 0.06094330809338958\n",
      "current loss at 22271 iteration 0.06094323792154256\n",
      "current loss at 22281 iteration 0.06094316781630978\n",
      "current loss at 22291 iteration 0.060943097777597335\n",
      "current loss at 22301 iteration 0.060943027805311495\n",
      "current loss at 22311 iteration 0.06094295789935868\n",
      "current loss at 22321 iteration 0.06094288805964549\n",
      "current loss at 22331 iteration 0.06094281828607876\n",
      "current loss at 22341 iteration 0.060942748578565395\n",
      "current loss at 22351 iteration 0.06094267893701258\n",
      "current loss at 22361 iteration 0.060942609361327574\n",
      "current loss at 22371 iteration 0.06094253985141788\n",
      "current loss at 22381 iteration 0.060942470407191154\n",
      "current loss at 22391 iteration 0.06094240102855518\n",
      "current loss at 22401 iteration 0.060942331715418\n",
      "current loss at 22411 iteration 0.06094226246768775\n",
      "current loss at 22421 iteration 0.06094219328527279\n",
      "current loss at 22431 iteration 0.060942124168081584\n",
      "current loss at 22441 iteration 0.060942055116022846\n",
      "current loss at 22451 iteration 0.0609419861290054\n",
      "current loss at 22461 iteration 0.06094191720693828\n",
      "current loss at 22471 iteration 0.06094184834973063\n",
      "current loss at 22481 iteration 0.06094177955729184\n",
      "current loss at 22491 iteration 0.060941710829531405\n",
      "current loss at 22501 iteration 0.06094164216635903\n",
      "current loss at 22511 iteration 0.060941573567684544\n",
      "current loss at 22521 iteration 0.06094150503341797\n",
      "current loss at 22531 iteration 0.06094143656346952\n",
      "current loss at 22541 iteration 0.060941368157749504\n",
      "current loss at 22551 iteration 0.060941299816168484\n",
      "current loss at 22561 iteration 0.06094123153863711\n",
      "current loss at 22571 iteration 0.06094116332506624\n",
      "current loss at 22581 iteration 0.06094109517536688\n",
      "current loss at 22591 iteration 0.06094102708945021\n",
      "current loss at 22601 iteration 0.06094095906722757\n",
      "current loss at 22611 iteration 0.06094089110861045\n",
      "current loss at 22621 iteration 0.06094082321351053\n",
      "current loss at 22631 iteration 0.06094075538183963\n",
      "current loss at 22641 iteration 0.06094068761350976\n",
      "current loss at 22651 iteration 0.06094061990843304\n",
      "current loss at 22661 iteration 0.06094055226652179\n",
      "current loss at 22671 iteration 0.0609404846876885\n",
      "current loss at 22681 iteration 0.06094041717184579\n",
      "current loss at 22691 iteration 0.06094034971890646\n",
      "current loss at 22701 iteration 0.06094028232878346\n",
      "current loss at 22711 iteration 0.06094021500138991\n",
      "current loss at 22721 iteration 0.060940147736639076\n",
      "current loss at 22731 iteration 0.060940080534444406\n",
      "current loss at 22741 iteration 0.060940013394719474\n",
      "current loss at 22751 iteration 0.06093994631737804\n",
      "current loss at 22761 iteration 0.06093987930233401\n",
      "current loss at 22771 iteration 0.060939812349501445\n",
      "current loss at 22781 iteration 0.06093974545879456\n",
      "current loss at 22791 iteration 0.06093967863012774\n",
      "current loss at 22801 iteration 0.06093961186341552\n",
      "current loss at 22811 iteration 0.06093954515857257\n",
      "current loss at 22821 iteration 0.06093947851551377\n",
      "current loss at 22831 iteration 0.0609394119341541\n",
      "current loss at 22841 iteration 0.06093934541440872\n",
      "current loss at 22851 iteration 0.06093927895619292\n",
      "current loss at 22861 iteration 0.060939212559422216\n",
      "current loss at 22871 iteration 0.06093914622401218\n",
      "current loss at 22881 iteration 0.060939079949878606\n",
      "current loss at 22891 iteration 0.06093901373693741\n",
      "current loss at 22901 iteration 0.060938947585104666\n",
      "current loss at 22911 iteration 0.06093888149429662\n",
      "current loss at 22921 iteration 0.06093881546442962\n",
      "current loss at 22931 iteration 0.06093874949542025\n",
      "current loss at 22941 iteration 0.06093868358718516\n",
      "current loss at 22951 iteration 0.0609386177396412\n",
      "current loss at 22961 iteration 0.06093855195270535\n",
      "current loss at 22971 iteration 0.06093848622629477\n",
      "current loss at 22981 iteration 0.06093842056032671\n",
      "current loss at 22991 iteration 0.06093835495471865\n",
      "current loss at 23001 iteration 0.06093828940938814\n",
      "current loss at 23011 iteration 0.06093822392425294\n",
      "current loss at 23021 iteration 0.06093815849923093\n",
      "current loss at 23031 iteration 0.06093809313424012\n",
      "current loss at 23041 iteration 0.060938027829198736\n",
      "current loss at 23051 iteration 0.06093796258402506\n",
      "current loss at 23061 iteration 0.06093789739863763\n",
      "current loss at 23071 iteration 0.06093783227295499\n",
      "current loss at 23081 iteration 0.06093776720689598\n",
      "current loss at 23091 iteration 0.060937702200379466\n",
      "current loss at 23101 iteration 0.060937637253324545\n",
      "current loss at 23111 iteration 0.060937572365650415\n",
      "current loss at 23121 iteration 0.06093750753727642\n",
      "current loss at 23131 iteration 0.060937442768122074\n",
      "current loss at 23141 iteration 0.060937378058107027\n",
      "current loss at 23151 iteration 0.060937313407151056\n",
      "current loss at 23161 iteration 0.060937248815174073\n",
      "current loss at 23171 iteration 0.060937184282096185\n",
      "current loss at 23181 iteration 0.06093711980783761\n",
      "current loss at 23191 iteration 0.060937055392318674\n",
      "current loss at 23201 iteration 0.060936991035459936\n",
      "current loss at 23211 iteration 0.06093692673718201\n",
      "current loss at 23221 iteration 0.060936862497405694\n",
      "current loss at 23231 iteration 0.06093679831605192\n",
      "current loss at 23241 iteration 0.06093673419304176\n",
      "current loss at 23251 iteration 0.06093667012829645\n",
      "current loss at 23261 iteration 0.06093660612173732\n",
      "current loss at 23271 iteration 0.060936542173285875\n",
      "current loss at 23281 iteration 0.06093647828286375\n",
      "current loss at 23291 iteration 0.060936414450392724\n",
      "current loss at 23301 iteration 0.060936350675794726\n",
      "current loss at 23311 iteration 0.06093628695899178\n",
      "current loss at 23321 iteration 0.060936223299906125\n",
      "current loss at 23331 iteration 0.060936159698460064\n",
      "current loss at 23341 iteration 0.060936096154576055\n",
      "current loss at 23351 iteration 0.06093603266817676\n",
      "current loss at 23361 iteration 0.06093596923918489\n",
      "current loss at 23371 iteration 0.060935905867523336\n",
      "current loss at 23381 iteration 0.06093584255311514\n",
      "current loss at 23391 iteration 0.060935779295883424\n",
      "current loss at 23401 iteration 0.06093571609575153\n",
      "current loss at 23411 iteration 0.06093565295264285\n",
      "current loss at 23421 iteration 0.06093558986648098\n",
      "current loss at 23431 iteration 0.06093552683718963\n",
      "current loss at 23441 iteration 0.06093546386469262\n",
      "current loss at 23451 iteration 0.06093540094891392\n",
      "current loss at 23461 iteration 0.060935338089777656\n",
      "current loss at 23471 iteration 0.06093527528720807\n",
      "current loss at 23481 iteration 0.060935212541129546\n",
      "current loss at 23491 iteration 0.06093514985146658\n",
      "current loss at 23501 iteration 0.06093508721814384\n",
      "current loss at 23511 iteration 0.0609350246410861\n",
      "current loss at 23521 iteration 0.06093496212021827\n",
      "current loss at 23531 iteration 0.06093489965546538\n",
      "current loss at 23541 iteration 0.060934837246752624\n",
      "current loss at 23551 iteration 0.060934774894005325\n",
      "current loss at 23561 iteration 0.0609347125971489\n",
      "current loss at 23571 iteration 0.060934650356108934\n",
      "current loss at 23581 iteration 0.06093458817081115\n",
      "current loss at 23591 iteration 0.06093452604118134\n",
      "current loss at 23601 iteration 0.06093446396714551\n",
      "current loss at 23611 iteration 0.06093440194862974\n",
      "current loss at 23621 iteration 0.060934339985560276\n",
      "current loss at 23631 iteration 0.060934278077863446\n",
      "current loss at 23641 iteration 0.060934216225465763\n",
      "current loss at 23651 iteration 0.06093415442829383\n",
      "current loss at 23661 iteration 0.060934092686274394\n",
      "current loss at 23671 iteration 0.06093403099933432\n",
      "current loss at 23681 iteration 0.060933969367400635\n",
      "current loss at 23691 iteration 0.060933907790400435\n",
      "current loss at 23701 iteration 0.060933846268261024\n",
      "current loss at 23711 iteration 0.06093378480090975\n",
      "current loss at 23721 iteration 0.06093372338827415\n",
      "current loss at 23731 iteration 0.06093366203028183\n",
      "current loss at 23741 iteration 0.060933600726860604\n",
      "current loss at 23751 iteration 0.06093353947793834\n",
      "current loss at 23761 iteration 0.06093347828344305\n",
      "current loss at 23771 iteration 0.06093341714330289\n",
      "current loss at 23781 iteration 0.06093335605744613\n",
      "current loss at 23791 iteration 0.0609332950258012\n",
      "current loss at 23801 iteration 0.060933234048296564\n",
      "current loss at 23811 iteration 0.06093317312486091\n",
      "current loss at 23821 iteration 0.060933112255422986\n",
      "current loss at 23831 iteration 0.060933051439911706\n",
      "current loss at 23841 iteration 0.06093299067825608\n",
      "current loss at 23851 iteration 0.060932929970385255\n",
      "current loss at 23861 iteration 0.06093286931622851\n",
      "current loss at 23871 iteration 0.06093280871571521\n",
      "current loss at 23881 iteration 0.060932748168774885\n",
      "current loss at 23891 iteration 0.06093268767533718\n",
      "current loss at 23901 iteration 0.060932627235331834\n",
      "current loss at 23911 iteration 0.06093256684868874\n",
      "current loss at 23921 iteration 0.060932506515337895\n",
      "current loss at 23931 iteration 0.06093244623520942\n",
      "current loss at 23941 iteration 0.060932386008233586\n",
      "current loss at 23951 iteration 0.06093232583434073\n",
      "current loss at 23961 iteration 0.06093226571346136\n",
      "current loss at 23971 iteration 0.060932205645526066\n",
      "current loss at 23981 iteration 0.060932145630465576\n",
      "current loss at 23991 iteration 0.06093208566821077\n",
      "current loss at 24001 iteration 0.06093202575869258\n",
      "current loss at 24011 iteration 0.06093196590184212\n",
      "current loss at 24021 iteration 0.060931906097590596\n",
      "current loss at 24031 iteration 0.06093184634586931\n",
      "current loss at 24041 iteration 0.06093178664660975\n",
      "current loss at 24051 iteration 0.060931726999743425\n",
      "current loss at 24061 iteration 0.06093166740520206\n",
      "current loss at 24071 iteration 0.06093160786291745\n",
      "current loss at 24081 iteration 0.060931548372821505\n",
      "current loss at 24091 iteration 0.06093148893484626\n",
      "current loss at 24101 iteration 0.06093142954892387\n",
      "current loss at 24111 iteration 0.0609313702149866\n",
      "current loss at 24121 iteration 0.06093131093296684\n",
      "current loss at 24131 iteration 0.06093125170279709\n",
      "current loss at 24141 iteration 0.060931192524409976\n",
      "current loss at 24151 iteration 0.06093113339773823\n",
      "current loss at 24161 iteration 0.06093107432271472\n",
      "current loss at 24171 iteration 0.06093101529927239\n",
      "current loss at 24181 iteration 0.06093095632734432\n",
      "current loss at 24191 iteration 0.060930897406863735\n",
      "current loss at 24201 iteration 0.06093083853776392\n",
      "current loss at 24211 iteration 0.060930779719978304\n",
      "current loss at 24221 iteration 0.06093072095344043\n",
      "current loss at 24231 iteration 0.060930662238083994\n",
      "current loss at 24241 iteration 0.06093060357384269\n",
      "current loss at 24251 iteration 0.06093054496065045\n",
      "current loss at 24261 iteration 0.060930486398441244\n",
      "current loss at 24271 iteration 0.06093042788714921\n",
      "current loss at 24281 iteration 0.06093036942670856\n",
      "current loss at 24291 iteration 0.06093031101705361\n",
      "current loss at 24301 iteration 0.060930252658118814\n",
      "current loss at 24311 iteration 0.06093019434983874\n",
      "current loss at 24321 iteration 0.06093013609214806\n",
      "current loss at 24331 iteration 0.06093007788498154\n",
      "current loss at 24341 iteration 0.06093001972827409\n",
      "current loss at 24351 iteration 0.0609299616219607\n",
      "current loss at 24361 iteration 0.060929903565976495\n",
      "current loss at 24371 iteration 0.060929845560256685\n",
      "current loss at 24381 iteration 0.060929787604736635\n",
      "current loss at 24391 iteration 0.06092972969935178\n",
      "current loss at 24401 iteration 0.06092967184403767\n",
      "current loss at 24411 iteration 0.060929614038729984\n",
      "current loss at 24421 iteration 0.06092955628336448\n",
      "current loss at 24431 iteration 0.06092949857787706\n",
      "current loss at 24441 iteration 0.0609294409222037\n",
      "current loss at 24451 iteration 0.06092938331628053\n",
      "current loss at 24461 iteration 0.06092932576004374\n",
      "current loss at 24471 iteration 0.06092926825342967\n",
      "current loss at 24481 iteration 0.06092921079637473\n",
      "current loss at 24491 iteration 0.06092915338881545\n",
      "current loss at 24501 iteration 0.060929096030688495\n",
      "current loss at 24511 iteration 0.06092903872193061\n",
      "current loss at 24521 iteration 0.06092898146247865\n",
      "current loss at 24531 iteration 0.06092892425226959\n",
      "current loss at 24541 iteration 0.06092886709124049\n",
      "current loss at 24551 iteration 0.060928809979328556\n",
      "current loss at 24561 iteration 0.06092875291647103\n",
      "current loss at 24571 iteration 0.060928695902605336\n",
      "current loss at 24581 iteration 0.06092863893766895\n",
      "current loss at 24591 iteration 0.0609285820215995\n",
      "current loss at 24601 iteration 0.06092852515433468\n",
      "current loss at 24611 iteration 0.0609284683358123\n",
      "current loss at 24621 iteration 0.060928411565970286\n",
      "current loss at 24631 iteration 0.060928354844746666\n",
      "current loss at 24641 iteration 0.06092829817207955\n",
      "current loss at 24651 iteration 0.060928241547907186\n",
      "current loss at 24661 iteration 0.06092818497216791\n",
      "current loss at 24671 iteration 0.06092812844480016\n",
      "current loss at 24681 iteration 0.06092807196574247\n",
      "current loss at 24691 iteration 0.06092801553493351\n",
      "current loss at 24701 iteration 0.06092795915231202\n",
      "current loss at 24711 iteration 0.06092790281781684\n",
      "current loss at 24721 iteration 0.060927846531386956\n",
      "current loss at 24731 iteration 0.060927790292961394\n",
      "current loss at 24741 iteration 0.06092773410247934\n",
      "current loss at 24751 iteration 0.06092767795988004\n",
      "current loss at 24761 iteration 0.06092762186510287\n",
      "current loss at 24771 iteration 0.06092756581808731\n",
      "current loss at 24781 iteration 0.060927509818772915\n",
      "current loss at 24791 iteration 0.060927453867099336\n",
      "current loss at 24801 iteration 0.06092739796300639\n",
      "current loss at 24811 iteration 0.06092734210643391\n",
      "current loss at 24821 iteration 0.0609272862973219\n",
      "current loss at 24831 iteration 0.06092723053561041\n",
      "current loss at 24841 iteration 0.060927174821239626\n",
      "current loss at 24851 iteration 0.060927119154149806\n",
      "current loss at 24861 iteration 0.06092706353428133\n",
      "current loss at 24871 iteration 0.0609270079615747\n",
      "current loss at 24881 iteration 0.06092695243597045\n",
      "current loss at 24891 iteration 0.06092689695740926\n",
      "current loss at 24901 iteration 0.060926841525831915\n",
      "current loss at 24911 iteration 0.06092678614117929\n",
      "current loss at 24921 iteration 0.060926730803392344\n",
      "current loss at 24931 iteration 0.060926675512412126\n",
      "current loss at 24941 iteration 0.060926620268179826\n",
      "current loss at 24951 iteration 0.060926565070636686\n",
      "current loss at 24961 iteration 0.0609265099197241\n",
      "current loss at 24971 iteration 0.06092645481538349\n",
      "current loss at 24981 iteration 0.06092639975755644\n",
      "current loss at 24991 iteration 0.06092634474618459\n",
      "current loss at 25001 iteration 0.060926289781209696\n",
      "current loss at 25011 iteration 0.06092623486257357\n",
      "current loss at 25021 iteration 0.060926179990218224\n",
      "current loss at 25031 iteration 0.06092612516408564\n",
      "current loss at 25041 iteration 0.06092607038411798\n",
      "current loss at 25051 iteration 0.06092601565025747\n",
      "current loss at 25061 iteration 0.060925960962446436\n",
      "current loss at 25071 iteration 0.06092590632062732\n",
      "current loss at 25081 iteration 0.0609258517247426\n",
      "current loss at 25091 iteration 0.060925797174734925\n",
      "current loss at 25101 iteration 0.06092574267054699\n",
      "current loss at 25111 iteration 0.060925688212121604\n",
      "current loss at 25121 iteration 0.06092563379940167\n",
      "current loss at 25131 iteration 0.06092557943233017\n",
      "current loss at 25141 iteration 0.060925525110850205\n",
      "current loss at 25151 iteration 0.060925470834904946\n",
      "current loss at 25161 iteration 0.06092541660443765\n",
      "current loss at 25171 iteration 0.060925362419391726\n",
      "current loss at 25181 iteration 0.06092530827971062\n",
      "current loss at 25191 iteration 0.06092525418533786\n",
      "current loss at 25201 iteration 0.060925200136217134\n",
      "current loss at 25211 iteration 0.06092514613229216\n",
      "current loss at 25221 iteration 0.06092509217350677\n",
      "current loss at 25231 iteration 0.06092503825980492\n",
      "current loss at 25241 iteration 0.0609249843911306\n",
      "current loss at 25251 iteration 0.06092493056742793\n",
      "current loss at 25261 iteration 0.0609248767886411\n",
      "current loss at 25271 iteration 0.06092482305471444\n",
      "current loss at 25281 iteration 0.060924769365592296\n",
      "current loss at 25291 iteration 0.06092471572121917\n",
      "current loss at 25301 iteration 0.06092466212153961\n",
      "current loss at 25311 iteration 0.06092460856649831\n",
      "current loss at 25321 iteration 0.06092455505603998\n",
      "current loss at 25331 iteration 0.0609245015901095\n",
      "current loss at 25341 iteration 0.06092444816865177\n",
      "current loss at 25351 iteration 0.06092439479161184\n",
      "current loss at 25361 iteration 0.060924341458934786\n",
      "current loss at 25371 iteration 0.060924288170565846\n",
      "current loss at 25381 iteration 0.060924234926450285\n",
      "current loss at 25391 iteration 0.0609241817265335\n",
      "current loss at 25401 iteration 0.06092412857076094\n",
      "current loss at 25411 iteration 0.0609240754590782\n",
      "current loss at 25421 iteration 0.06092402239143091\n",
      "current loss at 25431 iteration 0.0609239693677648\n",
      "current loss at 25441 iteration 0.06092391638802571\n",
      "current loss at 25451 iteration 0.060923863452159534\n",
      "current loss at 25461 iteration 0.0609238105601123\n",
      "current loss at 25471 iteration 0.06092375771183008\n",
      "current loss at 25481 iteration 0.06092370490725907\n",
      "current loss at 25491 iteration 0.060923652146345514\n",
      "current loss at 25501 iteration 0.060923599429035774\n",
      "current loss at 25511 iteration 0.0609235467552763\n",
      "current loss at 25521 iteration 0.06092349412501362\n",
      "current loss at 25531 iteration 0.06092344153819435\n",
      "current loss at 25541 iteration 0.060923388994765175\n",
      "current loss at 25551 iteration 0.06092333649467289\n",
      "current loss at 25561 iteration 0.060923284037864386\n",
      "current loss at 25571 iteration 0.06092323162428662\n",
      "current loss at 25581 iteration 0.06092317925388662\n",
      "current loss at 25591 iteration 0.06092312692661155\n",
      "current loss at 25601 iteration 0.06092307464240861\n",
      "current loss at 25611 iteration 0.06092302240122511\n",
      "current loss at 25621 iteration 0.06092297020300842\n",
      "current loss at 25631 iteration 0.06092291804770606\n",
      "current loss at 25641 iteration 0.06092286593526555\n",
      "current loss at 25651 iteration 0.060922813865634556\n",
      "current loss at 25661 iteration 0.060922761838760824\n",
      "current loss at 25671 iteration 0.06092270985459214\n",
      "current loss at 25681 iteration 0.06092265791307641\n",
      "current loss at 25691 iteration 0.06092260601416163\n",
      "current loss at 25701 iteration 0.06092255415779584\n",
      "current loss at 25711 iteration 0.06092250234392723\n",
      "current loss at 25721 iteration 0.06092245057250402\n",
      "current loss at 25731 iteration 0.060922398843474515\n",
      "current loss at 25741 iteration 0.06092234715678713\n",
      "current loss at 25751 iteration 0.060922295512390354\n",
      "current loss at 25761 iteration 0.06092224391023276\n",
      "current loss at 25771 iteration 0.06092219235026297\n",
      "current loss at 25781 iteration 0.060922140832429764\n",
      "current loss at 25791 iteration 0.060922089356681926\n",
      "current loss at 25801 iteration 0.06092203792296837\n",
      "current loss at 25811 iteration 0.060921986531238065\n",
      "current loss at 25821 iteration 0.06092193518144008\n",
      "current loss at 25831 iteration 0.06092188387352357\n",
      "current loss at 25841 iteration 0.06092183260743775\n",
      "current loss at 25851 iteration 0.060921781383131944\n",
      "current loss at 25861 iteration 0.06092173020055553\n",
      "current loss at 25871 iteration 0.060921679059657965\n",
      "current loss at 25881 iteration 0.06092162796038884\n",
      "current loss at 25891 iteration 0.06092157690269776\n",
      "current loss at 25901 iteration 0.06092152588653445\n",
      "current loss at 25911 iteration 0.060921474911848704\n",
      "current loss at 25921 iteration 0.06092142397859038\n",
      "current loss at 25931 iteration 0.06092137308670947\n",
      "current loss at 25941 iteration 0.06092132223615597\n",
      "current loss at 25951 iteration 0.06092127142688002\n",
      "current loss at 25961 iteration 0.06092122065883183\n",
      "current loss at 25971 iteration 0.060921169931961644\n",
      "current loss at 25981 iteration 0.06092111924621982\n",
      "current loss at 25991 iteration 0.060921068601556806\n",
      "current loss at 26001 iteration 0.06092101799792309\n",
      "current loss at 26011 iteration 0.06092096743526933\n",
      "current loss at 26021 iteration 0.060920916913546126\n",
      "current loss at 26031 iteration 0.06092086643270425\n",
      "current loss at 26041 iteration 0.060920815992694534\n",
      "current loss at 26051 iteration 0.06092076559346788\n",
      "current loss at 26061 iteration 0.060920715234975296\n",
      "current loss at 26071 iteration 0.060920664917167806\n",
      "current loss at 26081 iteration 0.06092061463999659\n",
      "current loss at 26091 iteration 0.06092056440341284\n",
      "current loss at 26101 iteration 0.060920514207367854\n",
      "current loss at 26111 iteration 0.06092046405181303\n",
      "current loss at 26121 iteration 0.060920413936699794\n",
      "current loss at 26131 iteration 0.06092036386197968\n",
      "current loss at 26141 iteration 0.06092031382760429\n",
      "current loss at 26151 iteration 0.06092026383352532\n",
      "current loss at 26161 iteration 0.06092021387969451\n",
      "current loss at 26171 iteration 0.060920163966063715\n",
      "current loss at 26181 iteration 0.06092011409258483\n",
      "current loss at 26191 iteration 0.06092006425920985\n",
      "current loss at 26201 iteration 0.060920014465890855\n",
      "current loss at 26211 iteration 0.06091996471257996\n",
      "current loss at 26221 iteration 0.06091991499922938\n",
      "current loss at 26231 iteration 0.060919865325791425\n",
      "current loss at 26241 iteration 0.06091981569221846\n",
      "current loss at 26251 iteration 0.06091976609846292\n",
      "current loss at 26261 iteration 0.060919716544477316\n",
      "current loss at 26271 iteration 0.06091966703021426\n",
      "current loss at 26281 iteration 0.060919617555626394\n",
      "current loss at 26291 iteration 0.06091956812066649\n",
      "current loss at 26301 iteration 0.06091951872528735\n",
      "current loss at 26311 iteration 0.06091946936944184\n",
      "current loss at 26321 iteration 0.060919420053082964\n",
      "current loss at 26331 iteration 0.06091937077616375\n",
      "current loss at 26341 iteration 0.060919321538637314\n",
      "current loss at 26351 iteration 0.06091927234045683\n",
      "current loss at 26361 iteration 0.060919223181575576\n",
      "current loss at 26371 iteration 0.06091917406194688\n",
      "current loss at 26381 iteration 0.06091912498152414\n",
      "current loss at 26391 iteration 0.06091907594026085\n",
      "current loss at 26401 iteration 0.06091902693811059\n",
      "current loss at 26411 iteration 0.06091897797502694\n",
      "current loss at 26421 iteration 0.060918929050963605\n",
      "current loss at 26431 iteration 0.0609188801658744\n",
      "current loss at 26441 iteration 0.06091883131971313\n",
      "current loss at 26451 iteration 0.06091878251243373\n",
      "current loss at 26461 iteration 0.06091873374399021\n",
      "current loss at 26471 iteration 0.0609186850143366\n",
      "current loss at 26481 iteration 0.060918636323427054\n",
      "current loss at 26491 iteration 0.06091858767121577\n",
      "current loss at 26501 iteration 0.06091853905765702\n",
      "current loss at 26511 iteration 0.06091849048270518\n",
      "current loss at 26521 iteration 0.060918441946314655\n",
      "current loss at 26531 iteration 0.06091839344843994\n",
      "current loss at 26541 iteration 0.06091834498903559\n",
      "current loss at 26551 iteration 0.060918296568056265\n",
      "current loss at 26561 iteration 0.06091824818545663\n",
      "current loss at 26571 iteration 0.06091819984119151\n",
      "current loss at 26581 iteration 0.06091815153521571\n",
      "current loss at 26591 iteration 0.060918103267484196\n",
      "current loss at 26601 iteration 0.06091805503795192\n",
      "current loss at 26611 iteration 0.060918006846573926\n",
      "current loss at 26621 iteration 0.06091795869330538\n",
      "current loss at 26631 iteration 0.060917910578101475\n",
      "current loss at 26641 iteration 0.06091786250091746\n",
      "current loss at 26651 iteration 0.06091781446170869\n",
      "current loss at 26661 iteration 0.06091776646043057\n",
      "current loss at 26671 iteration 0.060917718497038566\n",
      "current loss at 26681 iteration 0.06091767057148824\n",
      "current loss at 26691 iteration 0.06091762268373519\n",
      "current loss at 26701 iteration 0.060917574833735104\n",
      "current loss at 26711 iteration 0.060917527021443756\n",
      "current loss at 26721 iteration 0.06091747924681696\n",
      "current loss at 26731 iteration 0.060917431509810584\n",
      "current loss at 26741 iteration 0.06091738381038062\n",
      "current loss at 26751 iteration 0.060917336148483076\n",
      "current loss at 26761 iteration 0.06091728852407404\n",
      "current loss at 26771 iteration 0.06091724093710969\n",
      "current loss at 26781 iteration 0.060917193387546274\n",
      "current loss at 26791 iteration 0.060917145875340056\n",
      "current loss at 26801 iteration 0.06091709840044742\n",
      "current loss at 26811 iteration 0.0609170509628248\n",
      "current loss at 26821 iteration 0.0609170035624287\n",
      "current loss at 26831 iteration 0.060916956199215694\n",
      "current loss at 26841 iteration 0.06091690887314241\n",
      "current loss at 26851 iteration 0.060916861584165566\n",
      "current loss at 26861 iteration 0.06091681433224189\n",
      "current loss at 26871 iteration 0.06091676711732828\n",
      "current loss at 26881 iteration 0.0609167199393816\n",
      "current loss at 26891 iteration 0.060916672798358835\n",
      "current loss at 26901 iteration 0.06091662569421701\n",
      "current loss at 26911 iteration 0.060916578626913256\n",
      "current loss at 26921 iteration 0.060916531596404706\n",
      "current loss at 26931 iteration 0.06091648460264862\n",
      "current loss at 26941 iteration 0.0609164376456023\n",
      "current loss at 26951 iteration 0.060916390725223084\n",
      "current loss at 26961 iteration 0.06091634384146844\n",
      "current loss at 26971 iteration 0.060916296994295856\n",
      "current loss at 26981 iteration 0.06091625018366288\n",
      "current loss at 26991 iteration 0.06091620340952716\n",
      "current loss at 27001 iteration 0.06091615667184637\n",
      "current loss at 27011 iteration 0.06091610997057828\n",
      "current loss at 27021 iteration 0.06091606330568073\n",
      "current loss at 27031 iteration 0.060916016677111585\n",
      "current loss at 27041 iteration 0.06091597008482882\n",
      "current loss at 27051 iteration 0.06091592352879042\n",
      "current loss at 27061 iteration 0.06091587700895451\n",
      "current loss at 27071 iteration 0.06091583052527919\n",
      "current loss at 27081 iteration 0.060915784077722704\n",
      "current loss at 27091 iteration 0.060915737666243315\n",
      "current loss at 27101 iteration 0.06091569129079936\n",
      "current loss at 27111 iteration 0.06091564495134924\n",
      "current loss at 27121 iteration 0.06091559864785143\n",
      "current loss at 27131 iteration 0.06091555238026446\n",
      "current loss at 27141 iteration 0.060915506148546905\n",
      "current loss at 27151 iteration 0.06091545995265743\n",
      "current loss at 27161 iteration 0.060915413792554766\n",
      "current loss at 27171 iteration 0.06091536766819768\n",
      "current loss at 27181 iteration 0.06091532157954503\n",
      "current loss at 27191 iteration 0.06091527552655571\n",
      "current loss at 27201 iteration 0.060915229509188706\n",
      "current loss at 27211 iteration 0.06091518352740304\n",
      "current loss at 27221 iteration 0.060915137581157804\n",
      "current loss at 27231 iteration 0.06091509167041217\n",
      "current loss at 27241 iteration 0.06091504579512535\n",
      "current loss at 27251 iteration 0.06091499995525662\n",
      "current loss at 27261 iteration 0.060914954150765345\n",
      "current loss at 27271 iteration 0.06091490838161092\n",
      "current loss at 27281 iteration 0.06091486264775281\n",
      "current loss at 27291 iteration 0.06091481694915054\n",
      "current loss at 27301 iteration 0.0609147712857637\n",
      "current loss at 27311 iteration 0.060914725657551974\n",
      "current loss at 27321 iteration 0.060914680064475014\n",
      "current loss at 27331 iteration 0.06091463450649267\n",
      "current loss at 27341 iteration 0.06091458898356472\n",
      "current loss at 27351 iteration 0.0609145434956511\n",
      "current loss at 27361 iteration 0.06091449804271173\n",
      "current loss at 27371 iteration 0.060914452624706655\n",
      "current loss at 27381 iteration 0.06091440724159593\n",
      "current loss at 27391 iteration 0.060914361893339736\n",
      "current loss at 27401 iteration 0.060914316579898244\n",
      "current loss at 27411 iteration 0.0609142713012317\n",
      "current loss at 27421 iteration 0.06091422605730046\n",
      "current loss at 27431 iteration 0.060914180848064894\n",
      "current loss at 27441 iteration 0.06091413567348541\n",
      "current loss at 27451 iteration 0.06091409053352254\n",
      "current loss at 27461 iteration 0.06091404542813686\n",
      "current loss at 27471 iteration 0.06091400035728895\n",
      "current loss at 27481 iteration 0.060913955320939496\n",
      "current loss at 27491 iteration 0.06091391031904926\n",
      "current loss at 27501 iteration 0.06091386535157901\n",
      "current loss at 27511 iteration 0.06091382041848962\n",
      "current loss at 27521 iteration 0.06091377551974201\n",
      "current loss at 27531 iteration 0.060913730655297135\n",
      "current loss at 27541 iteration 0.060913685825116054\n",
      "current loss at 27551 iteration 0.06091364102915983\n",
      "current loss at 27561 iteration 0.060913596267389646\n",
      "current loss at 27571 iteration 0.06091355153976668\n",
      "current loss at 27581 iteration 0.06091350684625221\n",
      "current loss at 27591 iteration 0.060913462186807564\n",
      "current loss at 27601 iteration 0.06091341756139412\n",
      "current loss at 27611 iteration 0.06091337296997333\n",
      "current loss at 27621 iteration 0.06091332841250669\n",
      "current loss at 27631 iteration 0.060913283888955734\n",
      "current loss at 27641 iteration 0.060913239399282136\n",
      "current loss at 27651 iteration 0.06091319494344749\n",
      "current loss at 27661 iteration 0.0609131505214136\n",
      "current loss at 27671 iteration 0.060913106133142186\n",
      "current loss at 27681 iteration 0.06091306177859515\n",
      "current loss at 27691 iteration 0.06091301745773438\n",
      "current loss at 27701 iteration 0.0609129731705218\n",
      "current loss at 27711 iteration 0.06091292891691946\n",
      "current loss at 27721 iteration 0.06091288469688944\n",
      "current loss at 27731 iteration 0.060912840510393834\n",
      "current loss at 27741 iteration 0.060912796357394855\n",
      "current loss at 27751 iteration 0.06091275223785472\n",
      "current loss at 27761 iteration 0.06091270815173577\n",
      "current loss at 27771 iteration 0.06091266409900034\n",
      "current loss at 27781 iteration 0.06091262007961082\n",
      "current loss at 27791 iteration 0.0609125760935297\n",
      "current loss at 27801 iteration 0.06091253214071949\n",
      "current loss at 27811 iteration 0.0609124882211428\n",
      "current loss at 27821 iteration 0.060912444334762235\n",
      "current loss at 27831 iteration 0.0609124004815405\n",
      "current loss at 27841 iteration 0.060912356661440334\n",
      "current loss at 27851 iteration 0.06091231287442453\n",
      "current loss at 27861 iteration 0.06091226912045597\n",
      "current loss at 27871 iteration 0.06091222539949756\n",
      "current loss at 27881 iteration 0.06091218171151225\n",
      "current loss at 27891 iteration 0.06091213805646309\n",
      "current loss at 27901 iteration 0.06091209443431314\n",
      "current loss at 27911 iteration 0.060912050845025524\n",
      "current loss at 27921 iteration 0.06091200728856346\n",
      "current loss at 27931 iteration 0.06091196376489015\n",
      "current loss at 27941 iteration 0.06091192027396893\n",
      "current loss at 27951 iteration 0.06091187681576314\n",
      "current loss at 27961 iteration 0.060911833390236175\n",
      "current loss at 27971 iteration 0.0609117899973515\n",
      "current loss at 27981 iteration 0.06091174663707263\n",
      "current loss at 27991 iteration 0.06091170330936313\n",
      "current loss at 28001 iteration 0.06091166001418661\n",
      "current loss at 28011 iteration 0.060911616751506775\n",
      "current loss at 28021 iteration 0.06091157352128735\n",
      "current loss at 28031 iteration 0.06091153032349211\n",
      "current loss at 28041 iteration 0.060911487158084886\n",
      "current loss at 28051 iteration 0.06091144402502957\n",
      "current loss at 28061 iteration 0.060911400924290114\n",
      "current loss at 28071 iteration 0.06091135785583051\n",
      "current loss at 28081 iteration 0.06091131481961482\n",
      "current loss at 28091 iteration 0.06091127181560713\n",
      "current loss at 28101 iteration 0.0609112288437716\n",
      "current loss at 28111 iteration 0.06091118590407244\n",
      "current loss at 28121 iteration 0.06091114299647392\n",
      "current loss at 28131 iteration 0.06091110012094035\n",
      "current loss at 28141 iteration 0.0609110572774361\n",
      "current loss at 28151 iteration 0.06091101446592558\n",
      "current loss at 28161 iteration 0.060910971686373286\n",
      "current loss at 28171 iteration 0.06091092893874371\n",
      "current loss at 28181 iteration 0.06091088622300145\n",
      "current loss at 28191 iteration 0.06091084353911112\n",
      "current loss at 28201 iteration 0.060910800887037424\n",
      "current loss at 28211 iteration 0.06091075826674508\n",
      "current loss at 28221 iteration 0.06091071567819887\n",
      "current loss at 28231 iteration 0.06091067312136363\n",
      "current loss at 28241 iteration 0.06091063059620426\n",
      "current loss at 28251 iteration 0.06091058810268568\n",
      "current loss at 28261 iteration 0.06091054564077293\n",
      "current loss at 28271 iteration 0.060910503210430976\n",
      "current loss at 28281 iteration 0.06091046081162496\n",
      "current loss at 28291 iteration 0.060910418444320026\n",
      "current loss at 28301 iteration 0.060910376108481365\n",
      "current loss at 28311 iteration 0.06091033380407421\n",
      "current loss at 28321 iteration 0.060910291531063865\n",
      "current loss at 28331 iteration 0.0609102492894157\n",
      "current loss at 28341 iteration 0.060910207079095086\n",
      "current loss at 28351 iteration 0.06091016490006748\n",
      "current loss at 28361 iteration 0.06091012275229841\n",
      "current loss at 28371 iteration 0.06091008063575338\n",
      "current loss at 28381 iteration 0.06091003855039803\n",
      "current loss at 28391 iteration 0.06090999649619798\n",
      "current loss at 28401 iteration 0.06090995447311896\n",
      "current loss at 28411 iteration 0.0609099124811267\n",
      "current loss at 28421 iteration 0.06090987052018701\n",
      "current loss at 28431 iteration 0.06090982859026576\n",
      "current loss at 28441 iteration 0.06090978669132882\n",
      "current loss at 28451 iteration 0.06090974482334215\n",
      "current loss at 28461 iteration 0.060909702986271755\n",
      "current loss at 28471 iteration 0.0609096611800837\n",
      "current loss at 28481 iteration 0.060909619404744056\n",
      "current loss at 28491 iteration 0.060909577660219\n",
      "current loss at 28501 iteration 0.0609095359464747\n",
      "current loss at 28511 iteration 0.06090949426347742\n",
      "current loss at 28521 iteration 0.06090945261119345\n",
      "current loss at 28531 iteration 0.06090941098958916\n",
      "current loss at 28541 iteration 0.06090936939863091\n",
      "current loss at 28551 iteration 0.06090932783828515\n",
      "current loss at 28561 iteration 0.06090928630851839\n",
      "current loss at 28571 iteration 0.06090924480929716\n",
      "current loss at 28581 iteration 0.06090920334058804\n",
      "current loss at 28591 iteration 0.060909161902357685\n",
      "current loss at 28601 iteration 0.06090912049457277\n",
      "current loss at 28611 iteration 0.06090907911720003\n",
      "current loss at 28621 iteration 0.06090903777020625\n",
      "current loss at 28631 iteration 0.060908996453558246\n",
      "current loss at 28641 iteration 0.06090895516722291\n",
      "current loss at 28651 iteration 0.060908913911167165\n",
      "current loss at 28661 iteration 0.060908872685358\n",
      "current loss at 28671 iteration 0.060908831489762415\n",
      "current loss at 28681 iteration 0.060908790324347484\n",
      "current loss at 28691 iteration 0.060908749189080326\n",
      "current loss at 28701 iteration 0.060908708083928106\n",
      "current loss at 28711 iteration 0.060908667008858046\n",
      "current loss at 28721 iteration 0.06090862596383741\n",
      "current loss at 28731 iteration 0.06090858494883347\n",
      "current loss at 28741 iteration 0.060908543963813594\n",
      "current loss at 28751 iteration 0.06090850300874521\n",
      "current loss at 28761 iteration 0.06090846208359573\n",
      "current loss at 28771 iteration 0.060908421188332666\n",
      "current loss at 28781 iteration 0.060908380322923575\n",
      "current loss at 28791 iteration 0.06090833948733601\n",
      "current loss at 28801 iteration 0.060908298681537625\n",
      "current loss at 28811 iteration 0.060908257905496095\n",
      "current loss at 28821 iteration 0.06090821715917917\n",
      "current loss at 28831 iteration 0.060908176442554605\n",
      "current loss at 28841 iteration 0.06090813575559021\n",
      "current loss at 28851 iteration 0.06090809509825387\n",
      "current loss at 28861 iteration 0.060908054470513506\n",
      "current loss at 28871 iteration 0.06090801387233705\n",
      "current loss at 28881 iteration 0.06090797330369253\n",
      "current loss at 28891 iteration 0.06090793276454798\n",
      "current loss at 28901 iteration 0.06090789225487151\n",
      "current loss at 28911 iteration 0.060907851774631254\n",
      "current loss at 28921 iteration 0.06090781132379543\n",
      "current loss at 28931 iteration 0.06090777090233221\n",
      "current loss at 28941 iteration 0.06090773051020992\n",
      "current loss at 28951 iteration 0.060907690147396894\n",
      "current loss at 28961 iteration 0.060907649813861466\n",
      "current loss at 28971 iteration 0.06090760950957207\n",
      "current loss at 28981 iteration 0.060907569234497166\n",
      "current loss at 28991 iteration 0.06090752898860526\n",
      "current loss at 29001 iteration 0.0609074887718649\n",
      "current loss at 29011 iteration 0.0609074485842447\n",
      "current loss at 29021 iteration 0.06090740842571326\n",
      "current loss at 29031 iteration 0.0609073682962393\n",
      "current loss at 29041 iteration 0.06090732819579154\n",
      "current loss at 29051 iteration 0.06090728812433874\n",
      "current loss at 29061 iteration 0.060907248081849746\n",
      "current loss at 29071 iteration 0.06090720806829339\n",
      "current loss at 29081 iteration 0.06090716808363862\n",
      "current loss at 29091 iteration 0.06090712812785436\n",
      "current loss at 29101 iteration 0.060907088200909607\n",
      "current loss at 29111 iteration 0.060907048302773416\n",
      "current loss at 29121 iteration 0.06090700843341485\n",
      "current loss at 29131 iteration 0.06090696859280306\n",
      "current loss at 29141 iteration 0.06090692878090721\n",
      "current loss at 29151 iteration 0.06090688899769652\n",
      "current loss at 29161 iteration 0.060906849243140236\n",
      "current loss at 29171 iteration 0.06090680951720768\n",
      "current loss at 29181 iteration 0.06090676981986819\n",
      "current loss at 29191 iteration 0.06090673015109116\n",
      "current loss at 29201 iteration 0.060906690510846025\n",
      "current loss at 29211 iteration 0.060906650899102265\n",
      "current loss at 29221 iteration 0.06090661131582938\n",
      "current loss at 29231 iteration 0.06090657176099696\n",
      "current loss at 29241 iteration 0.06090653223457459\n",
      "current loss at 29251 iteration 0.06090649273653195\n",
      "current loss at 29261 iteration 0.06090645326683871\n",
      "current loss at 29271 iteration 0.06090641382546463\n",
      "current loss at 29281 iteration 0.06090637441237946\n",
      "current loss at 29291 iteration 0.06090633502755302\n",
      "current loss at 29301 iteration 0.06090629567095521\n",
      "current loss at 29311 iteration 0.06090625634255591\n",
      "current loss at 29321 iteration 0.06090621704232509\n",
      "current loss at 29331 iteration 0.060906177770232724\n",
      "current loss at 29341 iteration 0.060906138526248856\n",
      "current loss at 29351 iteration 0.060906099310343555\n",
      "current loss at 29361 iteration 0.06090606012248694\n",
      "current loss at 29371 iteration 0.060906020962649174\n",
      "current loss at 29381 iteration 0.060905981830800475\n",
      "current loss at 29391 iteration 0.06090594272691108\n",
      "current loss at 29401 iteration 0.06090590365095125\n",
      "current loss at 29411 iteration 0.06090586460289135\n",
      "current loss at 29421 iteration 0.06090582558270176\n",
      "current loss at 29431 iteration 0.06090578659035284\n",
      "current loss at 29441 iteration 0.0609057476258151\n",
      "current loss at 29451 iteration 0.06090570868905901\n",
      "current loss at 29461 iteration 0.0609056697800551\n",
      "current loss at 29471 iteration 0.06090563089877395\n",
      "current loss at 29481 iteration 0.06090559204518621\n",
      "current loss at 29491 iteration 0.06090555321926251\n",
      "current loss at 29501 iteration 0.06090551442097357\n",
      "current loss at 29511 iteration 0.06090547565029013\n",
      "current loss at 29521 iteration 0.06090543690718296\n",
      "current loss at 29531 iteration 0.06090539819162292\n",
      "current loss at 29541 iteration 0.06090535950358084\n",
      "current loss at 29551 iteration 0.06090532084302764\n",
      "current loss at 29561 iteration 0.06090528220993428\n",
      "current loss at 29571 iteration 0.06090524360427174\n",
      "current loss at 29581 iteration 0.06090520502601106\n",
      "current loss at 29591 iteration 0.060905166475123305\n",
      "current loss at 29601 iteration 0.06090512795157957\n",
      "current loss at 29611 iteration 0.06090508945535103\n",
      "current loss at 29621 iteration 0.060905050986408846\n",
      "current loss at 29631 iteration 0.060905012544724295\n",
      "current loss at 29641 iteration 0.06090497413026863\n",
      "current loss at 29651 iteration 0.06090493574301313\n",
      "current loss at 29661 iteration 0.060904897382929195\n",
      "current loss at 29671 iteration 0.060904859049988186\n",
      "current loss at 29681 iteration 0.06090482074416158\n",
      "current loss at 29691 iteration 0.0609047824654208\n",
      "current loss at 29701 iteration 0.06090474421373737\n",
      "current loss at 29711 iteration 0.06090470598908285\n",
      "current loss at 29721 iteration 0.060904667791428814\n",
      "current loss at 29731 iteration 0.06090462962074693\n",
      "current loss at 29741 iteration 0.060904591477008836\n",
      "current loss at 29751 iteration 0.06090455336018626\n",
      "current loss at 29761 iteration 0.06090451527025095\n",
      "current loss at 29771 iteration 0.060904477207174686\n",
      "current loss at 29781 iteration 0.060904439170929296\n",
      "current loss at 29791 iteration 0.060904401161486654\n",
      "current loss at 29801 iteration 0.06090436317881867\n",
      "current loss at 29811 iteration 0.06090432522289727\n",
      "current loss at 29821 iteration 0.06090428729369446\n",
      "current loss at 29831 iteration 0.060904249391182254\n",
      "current loss at 29841 iteration 0.060904211515332714\n",
      "current loss at 29851 iteration 0.06090417366611795\n",
      "current loss at 29861 iteration 0.060904135843510065\n",
      "current loss at 29871 iteration 0.0609040980474813\n",
      "current loss at 29881 iteration 0.06090406027800382\n",
      "current loss at 29891 iteration 0.06090402253504991\n",
      "current loss at 29901 iteration 0.06090398481859184\n",
      "current loss at 29911 iteration 0.060903947128601955\n",
      "current loss at 29921 iteration 0.06090390946505263\n",
      "current loss at 29931 iteration 0.06090387182791627\n",
      "current loss at 29941 iteration 0.06090383421716531\n",
      "current loss at 29951 iteration 0.06090379663277226\n",
      "current loss at 29961 iteration 0.06090375907470962\n",
      "current loss at 29971 iteration 0.06090372154294996\n",
      "current loss at 29981 iteration 0.060903684037465884\n",
      "current loss at 29991 iteration 0.060903646558230023\n",
      "current loss at 30001 iteration 0.06090360910521505\n",
      "current loss at 30011 iteration 0.06090357167839368\n",
      "current loss at 30021 iteration 0.06090353427773866\n",
      "current loss at 30031 iteration 0.06090349690322278\n",
      "current loss at 30041 iteration 0.06090345955481886\n",
      "current loss at 30051 iteration 0.06090342223249978\n",
      "current loss at 30061 iteration 0.06090338493623843\n",
      "current loss at 30071 iteration 0.06090334766600774\n",
      "current loss at 30081 iteration 0.06090331042178069\n",
      "current loss at 30091 iteration 0.060903273203530284\n",
      "current loss at 30101 iteration 0.060903236011229586\n",
      "current loss at 30111 iteration 0.06090319884485167\n",
      "current loss at 30121 iteration 0.060903161704369664\n",
      "current loss at 30131 iteration 0.060903124589756746\n",
      "current loss at 30141 iteration 0.06090308750098608\n",
      "current loss at 30151 iteration 0.06090305043803092\n",
      "current loss at 30161 iteration 0.06090301340086452\n",
      "current loss at 30171 iteration 0.06090297638946021\n",
      "current loss at 30181 iteration 0.06090293940379131\n",
      "current loss at 30191 iteration 0.06090290244383123\n",
      "current loss at 30201 iteration 0.06090286550955336\n",
      "current loss at 30211 iteration 0.06090282860093115\n",
      "current loss at 30221 iteration 0.06090279171793812\n",
      "current loss at 30231 iteration 0.060902754860547775\n",
      "current loss at 30241 iteration 0.06090271802873367\n",
      "current loss at 30251 iteration 0.06090268122246942\n",
      "current loss at 30261 iteration 0.06090264444172866\n",
      "current loss at 30271 iteration 0.06090260768648506\n",
      "current loss at 30281 iteration 0.06090257095671232\n",
      "current loss at 30291 iteration 0.06090253425238417\n",
      "current loss at 30301 iteration 0.06090249757347442\n",
      "current loss at 30311 iteration 0.06090246091995685\n",
      "current loss at 30321 iteration 0.06090242429180534\n",
      "current loss at 30331 iteration 0.06090238768899376\n",
      "current loss at 30341 iteration 0.060902351111496035\n",
      "current loss at 30351 iteration 0.06090231455928612\n",
      "current loss at 30361 iteration 0.06090227803233802\n",
      "current loss at 30371 iteration 0.06090224153062575\n",
      "current loss at 30381 iteration 0.06090220505412336\n",
      "current loss at 30391 iteration 0.060902168602804986\n",
      "current loss at 30401 iteration 0.06090213217664474\n",
      "current loss at 30411 iteration 0.060902095775616796\n",
      "current loss at 30421 iteration 0.06090205939969536\n",
      "current loss at 30431 iteration 0.06090202304885465\n",
      "current loss at 30441 iteration 0.060901986723068964\n",
      "current loss at 30451 iteration 0.06090195042231261\n",
      "current loss at 30461 iteration 0.06090191414655994\n",
      "current loss at 30471 iteration 0.06090187789578531\n",
      "current loss at 30481 iteration 0.06090184166996315\n",
      "current loss at 30491 iteration 0.060901805469067906\n",
      "current loss at 30501 iteration 0.06090176929307405\n",
      "current loss at 30511 iteration 0.06090173314195614\n",
      "current loss at 30521 iteration 0.06090169701568868\n",
      "current loss at 30531 iteration 0.060901660914246286\n",
      "current loss at 30541 iteration 0.06090162483760358\n",
      "current loss at 30551 iteration 0.06090158878573521\n",
      "current loss at 30561 iteration 0.060901552758615876\n",
      "current loss at 30571 iteration 0.06090151675622029\n",
      "current loss at 30581 iteration 0.0609014807785232\n",
      "current loss at 30591 iteration 0.06090144482549943\n",
      "current loss at 30601 iteration 0.06090140889712378\n",
      "current loss at 30611 iteration 0.06090137299337116\n",
      "current loss at 30621 iteration 0.0609013371142164\n",
      "current loss at 30631 iteration 0.06090130125963448\n",
      "current loss at 30641 iteration 0.060901265429600344\n",
      "current loss at 30651 iteration 0.060901229624088976\n",
      "current loss at 30661 iteration 0.060901193843075435\n",
      "current loss at 30671 iteration 0.06090115808653477\n",
      "current loss at 30681 iteration 0.06090112235444208\n",
      "current loss at 30691 iteration 0.06090108664677249\n",
      "current loss at 30701 iteration 0.06090105096350118\n",
      "current loss at 30711 iteration 0.06090101530460336\n",
      "current loss at 30721 iteration 0.06090097967005422\n",
      "current loss at 30731 iteration 0.06090094405982907\n",
      "current loss at 30741 iteration 0.060900908473903184\n",
      "current loss at 30751 iteration 0.06090087291225188\n",
      "current loss at 30761 iteration 0.060900837374850574\n",
      "current loss at 30771 iteration 0.06090080186167463\n",
      "current loss at 30781 iteration 0.06090076637269949\n",
      "current loss at 30791 iteration 0.0609007309079006\n",
      "current loss at 30801 iteration 0.060900695467253484\n",
      "current loss at 30811 iteration 0.06090066005073366\n",
      "current loss at 30821 iteration 0.06090062465831669\n",
      "current loss at 30831 iteration 0.060900589289978185\n",
      "current loss at 30841 iteration 0.06090055394569376\n",
      "current loss at 30851 iteration 0.060900518625439076\n",
      "current loss at 30861 iteration 0.06090048332918983\n",
      "current loss at 30871 iteration 0.06090044805692177\n",
      "current loss at 30881 iteration 0.06090041280861062\n",
      "current loss at 30891 iteration 0.060900377584232196\n",
      "current loss at 30901 iteration 0.060900342383762296\n",
      "current loss at 30911 iteration 0.060900307207176825\n",
      "current loss at 30921 iteration 0.060900272054451636\n",
      "current loss at 30931 iteration 0.060900236925562665\n",
      "current loss at 30941 iteration 0.06090020182048587\n",
      "current loss at 30951 iteration 0.060900166739197216\n",
      "current loss at 30961 iteration 0.06090013168167274\n",
      "current loss at 30971 iteration 0.06090009664788847\n",
      "current loss at 30981 iteration 0.06090006163782053\n",
      "current loss at 30991 iteration 0.06090002665144499\n",
      "current loss at 31001 iteration 0.060899991688738025\n",
      "current loss at 31011 iteration 0.060899956749675806\n",
      "current loss at 31021 iteration 0.06089992183423455\n",
      "current loss at 31031 iteration 0.06089988694239047\n",
      "current loss at 31041 iteration 0.06089985207411985\n",
      "current loss at 31051 iteration 0.06089981722939903\n",
      "current loss at 31061 iteration 0.060899782408204296\n",
      "current loss at 31071 iteration 0.06089974761051206\n",
      "current loss at 31081 iteration 0.060899712836298706\n",
      "current loss at 31091 iteration 0.06089967808554066\n",
      "current loss at 31101 iteration 0.06089964335821436\n",
      "current loss at 31111 iteration 0.06089960865429635\n",
      "current loss at 31121 iteration 0.06089957397376311\n",
      "current loss at 31131 iteration 0.060899539316591235\n",
      "current loss at 31141 iteration 0.06089950468275729\n",
      "current loss at 31151 iteration 0.060899470072237886\n",
      "current loss at 31161 iteration 0.060899435485009694\n",
      "current loss at 31171 iteration 0.06089940092104938\n",
      "current loss at 31181 iteration 0.060899366380333664\n",
      "current loss at 31191 iteration 0.06089933186283928\n",
      "current loss at 31201 iteration 0.06089929736854302\n",
      "current loss at 31211 iteration 0.06089926289742166\n",
      "current loss at 31221 iteration 0.060899228449452045\n",
      "current loss at 31231 iteration 0.06089919402461105\n",
      "current loss at 31241 iteration 0.06089915962287556\n",
      "current loss at 31251 iteration 0.06089912524422252\n",
      "current loss at 31261 iteration 0.06089909088862887\n",
      "current loss at 31271 iteration 0.060899056556071605\n",
      "current loss at 31281 iteration 0.06089902224652774\n",
      "current loss at 31291 iteration 0.06089898795997432\n",
      "current loss at 31301 iteration 0.06089895369638844\n",
      "current loss at 31311 iteration 0.060898919455747186\n",
      "current loss at 31321 iteration 0.0608988852380277\n",
      "current loss at 31331 iteration 0.060898851043207185\n",
      "current loss at 31341 iteration 0.06089881687126283\n",
      "current loss at 31351 iteration 0.06089878272217183\n",
      "current loss at 31361 iteration 0.06089874859591146\n",
      "current loss at 31371 iteration 0.060898714492459045\n",
      "current loss at 31381 iteration 0.06089868041179187\n",
      "current loss at 31391 iteration 0.060898646353887295\n",
      "current loss at 31401 iteration 0.0608986123187227\n",
      "current loss at 31411 iteration 0.06089857830627548\n",
      "current loss at 31421 iteration 0.06089854431652311\n",
      "current loss at 31431 iteration 0.06089851034944303\n",
      "current loss at 31441 iteration 0.060898476405012746\n",
      "current loss at 31451 iteration 0.06089844248320978\n",
      "current loss at 31461 iteration 0.060898408584011715\n",
      "current loss at 31471 iteration 0.060898374707396105\n",
      "current loss at 31481 iteration 0.060898340853340595\n",
      "current loss at 31491 iteration 0.06089830702182281\n",
      "current loss at 31501 iteration 0.06089827321282044\n",
      "current loss at 31511 iteration 0.06089823942631118\n",
      "current loss at 31521 iteration 0.0608982056622728\n",
      "current loss at 31531 iteration 0.060898171920683\n",
      "current loss at 31541 iteration 0.06089813820151962\n",
      "current loss at 31551 iteration 0.06089810450476047\n",
      "current loss at 31561 iteration 0.060898070830383406\n",
      "current loss at 31571 iteration 0.060898037178366296\n",
      "current loss at 31581 iteration 0.06089800354868705\n",
      "current loss at 31591 iteration 0.06089796994132363\n",
      "current loss at 31601 iteration 0.060897936356253984\n",
      "current loss at 31611 iteration 0.06089790279345608\n",
      "current loss at 31621 iteration 0.060897869252908\n",
      "current loss at 31631 iteration 0.06089783573458776\n",
      "current loss at 31641 iteration 0.06089780223847346\n",
      "current loss at 31651 iteration 0.0608977687645432\n",
      "current loss at 31661 iteration 0.06089773531277512\n",
      "current loss at 31671 iteration 0.06089770188314737\n",
      "current loss at 31681 iteration 0.06089766847563819\n",
      "current loss at 31691 iteration 0.060897635090225784\n",
      "current loss at 31701 iteration 0.06089760172688839\n",
      "current loss at 31711 iteration 0.060897568385604305\n",
      "current loss at 31721 iteration 0.060897535066351836\n",
      "current loss at 31731 iteration 0.060897501769109325\n",
      "current loss at 31741 iteration 0.060897468493855134\n",
      "current loss at 31751 iteration 0.06089743524056766\n",
      "current loss at 31761 iteration 0.06089740200922533\n",
      "current loss at 31771 iteration 0.060897368799806596\n",
      "current loss at 31781 iteration 0.060897335612289924\n",
      "current loss at 31791 iteration 0.060897302446653825\n",
      "current loss at 31801 iteration 0.06089726930287684\n",
      "current loss at 31811 iteration 0.060897236180937536\n",
      "current loss at 31821 iteration 0.060897203080814505\n",
      "current loss at 31831 iteration 0.06089717000248634\n",
      "current loss at 31841 iteration 0.06089713694593172\n",
      "current loss at 31851 iteration 0.06089710391112931\n",
      "current loss at 31861 iteration 0.06089707089805781\n",
      "current loss at 31871 iteration 0.060897037906695956\n",
      "current loss at 31881 iteration 0.060897004937022485\n",
      "current loss at 31891 iteration 0.060896971989016196\n",
      "current loss at 31901 iteration 0.060896939062655905\n",
      "current loss at 31911 iteration 0.06089690615792044\n",
      "current loss at 31921 iteration 0.06089687327478868\n",
      "current loss at 31931 iteration 0.06089684041323952\n",
      "current loss at 31941 iteration 0.060896807573251874\n",
      "current loss at 31951 iteration 0.060896774754804706\n",
      "current loss at 31961 iteration 0.060896741957876976\n",
      "current loss at 31971 iteration 0.06089670918244768\n",
      "current loss at 31981 iteration 0.06089667642849588\n",
      "current loss at 31991 iteration 0.06089664369600061\n",
      "current loss at 32001 iteration 0.06089661098494097\n",
      "current loss at 32011 iteration 0.06089657829529606\n",
      "current loss at 32021 iteration 0.06089654562704502\n",
      "current loss at 32031 iteration 0.06089651298016704\n",
      "current loss at 32041 iteration 0.06089648035464127\n",
      "current loss at 32051 iteration 0.06089644775044697\n",
      "current loss at 32061 iteration 0.060896415167563354\n",
      "current loss at 32071 iteration 0.06089638260596971\n",
      "current loss at 32081 iteration 0.06089635006564536\n",
      "current loss at 32091 iteration 0.06089631754656961\n",
      "current loss at 32101 iteration 0.0608962850487218\n",
      "current loss at 32111 iteration 0.06089625257208134\n",
      "current loss at 32121 iteration 0.06089622011662762\n",
      "current loss at 32131 iteration 0.060896187682340064\n",
      "current loss at 32141 iteration 0.060896155269198164\n",
      "current loss at 32151 iteration 0.060896122877181366\n",
      "current loss at 32161 iteration 0.06089609050626922\n",
      "current loss at 32171 iteration 0.060896058156441245\n",
      "current loss at 32181 iteration 0.060896025827676996\n",
      "current loss at 32191 iteration 0.06089599351995609\n",
      "current loss at 32201 iteration 0.060895961233258133\n",
      "current loss at 32211 iteration 0.06089592896756278\n",
      "current loss at 32221 iteration 0.060895896722849674\n",
      "current loss at 32231 iteration 0.060895864499098534\n",
      "current loss at 32241 iteration 0.060895832296289094\n",
      "current loss at 32251 iteration 0.060895800114401065\n",
      "current loss at 32261 iteration 0.060895767953414226\n",
      "current loss at 32271 iteration 0.06089573581330841\n",
      "current loss at 32281 iteration 0.060895703694063434\n",
      "current loss at 32291 iteration 0.06089567159565913\n",
      "current loss at 32301 iteration 0.06089563951807539\n",
      "current loss at 32311 iteration 0.06089560746129211\n",
      "current loss at 32321 iteration 0.06089557542528924\n",
      "current loss at 32331 iteration 0.06089554341004671\n",
      "current loss at 32341 iteration 0.060895511415544515\n",
      "current loss at 32351 iteration 0.06089547944176264\n",
      "current loss at 32361 iteration 0.060895447488681156\n",
      "current loss at 32371 iteration 0.060895415556280086\n",
      "current loss at 32381 iteration 0.06089538364453953\n",
      "current loss at 32391 iteration 0.06089535175343958\n",
      "current loss at 32401 iteration 0.06089531988296039\n",
      "current loss at 32411 iteration 0.06089528803308211\n",
      "current loss at 32421 iteration 0.060895256203784914\n",
      "current loss at 32431 iteration 0.06089522439504903\n",
      "current loss at 32441 iteration 0.06089519260685469\n",
      "current loss at 32451 iteration 0.060895160839182144\n",
      "current loss at 32461 iteration 0.06089512909201168\n",
      "current loss at 32471 iteration 0.060895097365323614\n",
      "current loss at 32481 iteration 0.060895065659098274\n",
      "current loss at 32491 iteration 0.06089503397331603\n",
      "current loss at 32501 iteration 0.06089500230795727\n",
      "current loss at 32511 iteration 0.06089497066300239\n",
      "current loss at 32521 iteration 0.06089493903843182\n",
      "current loss at 32531 iteration 0.060894907434226035\n",
      "current loss at 32541 iteration 0.060894875850365536\n",
      "current loss at 32551 iteration 0.0608948442868308\n",
      "current loss at 32561 iteration 0.06089481274360239\n",
      "current loss at 32571 iteration 0.06089478122066083\n",
      "current loss at 32581 iteration 0.06089474971798673\n",
      "current loss at 32591 iteration 0.06089471823556071\n",
      "current loss at 32601 iteration 0.06089468677336337\n",
      "current loss at 32611 iteration 0.060894655331375405\n",
      "current loss at 32621 iteration 0.06089462390957748\n",
      "current loss at 32631 iteration 0.060894592507950276\n",
      "current loss at 32641 iteration 0.06089456112647456\n",
      "current loss at 32651 iteration 0.06089452976513107\n",
      "current loss at 32661 iteration 0.060894498423900614\n",
      "current loss at 32671 iteration 0.060894467102763965\n",
      "current loss at 32681 iteration 0.06089443580170195\n",
      "current loss at 32691 iteration 0.060894404520695465\n",
      "current loss at 32701 iteration 0.060894373259725326\n",
      "current loss at 32711 iteration 0.06089434201877248\n",
      "current loss at 32721 iteration 0.06089431079781783\n",
      "current loss at 32731 iteration 0.06089427959684235\n",
      "current loss at 32741 iteration 0.060894248415826975\n",
      "current loss at 32751 iteration 0.06089421725475273\n",
      "current loss at 32761 iteration 0.060894186113600646\n",
      "current loss at 32771 iteration 0.06089415499235176\n",
      "current loss at 32781 iteration 0.06089412389098711\n",
      "current loss at 32791 iteration 0.06089409280948784\n",
      "current loss at 32801 iteration 0.060894061747835036\n",
      "current loss at 32811 iteration 0.06089403070600985\n",
      "current loss at 32821 iteration 0.060893999683993436\n",
      "current loss at 32831 iteration 0.060893968681767\n",
      "current loss at 32841 iteration 0.06089393769931174\n",
      "current loss at 32851 iteration 0.060893906736608897\n",
      "current loss at 32861 iteration 0.060893875793639744\n",
      "current loss at 32871 iteration 0.060893844870385544\n",
      "current loss at 32881 iteration 0.06089381396682761\n",
      "current loss at 32891 iteration 0.06089378308294727\n",
      "current loss at 32901 iteration 0.06089375221872589\n",
      "current loss at 32911 iteration 0.06089372137414484\n",
      "current loss at 32921 iteration 0.060893690549185524\n",
      "current loss at 32931 iteration 0.06089365974382935\n",
      "current loss at 32941 iteration 0.060893628958057784\n",
      "current loss at 32951 iteration 0.0608935981918523\n",
      "current loss at 32961 iteration 0.06089356744519436\n",
      "current loss at 32971 iteration 0.06089353671806552\n",
      "current loss at 32981 iteration 0.060893506010447296\n",
      "current loss at 32991 iteration 0.06089347532232127\n",
      "current loss at 33001 iteration 0.060893444653669006\n",
      "current loss at 33011 iteration 0.060893414004472124\n",
      "current loss at 33021 iteration 0.06089338337471227\n",
      "current loss at 33031 iteration 0.0608933527643711\n",
      "current loss at 33041 iteration 0.060893322173430256\n",
      "current loss at 33051 iteration 0.060893291601871465\n",
      "current loss at 33061 iteration 0.06089326104967647\n",
      "current loss at 33071 iteration 0.060893230516826984\n",
      "current loss at 33081 iteration 0.06089320000330481\n",
      "current loss at 33091 iteration 0.0608931695090917\n",
      "current loss at 33101 iteration 0.06089313903416951\n",
      "current loss at 33111 iteration 0.06089310857852005\n",
      "current loss at 33121 iteration 0.060893078142125204\n",
      "current loss at 33131 iteration 0.06089304772496684\n",
      "current loss at 33141 iteration 0.060893017327026865\n",
      "current loss at 33151 iteration 0.060892986948287216\n",
      "current loss at 33161 iteration 0.06089295658872984\n",
      "current loss at 33171 iteration 0.060892926248336705\n",
      "current loss at 33181 iteration 0.06089289592708983\n",
      "current loss at 33191 iteration 0.060892865624971204\n",
      "current loss at 33201 iteration 0.06089283534196288\n",
      "current loss at 33211 iteration 0.060892805078046924\n",
      "current loss at 33221 iteration 0.06089277483320543\n",
      "current loss at 33231 iteration 0.060892744607420494\n",
      "current loss at 33241 iteration 0.06089271440067426\n",
      "current loss at 33251 iteration 0.06089268421294886\n",
      "current loss at 33261 iteration 0.06089265404422649\n",
      "current loss at 33271 iteration 0.06089262389448932\n",
      "current loss at 33281 iteration 0.06089259376371961\n",
      "current loss at 33291 iteration 0.06089256365189956\n",
      "current loss at 33301 iteration 0.06089253355901145\n",
      "current loss at 33311 iteration 0.06089250348503757\n",
      "current loss at 33321 iteration 0.06089247342996023\n",
      "current loss at 33331 iteration 0.06089244339376175\n",
      "current loss at 33341 iteration 0.060892413376424495\n",
      "current loss at 33351 iteration 0.06089238337793082\n",
      "current loss at 33361 iteration 0.06089235339826312\n",
      "current loss at 33371 iteration 0.060892323437403834\n",
      "current loss at 33381 iteration 0.06089229349533538\n",
      "current loss at 33391 iteration 0.060892263572040216\n",
      "current loss at 33401 iteration 0.060892233667500836\n",
      "current loss at 33411 iteration 0.060892203781699746\n",
      "current loss at 33421 iteration 0.06089217391461948\n",
      "current loss at 33431 iteration 0.06089214406624255\n",
      "current loss at 33441 iteration 0.06089211423655156\n",
      "current loss at 33451 iteration 0.06089208442552907\n",
      "current loss at 33461 iteration 0.06089205463315771\n",
      "current loss at 33471 iteration 0.06089202485942012\n",
      "current loss at 33481 iteration 0.060891995104298956\n",
      "current loss at 33491 iteration 0.06089196536777686\n",
      "current loss at 33501 iteration 0.06089193564983656\n",
      "current loss at 33511 iteration 0.060891905950460776\n",
      "current loss at 33521 iteration 0.06089187626963223\n",
      "current loss at 33531 iteration 0.06089184660733371\n",
      "current loss at 33541 iteration 0.060891816963547965\n",
      "current loss at 33551 iteration 0.06089178733825783\n",
      "current loss at 33561 iteration 0.060891757731446096\n",
      "current loss at 33571 iteration 0.06089172814309564\n",
      "current loss at 33581 iteration 0.06089169857318932\n",
      "current loss at 33591 iteration 0.060891669021710025\n",
      "current loss at 33601 iteration 0.06089163948864065\n",
      "current loss at 33611 iteration 0.06089160997396416\n",
      "current loss at 33621 iteration 0.06089158047766346\n",
      "current loss at 33631 iteration 0.06089155099972156\n",
      "current loss at 33641 iteration 0.06089152154012144\n",
      "current loss at 33651 iteration 0.06089149209884611\n",
      "current loss at 33661 iteration 0.06089146267587861\n",
      "current loss at 33671 iteration 0.06089143327120201\n",
      "current loss at 33681 iteration 0.06089140388479935\n",
      "current loss at 33691 iteration 0.060891374516653766\n",
      "current loss at 33701 iteration 0.06089134516674836\n",
      "current loss at 33711 iteration 0.06089131583506627\n",
      "current loss at 33721 iteration 0.06089128652159064\n",
      "current loss at 33731 iteration 0.06089125722630468\n",
      "current loss at 33741 iteration 0.06089122794919158\n",
      "current loss at 33751 iteration 0.06089119869023455\n",
      "current loss at 33761 iteration 0.06089116944941686\n",
      "current loss at 33771 iteration 0.06089114022672174\n",
      "current loss at 33781 iteration 0.06089111102213249\n",
      "current loss at 33791 iteration 0.06089108183563241\n",
      "current loss at 33801 iteration 0.060891052667204834\n",
      "current loss at 33811 iteration 0.06089102351683309\n",
      "current loss at 33821 iteration 0.06089099438450055\n",
      "current loss at 33831 iteration 0.060890965270190604\n",
      "current loss at 33841 iteration 0.06089093617388666\n",
      "current loss at 33851 iteration 0.060890907095572114\n",
      "current loss at 33861 iteration 0.060890878035230446\n",
      "current loss at 33871 iteration 0.06089084899284511\n",
      "current loss at 33881 iteration 0.060890819968399595\n",
      "current loss at 33891 iteration 0.06089079096187739\n",
      "current loss at 33901 iteration 0.060890761973262056\n",
      "current loss at 33911 iteration 0.0608907330025371\n",
      "current loss at 33921 iteration 0.060890704049686134\n",
      "current loss at 33931 iteration 0.06089067511469269\n",
      "current loss at 33941 iteration 0.06089064619754043\n",
      "current loss at 33951 iteration 0.06089061729821294\n",
      "current loss at 33961 iteration 0.06089058841669389\n",
      "current loss at 33971 iteration 0.06089055955296693\n",
      "current loss at 33981 iteration 0.06089053070701575\n",
      "current loss at 33991 iteration 0.06089050187882408\n",
      "current loss at 34001 iteration 0.06089047306837561\n",
      "current loss at 34011 iteration 0.06089044427565409\n",
      "current loss at 34021 iteration 0.06089041550064332\n",
      "current loss at 34031 iteration 0.060890386743327064\n",
      "current loss at 34041 iteration 0.06089035800368911\n",
      "current loss at 34051 iteration 0.06089032928171329\n",
      "current loss at 34061 iteration 0.06089030057738349\n",
      "current loss at 34071 iteration 0.06089027189068351\n",
      "current loss at 34081 iteration 0.06089024322159727\n",
      "current loss at 34091 iteration 0.06089021457010868\n",
      "current loss at 34101 iteration 0.06089018593620164\n",
      "current loss at 34111 iteration 0.060890157319860104\n",
      "current loss at 34121 iteration 0.060890128721068026\n",
      "current loss at 34131 iteration 0.06089010013980938\n",
      "current loss at 34141 iteration 0.06089007157606821\n",
      "current loss at 34151 iteration 0.060890043029828464\n",
      "current loss at 34161 iteration 0.060890014501074244\n",
      "current loss at 34171 iteration 0.06088998598978959\n",
      "current loss at 34181 iteration 0.06088995749595857\n",
      "current loss at 34191 iteration 0.060889929019565275\n",
      "current loss at 34201 iteration 0.06088990056059383\n",
      "current loss at 34211 iteration 0.060889872119028375\n",
      "current loss at 34221 iteration 0.06088984369485306\n",
      "current loss at 34231 iteration 0.06088981528805207\n",
      "current loss at 34241 iteration 0.06088978689860958\n",
      "current loss at 34251 iteration 0.060889758526509796\n",
      "current loss at 34261 iteration 0.06088973017173698\n",
      "current loss at 34271 iteration 0.06088970183427536\n",
      "current loss at 34281 iteration 0.060889673514109205\n",
      "current loss at 34291 iteration 0.0608896452112228\n",
      "current loss at 34301 iteration 0.060889616925600465\n",
      "current loss at 34311 iteration 0.06088958865722652\n",
      "current loss at 34321 iteration 0.060889560406085286\n",
      "current loss at 34331 iteration 0.060889532172161163\n",
      "current loss at 34341 iteration 0.060889503955438525\n",
      "current loss at 34351 iteration 0.060889475755901756\n",
      "current loss at 34361 iteration 0.06088944757353528\n",
      "current loss at 34371 iteration 0.06088941940832356\n",
      "current loss at 34381 iteration 0.06088939126025101\n",
      "current loss at 34391 iteration 0.06088936312930214\n",
      "current loss at 34401 iteration 0.06088933501546144\n",
      "current loss at 34411 iteration 0.060889306918713415\n",
      "current loss at 34421 iteration 0.06088927883904259\n",
      "current loss at 34431 iteration 0.06088925077643353\n",
      "current loss at 34441 iteration 0.0608892227308708\n",
      "current loss at 34451 iteration 0.060889194702338986\n",
      "current loss at 34461 iteration 0.0608891666908227\n",
      "current loss at 34471 iteration 0.06088913869630655\n",
      "current loss at 34481 iteration 0.060889110718775195\n",
      "current loss at 34491 iteration 0.06088908275821328\n",
      "current loss at 34501 iteration 0.06088905481460549\n",
      "current loss at 34511 iteration 0.06088902688793654\n",
      "current loss at 34521 iteration 0.060888998978191125\n",
      "current loss at 34531 iteration 0.060888971085353995\n",
      "current loss at 34541 iteration 0.0608889432094099\n",
      "current loss at 34551 iteration 0.060888915350343575\n",
      "current loss at 34561 iteration 0.06088888750813987\n",
      "current loss at 34571 iteration 0.060888859682783555\n",
      "current loss at 34581 iteration 0.06088883187425947\n",
      "current loss at 34591 iteration 0.060888804082552435\n",
      "current loss at 34601 iteration 0.060888776307647344\n",
      "current loss at 34611 iteration 0.06088874854952907\n",
      "current loss at 34621 iteration 0.06088872080818248\n",
      "current loss at 34631 iteration 0.06088869308359251\n",
      "current loss at 34641 iteration 0.06088866537574411\n",
      "current loss at 34651 iteration 0.06088863768462222\n",
      "current loss at 34661 iteration 0.060888610010211806\n",
      "current loss at 34671 iteration 0.06088858235249786\n",
      "current loss at 34681 iteration 0.06088855471146537\n",
      "current loss at 34691 iteration 0.06088852708709939\n",
      "current loss at 34701 iteration 0.06088849947938493\n",
      "current loss at 34711 iteration 0.06088847188830709\n",
      "current loss at 34721 iteration 0.060888444313850903\n",
      "current loss at 34731 iteration 0.06088841675600148\n",
      "current loss at 34741 iteration 0.06088838921474396\n",
      "current loss at 34751 iteration 0.06088836169006343\n",
      "current loss at 34761 iteration 0.06088833418194507\n",
      "current loss at 34771 iteration 0.060888306690374025\n",
      "current loss at 34781 iteration 0.06088827921533549\n",
      "current loss at 34791 iteration 0.06088825175681464\n",
      "current loss at 34801 iteration 0.06088822431479674\n",
      "current loss at 34811 iteration 0.06088819688926699\n",
      "current loss at 34821 iteration 0.06088816948021067\n",
      "current loss at 34831 iteration 0.06088814208761301\n",
      "current loss at 34841 iteration 0.06088811471145935\n",
      "current loss at 34851 iteration 0.06088808735173496\n",
      "current loss at 34861 iteration 0.06088806000842516\n",
      "current loss at 34871 iteration 0.06088803268151531\n",
      "current loss at 34881 iteration 0.06088800537099077\n",
      "current loss at 34891 iteration 0.06088797807683689\n",
      "current loss at 34901 iteration 0.06088795079903911\n",
      "current loss at 34911 iteration 0.06088792353758278\n",
      "current loss at 34921 iteration 0.060887896292453364\n",
      "current loss at 34931 iteration 0.06088786906363629\n",
      "current loss at 34941 iteration 0.060887841851117036\n",
      "current loss at 34951 iteration 0.06088781465488107\n",
      "current loss at 34961 iteration 0.06088778747491389\n",
      "current loss at 34971 iteration 0.06088776031120101\n",
      "current loss at 34981 iteration 0.06088773316372796\n",
      "current loss at 34991 iteration 0.060887706032480296\n",
      "current loss at 35001 iteration 0.06088767891744357\n",
      "current loss at 35011 iteration 0.06088765181860337\n",
      "current loss at 35021 iteration 0.060887624735945285\n",
      "current loss at 35031 iteration 0.060887597669454924\n",
      "current loss at 35041 iteration 0.06088757061911795\n",
      "current loss at 35051 iteration 0.060887543584920004\n",
      "current loss at 35061 iteration 0.06088751656684672\n",
      "current loss at 35071 iteration 0.060887489564883816\n",
      "current loss at 35081 iteration 0.06088746257901698\n",
      "current loss at 35091 iteration 0.06088743560923193\n",
      "current loss at 35101 iteration 0.060887408655514404\n",
      "current loss at 35111 iteration 0.060887381717850145\n",
      "current loss at 35121 iteration 0.0608873547962249\n",
      "current loss at 35131 iteration 0.0608873278906245\n",
      "current loss at 35141 iteration 0.060887301001034726\n",
      "current loss at 35151 iteration 0.06088727412744136\n",
      "current loss at 35161 iteration 0.0608872472698303\n",
      "current loss at 35171 iteration 0.06088722042818736\n",
      "current loss at 35181 iteration 0.060887193602498405\n",
      "current loss at 35191 iteration 0.0608871667927493\n",
      "current loss at 35201 iteration 0.06088713999892598\n",
      "current loss at 35211 iteration 0.06088711322101438\n",
      "current loss at 35221 iteration 0.06088708645900039\n",
      "current loss at 35231 iteration 0.06088705971286996\n",
      "current loss at 35241 iteration 0.060887032982609085\n",
      "current loss at 35251 iteration 0.060887006268203744\n",
      "current loss at 35261 iteration 0.06088697956963992\n",
      "current loss at 35271 iteration 0.06088695288690363\n",
      "current loss at 35281 iteration 0.0608869262199809\n",
      "current loss at 35291 iteration 0.060886899568857816\n",
      "current loss at 35301 iteration 0.060886872933520415\n",
      "current loss at 35311 iteration 0.06088684631395477\n",
      "current loss at 35321 iteration 0.060886819710147005\n",
      "current loss at 35331 iteration 0.0608867931220832\n",
      "current loss at 35341 iteration 0.0608867665497495\n",
      "current loss at 35351 iteration 0.06088673999313207\n",
      "current loss at 35361 iteration 0.060886713452217044\n",
      "current loss at 35371 iteration 0.06088668692699062\n",
      "current loss at 35381 iteration 0.06088666041743899\n",
      "current loss at 35391 iteration 0.06088663392354835\n",
      "current loss at 35401 iteration 0.06088660744530494\n",
      "current loss at 35411 iteration 0.06088658098269501\n",
      "current loss at 35421 iteration 0.060886554535704795\n",
      "current loss at 35431 iteration 0.06088652810432059\n",
      "current loss at 35441 iteration 0.06088650168852868\n",
      "current loss at 35451 iteration 0.06088647528831537\n",
      "current loss at 35461 iteration 0.060886448903666984\n",
      "current loss at 35471 iteration 0.060886422534569856\n",
      "current loss at 35481 iteration 0.06088639618101036\n",
      "current loss at 35491 iteration 0.06088636984297486\n",
      "current loss at 35501 iteration 0.060886343520449714\n",
      "current loss at 35511 iteration 0.06088631721342135\n",
      "current loss at 35521 iteration 0.06088629092187619\n",
      "current loss at 35531 iteration 0.06088626464580065\n",
      "current loss at 35541 iteration 0.06088623838518121\n",
      "current loss at 35551 iteration 0.06088621214000429\n",
      "current loss at 35561 iteration 0.06088618591025641\n",
      "current loss at 35571 iteration 0.06088615969592405\n",
      "current loss at 35581 iteration 0.060886133496993726\n",
      "current loss at 35591 iteration 0.060886107313451975\n",
      "current loss at 35601 iteration 0.06088608114528531\n",
      "current loss at 35611 iteration 0.06088605499248034\n",
      "current loss at 35621 iteration 0.0608860288550236\n",
      "current loss at 35631 iteration 0.06088600273290169\n",
      "current loss at 35641 iteration 0.06088597662610123\n",
      "current loss at 35651 iteration 0.06088595053460882\n",
      "current loss at 35661 iteration 0.06088592445841111\n",
      "current loss at 35671 iteration 0.06088589839749476\n",
      "current loss at 35681 iteration 0.06088587235184642\n",
      "current loss at 35691 iteration 0.06088584632145279\n",
      "current loss at 35701 iteration 0.060885820306300555\n",
      "current loss at 35711 iteration 0.060885794306376445\n",
      "current loss at 35721 iteration 0.06088576832166716\n",
      "current loss at 35731 iteration 0.06088574235215949\n",
      "current loss at 35741 iteration 0.060885716397840156\n",
      "current loss at 35751 iteration 0.06088569045869595\n",
      "current loss at 35761 iteration 0.06088566453471367\n",
      "current loss at 35771 iteration 0.06088563862588011\n",
      "current loss at 35781 iteration 0.0608856127321821\n",
      "current loss at 35791 iteration 0.060885586853606455\n",
      "current loss at 35801 iteration 0.06088556099014006\n",
      "current loss at 35811 iteration 0.06088553514176977\n",
      "current loss at 35821 iteration 0.06088550930848244\n",
      "current loss at 35831 iteration 0.06088548349026503\n",
      "current loss at 35841 iteration 0.06088545768710438\n",
      "current loss at 35851 iteration 0.060885431898987465\n",
      "current loss at 35861 iteration 0.060885406125901195\n",
      "current loss at 35871 iteration 0.060885380367832566\n",
      "current loss at 35881 iteration 0.06088535462476852\n",
      "current loss at 35891 iteration 0.06088532889669607\n",
      "current loss at 35901 iteration 0.06088530318360218\n",
      "current loss at 35911 iteration 0.0608852774854739\n",
      "current loss at 35921 iteration 0.06088525180229825\n",
      "current loss at 35931 iteration 0.0608852261340623\n",
      "current loss at 35941 iteration 0.06088520048075306\n",
      "current loss at 35951 iteration 0.06088517484235767\n",
      "current loss at 35961 iteration 0.06088514921886317\n",
      "current loss at 35971 iteration 0.06088512361025671\n",
      "current loss at 35981 iteration 0.06088509801652537\n",
      "current loss at 35991 iteration 0.06088507243765631\n",
      "current loss at 36001 iteration 0.06088504687363669\n",
      "current loss at 36011 iteration 0.06088502132445365\n",
      "current loss at 36021 iteration 0.0608849957900944\n",
      "current loss at 36031 iteration 0.0608849702705461\n",
      "current loss at 36041 iteration 0.06088494476579598\n",
      "current loss at 36051 iteration 0.06088491927583127\n",
      "current loss at 36061 iteration 0.06088489380063922\n",
      "current loss at 36071 iteration 0.060884868340207045\n",
      "current loss at 36081 iteration 0.06088484289452204\n",
      "current loss at 36091 iteration 0.06088481746357149\n",
      "current loss at 36101 iteration 0.06088479204734268\n",
      "current loss at 36111 iteration 0.06088476664582292\n",
      "current loss at 36121 iteration 0.06088474125899955\n",
      "current loss at 36131 iteration 0.06088471588685992\n",
      "current loss at 36141 iteration 0.06088469052939135\n",
      "current loss at 36151 iteration 0.060884665186581234\n",
      "current loss at 36161 iteration 0.06088463985841697\n",
      "current loss at 36171 iteration 0.06088461454488593\n",
      "current loss at 36181 iteration 0.060884589245975534\n",
      "current loss at 36191 iteration 0.060884563961673216\n",
      "current loss at 36201 iteration 0.0608845386919664\n",
      "current loss at 36211 iteration 0.060884513436842584\n",
      "current loss at 36221 iteration 0.060884488196289194\n",
      "current loss at 36231 iteration 0.06088446297029374\n",
      "current loss at 36241 iteration 0.06088443775884372\n",
      "current loss at 36251 iteration 0.060884412561926636\n",
      "current loss at 36261 iteration 0.06088438737953001\n",
      "current loss at 36271 iteration 0.060884362211641396\n",
      "current loss at 36281 iteration 0.06088433705824835\n",
      "current loss at 36291 iteration 0.060884311919338456\n",
      "current loss at 36301 iteration 0.06088428679489926\n",
      "current loss at 36311 iteration 0.0608842616849184\n",
      "current loss at 36321 iteration 0.06088423658938347\n",
      "current loss at 36331 iteration 0.060884211508282084\n",
      "current loss at 36341 iteration 0.0608841864416019\n",
      "current loss at 36351 iteration 0.06088416138933058\n",
      "current loss at 36361 iteration 0.060884136351455766\n",
      "current loss at 36371 iteration 0.06088411132796518\n",
      "current loss at 36381 iteration 0.06088408631884648\n",
      "current loss at 36391 iteration 0.0608840613240874\n",
      "current loss at 36401 iteration 0.060884036343675646\n",
      "current loss at 36411 iteration 0.060884011377598975\n",
      "current loss at 36421 iteration 0.060883986425845124\n",
      "current loss at 36431 iteration 0.06088396148840187\n",
      "current loss at 36441 iteration 0.06088393656525699\n",
      "current loss at 36451 iteration 0.06088391165639827\n",
      "current loss at 36461 iteration 0.06088388676181354\n",
      "current loss at 36471 iteration 0.06088386188149059\n",
      "current loss at 36481 iteration 0.060883837015417296\n",
      "current loss at 36491 iteration 0.06088381216358147\n",
      "current loss at 36501 iteration 0.06088378732597098\n",
      "current loss at 36511 iteration 0.06088376250257372\n",
      "current loss at 36521 iteration 0.06088373769337758\n",
      "current loss at 36531 iteration 0.06088371289837044\n",
      "current loss at 36541 iteration 0.060883688117540254\n",
      "current loss at 36551 iteration 0.0608836633508749\n",
      "current loss at 36561 iteration 0.060883638598362384\n",
      "current loss at 36571 iteration 0.060883613859990623\n",
      "current loss at 36581 iteration 0.060883589135747616\n",
      "current loss at 36591 iteration 0.06088356442562134\n",
      "current loss at 36601 iteration 0.06088353972959976\n",
      "current loss at 36611 iteration 0.06088351504767094\n",
      "current loss at 36621 iteration 0.06088349037982288\n",
      "current loss at 36631 iteration 0.06088346572604361\n",
      "current loss at 36641 iteration 0.06088344108632123\n",
      "current loss at 36651 iteration 0.060883416460643755\n",
      "current loss at 36661 iteration 0.060883391848999296\n",
      "current loss at 36671 iteration 0.060883367251375935\n",
      "current loss at 36681 iteration 0.060883342667761776\n",
      "current loss at 36691 iteration 0.06088331809814494\n",
      "current loss at 36701 iteration 0.06088329354251357\n",
      "current loss at 36711 iteration 0.060883269000855805\n",
      "current loss at 36721 iteration 0.0608832444731598\n",
      "current loss at 36731 iteration 0.06088321995941375\n",
      "current loss at 36741 iteration 0.06088319545960583\n",
      "current loss at 36751 iteration 0.060883170973724234\n",
      "current loss at 36761 iteration 0.06088314650175719\n",
      "current loss at 36771 iteration 0.060883122043692915\n",
      "current loss at 36781 iteration 0.06088309759951963\n",
      "current loss at 36791 iteration 0.06088307316922562\n",
      "current loss at 36801 iteration 0.06088304875279915\n",
      "current loss at 36811 iteration 0.060883024350228486\n",
      "current loss at 36821 iteration 0.06088299996150193\n",
      "current loss at 36831 iteration 0.060882975586607775\n",
      "current loss at 36841 iteration 0.060882951225534346\n",
      "current loss at 36851 iteration 0.06088292687826998\n",
      "current loss at 36861 iteration 0.06088290254480301\n",
      "current loss at 36871 iteration 0.06088287822512181\n",
      "current loss at 36881 iteration 0.06088285391921475\n",
      "current loss at 36891 iteration 0.06088282962707021\n",
      "current loss at 36901 iteration 0.06088280534867658\n",
      "current loss at 36911 iteration 0.060882781084022265\n",
      "current loss at 36921 iteration 0.060882756833095714\n",
      "current loss at 36931 iteration 0.06088273259588536\n",
      "current loss at 36941 iteration 0.06088270837237962\n",
      "current loss at 36951 iteration 0.06088268416256697\n",
      "current loss at 36961 iteration 0.06088265996643591\n",
      "current loss at 36971 iteration 0.06088263578397492\n",
      "current loss at 36981 iteration 0.060882611615172465\n",
      "current loss at 36991 iteration 0.060882587460017096\n",
      "current loss at 37001 iteration 0.06088256331849732\n",
      "current loss at 37011 iteration 0.06088253919060169\n",
      "current loss at 37021 iteration 0.06088251507631875\n",
      "current loss at 37031 iteration 0.06088249097563706\n",
      "current loss at 37041 iteration 0.0608824668885452\n",
      "current loss at 37051 iteration 0.06088244281503178\n",
      "current loss at 37061 iteration 0.06088241875508538\n",
      "current loss at 37071 iteration 0.06088239470869462\n",
      "current loss at 37081 iteration 0.06088237067584812\n",
      "current loss at 37091 iteration 0.060882346656534546\n",
      "current loss at 37101 iteration 0.060882322650742525\n",
      "current loss at 37111 iteration 0.06088229865846073\n",
      "current loss at 37121 iteration 0.06088227467967785\n",
      "current loss at 37131 iteration 0.060882250714382566\n",
      "current loss at 37141 iteration 0.06088222676256361\n",
      "current loss at 37151 iteration 0.06088220282420964\n",
      "current loss at 37161 iteration 0.060882178899309435\n",
      "current loss at 37171 iteration 0.06088215498785172\n",
      "current loss at 37181 iteration 0.06088213108982526\n",
      "current loss at 37191 iteration 0.06088210720521879\n",
      "current loss at 37201 iteration 0.06088208333402112\n",
      "current loss at 37211 iteration 0.06088205947622102\n",
      "current loss at 37221 iteration 0.06088203563180732\n",
      "current loss at 37231 iteration 0.060882011800768826\n",
      "current loss at 37241 iteration 0.06088198798309435\n",
      "current loss at 37251 iteration 0.06088196417877275\n",
      "current loss at 37261 iteration 0.060881940387792884\n",
      "current loss at 37271 iteration 0.06088191661014359\n",
      "current loss at 37281 iteration 0.060881892845813766\n",
      "current loss at 37291 iteration 0.060881869094792304\n",
      "current loss at 37301 iteration 0.060881845357068115\n",
      "current loss at 37311 iteration 0.06088182163263009\n",
      "current loss at 37321 iteration 0.06088179792146717\n",
      "current loss at 37331 iteration 0.0608817742235683\n",
      "current loss at 37341 iteration 0.06088175053892242\n",
      "current loss at 37351 iteration 0.0608817268675185\n",
      "current loss at 37361 iteration 0.06088170320934551\n",
      "current loss at 37371 iteration 0.06088167956439245\n",
      "current loss at 37381 iteration 0.0608816559326483\n",
      "current loss at 37391 iteration 0.0608816323141021\n",
      "current loss at 37401 iteration 0.06088160870874285\n",
      "current loss at 37411 iteration 0.06088158511655961\n",
      "current loss at 37421 iteration 0.0608815615375414\n",
      "current loss at 37431 iteration 0.06088153797167731\n",
      "current loss at 37441 iteration 0.060881514418956395\n",
      "current loss at 37451 iteration 0.06088149087936775\n",
      "current loss at 37461 iteration 0.060881467352900454\n",
      "current loss at 37471 iteration 0.06088144383954365\n",
      "current loss at 37481 iteration 0.060881420339286424\n",
      "current loss at 37491 iteration 0.060881396852117936\n",
      "current loss at 37501 iteration 0.06088137337802731\n",
      "current loss at 37511 iteration 0.0608813499170037\n",
      "current loss at 37521 iteration 0.06088132646903631\n",
      "current loss at 37531 iteration 0.060881303034114286\n",
      "current loss at 37541 iteration 0.06088127961222683\n",
      "current loss at 37551 iteration 0.060881256203363154\n",
      "current loss at 37561 iteration 0.06088123280751247\n",
      "current loss at 37571 iteration 0.060881209424664004\n",
      "current loss at 37581 iteration 0.060881186054807\n",
      "current loss at 37591 iteration 0.06088116269793071\n",
      "current loss at 37601 iteration 0.06088113935402438\n",
      "current loss at 37611 iteration 0.06088111602307732\n",
      "current loss at 37621 iteration 0.060881092705078785\n",
      "current loss at 37631 iteration 0.060881069400018105\n",
      "current loss at 37641 iteration 0.06088104610788456\n",
      "current loss at 37651 iteration 0.06088102282866749\n",
      "current loss at 37661 iteration 0.06088099956235623\n",
      "current loss at 37671 iteration 0.06088097630894012\n",
      "current loss at 37681 iteration 0.06088095306840852\n",
      "current loss at 37691 iteration 0.060880929840750775\n",
      "current loss at 37701 iteration 0.06088090662595632\n",
      "current loss at 37711 iteration 0.060880883424014494\n",
      "current loss at 37721 iteration 0.06088086023491474\n",
      "current loss at 37731 iteration 0.06088083705864644\n",
      "current loss at 37741 iteration 0.06088081389519906\n",
      "current loss at 37751 iteration 0.06088079074456201\n",
      "current loss at 37761 iteration 0.06088076760672475\n",
      "current loss at 37771 iteration 0.060880744481676734\n",
      "current loss at 37781 iteration 0.06088072136940745\n",
      "current loss at 37791 iteration 0.06088069826990636\n",
      "current loss at 37801 iteration 0.06088067518316298\n",
      "current loss at 37811 iteration 0.06088065210916682\n",
      "current loss at 37821 iteration 0.06088062904790739\n",
      "current loss at 37831 iteration 0.06088060599937422\n",
      "current loss at 37841 iteration 0.06088058296355684\n",
      "current loss at 37851 iteration 0.06088055994044485\n",
      "current loss at 37861 iteration 0.060880536930027755\n",
      "current loss at 37871 iteration 0.060880513932295176\n",
      "current loss at 37881 iteration 0.06088049094723669\n",
      "current loss at 37891 iteration 0.060880467974841875\n",
      "current loss at 37901 iteration 0.06088044501510037\n",
      "current loss at 37911 iteration 0.06088042206800177\n",
      "current loss at 37921 iteration 0.060880399133535726\n",
      "current loss at 37931 iteration 0.06088037621169188\n",
      "current loss at 37941 iteration 0.06088035330245989\n",
      "current loss at 37951 iteration 0.06088033040582942\n",
      "current loss at 37961 iteration 0.06088030752179014\n",
      "current loss at 37971 iteration 0.06088028465033175\n",
      "current loss at 37981 iteration 0.060880261791443936\n",
      "current loss at 37991 iteration 0.06088023894511643\n",
      "current loss at 38001 iteration 0.060880216111338946\n",
      "current loss at 38011 iteration 0.060880193290101206\n",
      "current loss at 38021 iteration 0.06088017048139297\n",
      "current loss at 38031 iteration 0.060880147685203986\n",
      "current loss at 38041 iteration 0.06088012490152403\n",
      "current loss at 38051 iteration 0.06088010213034288\n",
      "current loss at 38061 iteration 0.06088007937165031\n",
      "current loss at 38071 iteration 0.06088005662543615\n",
      "current loss at 38081 iteration 0.06088003389169018\n",
      "current loss at 38091 iteration 0.06088001117040223\n",
      "current loss at 38101 iteration 0.060879988461562146\n",
      "current loss at 38111 iteration 0.06087996576515978\n",
      "current loss at 38121 iteration 0.06087994308118497\n",
      "current loss at 38131 iteration 0.060879920409627565\n",
      "current loss at 38141 iteration 0.06087989775047748\n",
      "current loss at 38151 iteration 0.060879875103724584\n",
      "current loss at 38161 iteration 0.060879852469358786\n",
      "current loss at 38171 iteration 0.06087982984737\n",
      "current loss at 38181 iteration 0.060879807237748135\n",
      "current loss at 38191 iteration 0.06087978464048311\n",
      "current loss at 38201 iteration 0.060879762055564904\n",
      "current loss at 38211 iteration 0.06087973948298343\n",
      "current loss at 38221 iteration 0.06087971692272869\n",
      "current loss at 38231 iteration 0.06087969437479065\n",
      "current loss at 38241 iteration 0.06087967183915929\n",
      "current loss at 38251 iteration 0.060879649315824604\n",
      "current loss at 38261 iteration 0.060879626804776606\n",
      "current loss at 38271 iteration 0.0608796043060053\n",
      "current loss at 38281 iteration 0.06087958181950074\n",
      "current loss at 38291 iteration 0.06087955934525297\n",
      "current loss at 38301 iteration 0.060879536883252\n",
      "current loss at 38311 iteration 0.06087951443348795\n",
      "current loss at 38321 iteration 0.06087949199595084\n",
      "current loss at 38331 iteration 0.06087946957063076\n",
      "current loss at 38341 iteration 0.06087944715751784\n",
      "current loss at 38351 iteration 0.060879424756602164\n",
      "current loss at 38361 iteration 0.06087940236787384\n",
      "current loss at 38371 iteration 0.06087937999132301\n",
      "current loss at 38381 iteration 0.06087935762693978\n",
      "current loss at 38391 iteration 0.06087933527471433\n",
      "current loss at 38401 iteration 0.06087931293463679\n",
      "current loss at 38411 iteration 0.06087929060669736\n",
      "current loss at 38421 iteration 0.060879268290886204\n",
      "current loss at 38431 iteration 0.06087924598719349\n",
      "current loss at 38441 iteration 0.06087922369560945\n",
      "current loss at 38451 iteration 0.06087920141612429\n",
      "current loss at 38461 iteration 0.0608791791487282\n",
      "current loss at 38471 iteration 0.06087915689341145\n",
      "current loss at 38481 iteration 0.06087913465016426\n",
      "current loss at 38491 iteration 0.060879112418976894\n",
      "current loss at 38501 iteration 0.0608790901998396\n",
      "current loss at 38511 iteration 0.06087906799274266\n",
      "current loss at 38521 iteration 0.06087904579767636\n",
      "current loss at 38531 iteration 0.060879023614630996\n",
      "current loss at 38541 iteration 0.06087900144359685\n",
      "current loss at 38551 iteration 0.06087897928456428\n",
      "current loss at 38561 iteration 0.06087895713752357\n",
      "current loss at 38571 iteration 0.06087893500246507\n",
      "current loss at 38581 iteration 0.060878912879379125\n",
      "current loss at 38591 iteration 0.06087889076825611\n",
      "current loss at 38601 iteration 0.06087886866908635\n",
      "current loss at 38611 iteration 0.06087884658186027\n",
      "current loss at 38621 iteration 0.060878824506568216\n",
      "current loss at 38631 iteration 0.060878802443200604\n",
      "current loss at 38641 iteration 0.06087878039174785\n",
      "current loss at 38651 iteration 0.06087875835220036\n",
      "current loss at 38661 iteration 0.06087873632454856\n",
      "current loss at 38671 iteration 0.06087871430878289\n",
      "current loss at 38681 iteration 0.060878692304893796\n",
      "current loss at 38691 iteration 0.06087867031287174\n",
      "current loss at 38701 iteration 0.0608786483327072\n",
      "current loss at 38711 iteration 0.06087862636439065\n",
      "current loss at 38721 iteration 0.06087860440791257\n",
      "current loss at 38731 iteration 0.06087858246326347\n",
      "current loss at 38741 iteration 0.06087856053043386\n",
      "current loss at 38751 iteration 0.060878538609414244\n",
      "current loss at 38761 iteration 0.06087851670019516\n",
      "current loss at 38771 iteration 0.06087849480276716\n",
      "current loss at 38781 iteration 0.06087847291712076\n",
      "current loss at 38791 iteration 0.06087845104324655\n",
      "current loss at 38801 iteration 0.06087842918113511\n",
      "current loss at 38811 iteration 0.060878407330777\n",
      "current loss at 38821 iteration 0.06087838549216278\n",
      "current loss at 38831 iteration 0.06087836366528311\n",
      "current loss at 38841 iteration 0.060878341850128574\n",
      "current loss at 38851 iteration 0.06087832004668978\n",
      "current loss at 38861 iteration 0.06087829825495737\n",
      "current loss at 38871 iteration 0.06087827647492198\n",
      "current loss at 38881 iteration 0.06087825470657425\n",
      "current loss at 38891 iteration 0.06087823294990486\n",
      "current loss at 38901 iteration 0.06087821120490447\n",
      "current loss at 38911 iteration 0.06087818947156377\n",
      "current loss at 38921 iteration 0.06087816774987341\n",
      "current loss at 38931 iteration 0.06087814603982414\n",
      "current loss at 38941 iteration 0.06087812434140663\n",
      "current loss at 38951 iteration 0.060878102654611625\n",
      "current loss at 38961 iteration 0.06087808097942983\n",
      "current loss at 38971 iteration 0.06087805931585201\n",
      "current loss at 38981 iteration 0.0608780376638689\n",
      "current loss at 38991 iteration 0.06087801602347125\n",
      "current loss at 39001 iteration 0.06087799439464984\n",
      "current loss at 39011 iteration 0.060877972777395435\n",
      "current loss at 39021 iteration 0.060877951171698835\n",
      "current loss at 39031 iteration 0.060877929577550845\n",
      "current loss at 39041 iteration 0.06087790799494224\n",
      "current loss at 39051 iteration 0.06087788642386386\n",
      "current loss at 39061 iteration 0.06087786486430654\n",
      "current loss at 39071 iteration 0.06087784331626108\n",
      "current loss at 39081 iteration 0.06087782177971838\n",
      "current loss at 39091 iteration 0.06087780025466923\n",
      "current loss at 39101 iteration 0.06087777874110453\n",
      "current loss at 39111 iteration 0.06087775723901518\n",
      "current loss at 39121 iteration 0.060877735748392024\n",
      "current loss at 39131 iteration 0.06087771426922596\n",
      "current loss at 39141 iteration 0.06087769280150791\n",
      "current loss at 39151 iteration 0.06087767134522878\n",
      "current loss at 39161 iteration 0.06087764990037947\n",
      "current loss at 39171 iteration 0.060877628466950946\n",
      "current loss at 39181 iteration 0.06087760704493413\n",
      "current loss at 39191 iteration 0.06087758563431998\n",
      "current loss at 39201 iteration 0.06087756423509944\n",
      "current loss at 39211 iteration 0.060877542847263516\n",
      "current loss at 39221 iteration 0.06087752147080315\n",
      "current loss at 39231 iteration 0.06087750010570936\n",
      "current loss at 39241 iteration 0.060877478751973135\n",
      "current loss at 39251 iteration 0.06087745740958547\n",
      "current loss at 39261 iteration 0.0608774360785374\n",
      "current loss at 39271 iteration 0.06087741475881994\n",
      "current loss at 39281 iteration 0.06087739345042412\n",
      "current loss at 39291 iteration 0.06087737215334102\n",
      "current loss at 39301 iteration 0.060877350867561644\n",
      "current loss at 39311 iteration 0.060877329593077095\n",
      "current loss at 39321 iteration 0.06087730832987844\n",
      "current loss at 39331 iteration 0.06087728707795675\n",
      "current loss at 39341 iteration 0.06087726583730314\n",
      "current loss at 39351 iteration 0.06087724460790869\n",
      "current loss at 39361 iteration 0.06087722338976452\n",
      "current loss at 39371 iteration 0.06087720218286176\n",
      "current loss at 39381 iteration 0.06087718098719151\n",
      "current loss at 39391 iteration 0.06087715980274495\n",
      "current loss at 39401 iteration 0.06087713862951319\n",
      "current loss at 39411 iteration 0.0608771174674874\n",
      "current loss at 39421 iteration 0.06087709631665878\n",
      "current loss at 39431 iteration 0.06087707517701846\n",
      "current loss at 39441 iteration 0.06087705404855764\n",
      "current loss at 39451 iteration 0.060877032931267526\n",
      "current loss at 39461 iteration 0.06087701182513932\n",
      "current loss at 39471 iteration 0.06087699073016422\n",
      "current loss at 39481 iteration 0.060876969646333455\n",
      "current loss at 39491 iteration 0.06087694857363827\n",
      "current loss at 39501 iteration 0.06087692751206989\n",
      "current loss at 39511 iteration 0.060876906461619584\n",
      "current loss at 39521 iteration 0.06087688542227857\n",
      "current loss at 39531 iteration 0.060876864394038166\n",
      "current loss at 39541 iteration 0.0608768433768896\n",
      "current loss at 39551 iteration 0.06087682237082421\n",
      "current loss at 39561 iteration 0.060876801375833255\n",
      "current loss at 39571 iteration 0.06087678039190805\n",
      "current loss at 39581 iteration 0.0608767594190399\n",
      "current loss at 39591 iteration 0.06087673845722016\n",
      "current loss at 39601 iteration 0.06087671750644011\n",
      "current loss at 39611 iteration 0.06087669656669112\n",
      "current loss at 39621 iteration 0.06087667563796454\n",
      "current loss at 39631 iteration 0.06087665472025172\n",
      "current loss at 39641 iteration 0.06087663381354406\n",
      "current loss at 39651 iteration 0.06087661291783289\n",
      "current loss at 39661 iteration 0.06087659203310961\n",
      "current loss at 39671 iteration 0.06087657115936562\n",
      "current loss at 39681 iteration 0.060876550296592336\n",
      "current loss at 39691 iteration 0.060876529444781155\n",
      "current loss at 39701 iteration 0.06087650860392349\n",
      "current loss at 39711 iteration 0.06087648777401078\n",
      "current loss at 39721 iteration 0.06087646695503447\n",
      "current loss at 39731 iteration 0.060876446146986014\n",
      "current loss at 39741 iteration 0.060876425349856836\n",
      "current loss at 39751 iteration 0.06087640456363846\n",
      "current loss at 39761 iteration 0.06087638378832229\n",
      "current loss at 39771 iteration 0.06087636302389986\n",
      "current loss at 39781 iteration 0.060876342270362635\n",
      "current loss at 39791 iteration 0.06087632152770214\n",
      "current loss at 39801 iteration 0.06087630079590986\n",
      "current loss at 39811 iteration 0.060876280074977335\n",
      "current loss at 39821 iteration 0.060876259364896074\n",
      "current loss at 39831 iteration 0.06087623866565764\n",
      "current loss at 39841 iteration 0.06087621797725354\n",
      "current loss at 39851 iteration 0.060876197299675354\n",
      "current loss at 39861 iteration 0.06087617663291464\n",
      "current loss at 39871 iteration 0.06087615597696297\n",
      "current loss at 39881 iteration 0.06087613533181191\n",
      "current loss at 39891 iteration 0.060876114697453056\n",
      "current loss at 39901 iteration 0.060876094073878034\n",
      "current loss at 39911 iteration 0.0608760734610784\n",
      "current loss at 39921 iteration 0.060876052859045814\n",
      "current loss at 39931 iteration 0.06087603226777188\n",
      "current loss at 39941 iteration 0.06087601168724821\n",
      "current loss at 39951 iteration 0.06087599111746648\n",
      "current loss at 39961 iteration 0.0608759705584183\n",
      "current loss at 39971 iteration 0.06087595001009538\n",
      "current loss at 39981 iteration 0.060875929472489335\n",
      "current loss at 39991 iteration 0.06087590894559187\n",
      "current loss at 40001 iteration 0.060875888429394655\n",
      "current loss at 40011 iteration 0.0608758679238894\n",
      "current loss at 40021 iteration 0.060875847429067785\n",
      "current loss at 40031 iteration 0.060875826944921524\n",
      "current loss at 40041 iteration 0.06087580647144234\n",
      "current loss at 40051 iteration 0.060875786008621965\n",
      "current loss at 40061 iteration 0.06087576555645212\n",
      "current loss at 40071 iteration 0.06087574511492455\n",
      "current loss at 40081 iteration 0.06087572468403103\n",
      "current loss at 40091 iteration 0.060875704263763285\n",
      "current loss at 40101 iteration 0.0608756838541131\n",
      "current loss at 40111 iteration 0.06087566345507226\n",
      "current loss at 40121 iteration 0.06087564306663256\n",
      "current loss at 40131 iteration 0.060875622688785755\n",
      "current loss at 40141 iteration 0.060875602321523684\n",
      "current loss at 40151 iteration 0.06087558196483814\n",
      "current loss at 40161 iteration 0.06087556161872095\n",
      "current loss at 40171 iteration 0.06087554128316394\n",
      "current loss at 40181 iteration 0.06087552095815893\n",
      "current loss at 40191 iteration 0.06087550064369781\n",
      "current loss at 40201 iteration 0.06087548033977238\n",
      "current loss at 40211 iteration 0.060875460046374515\n",
      "current loss at 40221 iteration 0.06087543976349611\n",
      "current loss at 40231 iteration 0.06087541949112904\n",
      "current loss at 40241 iteration 0.060875399229265166\n",
      "current loss at 40251 iteration 0.06087537897789638\n",
      "current loss at 40261 iteration 0.06087535873701463\n",
      "current loss at 40271 iteration 0.06087533850661177\n",
      "current loss at 40281 iteration 0.060875318286679775\n",
      "current loss at 40291 iteration 0.06087529807721052\n",
      "current loss at 40301 iteration 0.06087527787819596\n",
      "current loss at 40311 iteration 0.06087525768962806\n",
      "current loss at 40321 iteration 0.06087523751149875\n",
      "current loss at 40331 iteration 0.060875217343800005\n",
      "current loss at 40341 iteration 0.06087519718652379\n",
      "current loss at 40351 iteration 0.06087517703966207\n",
      "current loss at 40361 iteration 0.06087515690320685\n",
      "current loss at 40371 iteration 0.0608751367771501\n",
      "current loss at 40381 iteration 0.06087511666148385\n",
      "current loss at 40391 iteration 0.060875096556200076\n",
      "current loss at 40401 iteration 0.06087507646129083\n",
      "current loss at 40411 iteration 0.06087505637674811\n",
      "current loss at 40421 iteration 0.06087503630256397\n",
      "current loss at 40431 iteration 0.060875016238730456\n",
      "current loss at 40441 iteration 0.06087499618523959\n",
      "current loss at 40451 iteration 0.06087497614208345\n",
      "current loss at 40461 iteration 0.06087495610925412\n",
      "current loss at 40471 iteration 0.060874936086743645\n",
      "current loss at 40481 iteration 0.06087491607454413\n",
      "current loss at 40491 iteration 0.06087489607264765\n",
      "current loss at 40501 iteration 0.060874876081046296\n",
      "current loss at 40511 iteration 0.060874856099732214\n",
      "current loss at 40521 iteration 0.06087483612869747\n",
      "current loss at 40531 iteration 0.06087481616793422\n",
      "current loss at 40541 iteration 0.060874796217434586\n",
      "current loss at 40551 iteration 0.060874776277190706\n",
      "current loss at 40561 iteration 0.06087475634719472\n",
      "current loss at 40571 iteration 0.06087473642743882\n",
      "current loss at 40581 iteration 0.06087471651791512\n",
      "current loss at 40591 iteration 0.0608746966186158\n",
      "current loss at 40601 iteration 0.060874676729533075\n",
      "current loss at 40611 iteration 0.060874656850659105\n",
      "current loss at 40621 iteration 0.06087463698198608\n",
      "current loss at 40631 iteration 0.0608746171235062\n",
      "current loss at 40641 iteration 0.06087459727521169\n",
      "current loss at 40651 iteration 0.06087457743709476\n",
      "current loss at 40661 iteration 0.060874557609147666\n",
      "current loss at 40671 iteration 0.0608745377913626\n",
      "current loss at 40681 iteration 0.06087451798373183\n",
      "current loss at 40691 iteration 0.0608744981862476\n",
      "current loss at 40701 iteration 0.06087447839890216\n",
      "current loss at 40711 iteration 0.06087445862168777\n",
      "current loss at 40721 iteration 0.06087443885459672\n",
      "current loss at 40731 iteration 0.060874419097621285\n",
      "current loss at 40741 iteration 0.06087439935075377\n",
      "current loss at 40751 iteration 0.06087437961398644\n",
      "current loss at 40761 iteration 0.06087435988731164\n",
      "current loss at 40771 iteration 0.06087434017072165\n",
      "current loss at 40781 iteration 0.06087432046420879\n",
      "current loss at 40791 iteration 0.060874300767765406\n",
      "current loss at 40801 iteration 0.06087428108138383\n",
      "current loss at 40811 iteration 0.0608742614050564\n",
      "current loss at 40821 iteration 0.06087424173877546\n",
      "current loss at 40831 iteration 0.06087422208253341\n",
      "current loss at 40841 iteration 0.06087420243632255\n",
      "current loss at 40851 iteration 0.06087418280013531\n",
      "current loss at 40861 iteration 0.06087416317396404\n",
      "current loss at 40871 iteration 0.06087414355780116\n",
      "current loss at 40881 iteration 0.060874123951639034\n",
      "current loss at 40891 iteration 0.0608741043554701\n",
      "current loss at 40901 iteration 0.06087408476928675\n",
      "current loss at 40911 iteration 0.06087406519308139\n",
      "current loss at 40921 iteration 0.06087404562684648\n",
      "current loss at 40931 iteration 0.06087402607057445\n",
      "current loss at 40941 iteration 0.06087400652425772\n",
      "current loss at 40951 iteration 0.06087398698788877\n",
      "current loss at 40961 iteration 0.060873967461460045\n",
      "current loss at 40971 iteration 0.06087394794496401\n",
      "current loss at 40981 iteration 0.060873928438393135\n",
      "current loss at 40991 iteration 0.06087390894173991\n",
      "current loss at 41001 iteration 0.06087388945499682\n",
      "current loss at 41011 iteration 0.06087386997815637\n",
      "current loss at 41021 iteration 0.06087385051121105\n",
      "current loss at 41031 iteration 0.060873831054153386\n",
      "current loss at 41041 iteration 0.060873811606975886\n",
      "current loss at 41051 iteration 0.06087379216967107\n",
      "current loss at 41061 iteration 0.06087377274223149\n",
      "current loss at 41071 iteration 0.06087375332464968\n",
      "current loss at 41081 iteration 0.06087373391691818\n",
      "current loss at 41091 iteration 0.060873714519029565\n",
      "current loss at 41101 iteration 0.0608736951309764\n",
      "current loss at 41111 iteration 0.06087367575275123\n",
      "current loss at 41121 iteration 0.06087365638434666\n",
      "current loss at 41131 iteration 0.060873637025755246\n",
      "current loss at 41141 iteration 0.060873617676969624\n",
      "current loss at 41151 iteration 0.060873598337982365\n",
      "current loss at 41161 iteration 0.060873579008786066\n",
      "current loss at 41171 iteration 0.060873559689373385\n",
      "current loss at 41181 iteration 0.060873540379736926\n",
      "current loss at 41191 iteration 0.06087352107986929\n",
      "current loss at 41201 iteration 0.060873501789763175\n",
      "current loss at 41211 iteration 0.060873482509411186\n",
      "current loss at 41221 iteration 0.060873463238805964\n",
      "current loss at 41231 iteration 0.06087344397794021\n",
      "current loss at 41241 iteration 0.060873424726806555\n",
      "current loss at 41251 iteration 0.06087340548539769\n",
      "current loss at 41261 iteration 0.06087338625370631\n",
      "current loss at 41271 iteration 0.06087336703172508\n",
      "current loss at 41281 iteration 0.060873347819446706\n",
      "current loss at 41291 iteration 0.0608733286168639\n",
      "current loss at 41301 iteration 0.06087330942396935\n",
      "current loss at 41311 iteration 0.060873290240755815\n",
      "current loss at 41321 iteration 0.060873271067215975\n",
      "current loss at 41331 iteration 0.0608732519033426\n",
      "current loss at 41341 iteration 0.06087323274912842\n",
      "current loss at 41351 iteration 0.06087321360456618\n",
      "current loss at 41361 iteration 0.06087319446964861\n",
      "current loss at 41371 iteration 0.06087317534436851\n",
      "current loss at 41381 iteration 0.060873156228718646\n",
      "current loss at 41391 iteration 0.060873137122691774\n",
      "current loss at 41401 iteration 0.060873118026280676\n",
      "current loss at 41411 iteration 0.06087309893947817\n",
      "current loss at 41421 iteration 0.06087307986227703\n",
      "current loss at 41431 iteration 0.06087306079467008\n",
      "current loss at 41441 iteration 0.06087304173665012\n",
      "current loss at 41451 iteration 0.06087302268820996\n",
      "current loss at 41461 iteration 0.06087300364934245\n",
      "current loss at 41471 iteration 0.0608729846200404\n",
      "current loss at 41481 iteration 0.06087296560029666\n",
      "current loss at 41491 iteration 0.0608729465901041\n",
      "current loss at 41501 iteration 0.06087292758945554\n",
      "current loss at 41511 iteration 0.06087290859834386\n",
      "current loss at 41521 iteration 0.060872889616761926\n",
      "current loss at 41531 iteration 0.06087287064470263\n",
      "current loss at 41541 iteration 0.06087285168215882\n",
      "current loss at 41551 iteration 0.06087283272912342\n",
      "current loss at 41561 iteration 0.06087281378558932\n",
      "current loss at 41571 iteration 0.06087279485154941\n",
      "current loss at 41581 iteration 0.060872775926996615\n",
      "current loss at 41591 iteration 0.06087275701192385\n",
      "current loss at 41601 iteration 0.06087273810632405\n",
      "current loss at 41611 iteration 0.060872719210190135\n",
      "current loss at 41621 iteration 0.06087270032351503\n",
      "current loss at 41631 iteration 0.06087268144629172\n",
      "current loss at 41641 iteration 0.06087266257851313\n",
      "current loss at 41651 iteration 0.06087264372017223\n",
      "current loss at 41661 iteration 0.06087262487126199\n",
      "current loss at 41671 iteration 0.06087260603177537\n",
      "current loss at 41681 iteration 0.06087258720170537\n",
      "current loss at 41691 iteration 0.06087256838104498\n",
      "current loss at 41701 iteration 0.06087254956978719\n",
      "current loss at 41711 iteration 0.06087253076792498\n",
      "current loss at 41721 iteration 0.06087251197545139\n",
      "current loss at 41731 iteration 0.06087249319235942\n",
      "current loss at 41741 iteration 0.0608724744186421\n",
      "current loss at 41751 iteration 0.06087245565429246\n",
      "current loss at 41761 iteration 0.060872436899303536\n",
      "current loss at 41771 iteration 0.06087241815366836\n",
      "current loss at 41781 iteration 0.06087239941738\n",
      "current loss at 41791 iteration 0.0608723806904315\n",
      "current loss at 41801 iteration 0.06087236197281593\n",
      "current loss at 41811 iteration 0.06087234326452637\n",
      "current loss at 41821 iteration 0.060872324565555865\n",
      "current loss at 41831 iteration 0.06087230587589754\n",
      "current loss at 41841 iteration 0.06087228719554447\n",
      "current loss at 41851 iteration 0.06087226852448976\n",
      "current loss at 41861 iteration 0.0608722498627265\n",
      "current loss at 41871 iteration 0.0608722312102478\n",
      "current loss at 41881 iteration 0.06087221256704681\n",
      "current loss at 41891 iteration 0.06087219393311662\n",
      "current loss at 41901 iteration 0.06087217530845038\n",
      "current loss at 41911 iteration 0.06087215669304121\n",
      "current loss at 41921 iteration 0.0608721380868823\n",
      "current loss at 41931 iteration 0.06087211948996675\n",
      "current loss at 41941 iteration 0.06087210090228776\n",
      "current loss at 41951 iteration 0.060872082323838464\n",
      "current loss at 41961 iteration 0.060872063754612044\n",
      "current loss at 41971 iteration 0.06087204519460169\n",
      "current loss at 41981 iteration 0.0608720266438006\n",
      "current loss at 41991 iteration 0.06087200810220192\n",
      "current loss at 42001 iteration 0.060871989569798904\n",
      "current loss at 42011 iteration 0.06087197104658472\n",
      "current loss at 42021 iteration 0.06087195253255259\n",
      "current loss at 42031 iteration 0.06087193402769574\n",
      "current loss at 42041 iteration 0.0608719155320074\n",
      "current loss at 42051 iteration 0.060871897045480794\n",
      "current loss at 42061 iteration 0.06087187856810915\n",
      "current loss at 42071 iteration 0.06087186009988574\n",
      "current loss at 42081 iteration 0.0608718416408038\n",
      "current loss at 42091 iteration 0.06087182319085657\n",
      "current loss at 42101 iteration 0.06087180475003736\n",
      "current loss at 42111 iteration 0.060871786318339415\n",
      "current loss at 42121 iteration 0.060871767895756025\n",
      "current loss at 42131 iteration 0.060871749482280454\n",
      "current loss at 42141 iteration 0.06087173107790601\n",
      "current loss at 42151 iteration 0.060871712682626006\n",
      "current loss at 42161 iteration 0.060871694296433715\n",
      "current loss at 42171 iteration 0.06087167591932247\n",
      "current loss at 42181 iteration 0.06087165755128559\n",
      "current loss at 42191 iteration 0.06087163919231639\n",
      "current loss at 42201 iteration 0.06087162084240821\n",
      "current loss at 42211 iteration 0.06087160250155437\n",
      "current loss at 42221 iteration 0.06087158416974825\n",
      "current loss at 42231 iteration 0.06087156584698315\n",
      "current loss at 42241 iteration 0.060871547533252475\n",
      "current loss at 42251 iteration 0.06087152922854956\n",
      "current loss at 42261 iteration 0.0608715109328678\n",
      "current loss at 42271 iteration 0.06087149264620054\n",
      "current loss at 42281 iteration 0.060871474368541185\n",
      "current loss at 42291 iteration 0.06087145609988312\n",
      "current loss at 42301 iteration 0.06087143784021974\n",
      "current loss at 42311 iteration 0.06087141958954442\n",
      "current loss at 42321 iteration 0.06087140134785064\n",
      "current loss at 42331 iteration 0.06087138311513175\n",
      "current loss at 42341 iteration 0.060871364891381195\n",
      "current loss at 42351 iteration 0.06087134667659241\n",
      "current loss at 42361 iteration 0.06087132847075879\n",
      "current loss at 42371 iteration 0.060871310273873824\n",
      "current loss at 42381 iteration 0.06087129208593094\n",
      "current loss at 42391 iteration 0.060871273906923595\n",
      "current loss at 42401 iteration 0.06087125573684525\n",
      "current loss at 42411 iteration 0.060871237575689366\n",
      "current loss at 42421 iteration 0.06087121942344942\n",
      "current loss at 42431 iteration 0.060871201280118886\n",
      "current loss at 42441 iteration 0.06087118314569125\n",
      "current loss at 42451 iteration 0.060871165020160004\n",
      "current loss at 42461 iteration 0.06087114690351866\n",
      "current loss at 42471 iteration 0.0608711287957607\n",
      "current loss at 42481 iteration 0.060871110696879636\n",
      "current loss at 42491 iteration 0.06087109260686901\n",
      "current loss at 42501 iteration 0.06087107452572232\n",
      "current loss at 42511 iteration 0.06087105645343311\n",
      "current loss at 42521 iteration 0.06087103838999489\n",
      "current loss at 42531 iteration 0.060871020335401246\n",
      "current loss at 42541 iteration 0.06087100228964569\n",
      "current loss at 42551 iteration 0.0608709842527218\n",
      "current loss at 42561 iteration 0.0608709662246231\n",
      "current loss at 42571 iteration 0.06087094820534321\n",
      "current loss at 42581 iteration 0.06087093019487566\n",
      "current loss at 42591 iteration 0.06087091219321404\n",
      "current loss at 42601 iteration 0.06087089420035195\n",
      "current loss at 42611 iteration 0.06087087621628297\n",
      "current loss at 42621 iteration 0.06087085824100069\n",
      "current loss at 42631 iteration 0.06087084027449874\n",
      "current loss at 42641 iteration 0.060870822316770705\n",
      "current loss at 42651 iteration 0.06087080436781021\n",
      "current loss at 42661 iteration 0.06087078642761089\n",
      "current loss at 42671 iteration 0.060870768496166336\n",
      "current loss at 42681 iteration 0.060870750573470245\n",
      "current loss at 42691 iteration 0.060870732659516195\n",
      "current loss at 42701 iteration 0.060870714754297874\n",
      "current loss at 42711 iteration 0.06087069685780891\n",
      "current loss at 42721 iteration 0.06087067897004298\n",
      "current loss at 42731 iteration 0.060870661090993754\n",
      "current loss at 42741 iteration 0.06087064322065489\n",
      "current loss at 42751 iteration 0.06087062535902007\n",
      "current loss at 42761 iteration 0.060870607506082974\n",
      "current loss at 42771 iteration 0.06087058966183729\n",
      "current loss at 42781 iteration 0.06087057182627672\n",
      "current loss at 42791 iteration 0.06087055399939498\n",
      "current loss at 42801 iteration 0.060870536181185764\n",
      "current loss at 42811 iteration 0.060870518371642764\n",
      "current loss at 42821 iteration 0.060870500570759746\n",
      "current loss at 42831 iteration 0.0608704827785304\n",
      "current loss at 42841 iteration 0.06087046499494848\n",
      "current loss at 42851 iteration 0.0608704472200077\n",
      "current loss at 42861 iteration 0.06087042945370184\n",
      "current loss at 42871 iteration 0.06087041169602462\n",
      "current loss at 42881 iteration 0.06087039394696982\n",
      "current loss at 42891 iteration 0.06087037620653118\n",
      "current loss at 42901 iteration 0.06087035847470246\n",
      "current loss at 42911 iteration 0.060870340751477486\n",
      "current loss at 42921 iteration 0.06087032303684998\n",
      "current loss at 42931 iteration 0.06087030533081377\n",
      "current loss at 42941 iteration 0.06087028763336263\n",
      "current loss at 42951 iteration 0.060870269944490345\n",
      "current loss at 42961 iteration 0.06087025226419075\n",
      "current loss at 42971 iteration 0.06087023459245763\n",
      "current loss at 42981 iteration 0.06087021692928482\n",
      "current loss at 42991 iteration 0.060870199274666116\n",
      "current loss at 43001 iteration 0.06087018162859538\n",
      "current loss at 43011 iteration 0.06087016399106642\n",
      "current loss at 43021 iteration 0.060870146362073094\n",
      "current loss at 43031 iteration 0.06087012874160923\n",
      "current loss at 43041 iteration 0.06087011112966868\n",
      "current loss at 43051 iteration 0.06087009352624532\n",
      "current loss at 43061 iteration 0.060870075931333006\n",
      "current loss at 43071 iteration 0.0608700583449256\n",
      "current loss at 43081 iteration 0.060870040767016964\n",
      "current loss at 43091 iteration 0.060870023197601014\n",
      "current loss at 43101 iteration 0.060870005636671616\n",
      "current loss at 43111 iteration 0.06086998808422265\n",
      "current loss at 43121 iteration 0.06086997054024804\n",
      "current loss at 43131 iteration 0.06086995300474168\n",
      "current loss at 43141 iteration 0.06086993547769748\n",
      "current loss at 43151 iteration 0.06086991795910934\n",
      "current loss at 43161 iteration 0.06086990044897121\n",
      "current loss at 43171 iteration 0.06086988294727701\n",
      "current loss at 43181 iteration 0.06086986545402066\n",
      "current loss at 43191 iteration 0.06086984796919612\n",
      "current loss at 43201 iteration 0.06086983049279731\n",
      "current loss at 43211 iteration 0.0608698130248182\n",
      "current loss at 43221 iteration 0.060869795565252724\n",
      "current loss at 43231 iteration 0.06086977811409486\n",
      "current loss at 43241 iteration 0.060869760671338584\n",
      "current loss at 43251 iteration 0.060869743236977865\n",
      "current loss at 43261 iteration 0.060869725811006664\n",
      "current loss at 43271 iteration 0.060869708393418986\n",
      "current loss at 43281 iteration 0.060869690984208816\n",
      "current loss at 43291 iteration 0.06086967358337014\n",
      "current loss at 43301 iteration 0.06086965619089699\n",
      "current loss at 43311 iteration 0.060869638806783345\n",
      "current loss at 43321 iteration 0.06086962143102322\n",
      "current loss at 43331 iteration 0.06086960406361065\n",
      "current loss at 43341 iteration 0.06086958670453965\n",
      "current loss at 43351 iteration 0.06086956935380426\n",
      "current loss at 43361 iteration 0.06086955201139851\n",
      "current loss at 43371 iteration 0.06086953467731645\n",
      "current loss at 43381 iteration 0.06086951735155209\n",
      "current loss at 43391 iteration 0.060869500034099536\n",
      "current loss at 43401 iteration 0.06086948272495282\n",
      "current loss at 43411 iteration 0.06086946542410601\n",
      "current loss at 43421 iteration 0.06086944813155318\n",
      "current loss at 43431 iteration 0.06086943084728839\n",
      "current loss at 43441 iteration 0.06086941357130574\n",
      "current loss at 43451 iteration 0.06086939630359932\n",
      "current loss at 43461 iteration 0.0608693790441632\n",
      "current loss at 43471 iteration 0.0608693617929915\n",
      "current loss at 43481 iteration 0.06086934455007832\n",
      "current loss at 43491 iteration 0.060869327315417755\n",
      "current loss at 43501 iteration 0.060869310089003946\n",
      "current loss at 43511 iteration 0.060869292870830996\n",
      "current loss at 43521 iteration 0.060869275660893014\n",
      "current loss at 43531 iteration 0.06086925845918418\n",
      "current loss at 43541 iteration 0.06086924126569858\n",
      "current loss at 43551 iteration 0.0608692240804304\n",
      "current loss at 43561 iteration 0.060869206903373746\n",
      "current loss at 43571 iteration 0.06086918973452281\n",
      "current loss at 43581 iteration 0.06086917257387173\n",
      "current loss at 43591 iteration 0.060869155421414675\n",
      "current loss at 43601 iteration 0.06086913827714581\n",
      "current loss at 43611 iteration 0.06086912114105933\n",
      "current loss at 43621 iteration 0.060869104013149405\n",
      "current loss at 43631 iteration 0.060869086893410224\n",
      "current loss at 43641 iteration 0.06086906978183596\n",
      "current loss at 43651 iteration 0.06086905267842085\n",
      "current loss at 43661 iteration 0.060869035583159065\n",
      "current loss at 43671 iteration 0.06086901849604482\n",
      "current loss at 43681 iteration 0.06086900141707233\n",
      "current loss at 43691 iteration 0.060868984346235826\n",
      "current loss at 43701 iteration 0.06086896728352953\n",
      "current loss at 43711 iteration 0.06086895022894766\n",
      "current loss at 43721 iteration 0.060868933182484465\n",
      "current loss at 43731 iteration 0.06086891614413418\n",
      "current loss at 43741 iteration 0.06086889911389105\n",
      "current loss at 43751 iteration 0.06086888209174934\n",
      "current loss at 43761 iteration 0.06086886507770331\n",
      "current loss at 43771 iteration 0.060868848071747195\n",
      "current loss at 43781 iteration 0.06086883107387528\n",
      "current loss at 43791 iteration 0.060868814084081846\n",
      "current loss at 43801 iteration 0.060868797102361163\n",
      "current loss at 43811 iteration 0.06086878012870752\n",
      "current loss at 43821 iteration 0.060868763163115215\n",
      "current loss at 43831 iteration 0.06086874620557852\n",
      "current loss at 43841 iteration 0.06086872925609177\n",
      "current loss at 43851 iteration 0.06086871231464925\n",
      "current loss at 43861 iteration 0.06086869538124526\n",
      "current loss at 43871 iteration 0.06086867845587413\n",
      "current loss at 43881 iteration 0.06086866153853018\n",
      "current loss at 43891 iteration 0.06086864462920776\n",
      "current loss at 43901 iteration 0.06086862772790117\n",
      "current loss at 43911 iteration 0.06086861083460477\n",
      "current loss at 43921 iteration 0.06086859394931288\n",
      "current loss at 43931 iteration 0.06086857707201987\n",
      "current loss at 43941 iteration 0.0608685602027201\n",
      "current loss at 43951 iteration 0.06086854334140791\n",
      "current loss at 43961 iteration 0.06086852648807766\n",
      "current loss at 43971 iteration 0.06086850964272374\n",
      "current loss at 43981 iteration 0.06086849280534052\n",
      "current loss at 43991 iteration 0.060868475975922376\n",
      "current loss at 44001 iteration 0.06086845915446369\n",
      "current loss at 44011 iteration 0.06086844234095887\n",
      "current loss at 44021 iteration 0.06086842553540228\n",
      "current loss at 44031 iteration 0.060868408737788364\n",
      "current loss at 44041 iteration 0.06086839194811149\n",
      "current loss at 44051 iteration 0.060868375166366104\n",
      "current loss at 44061 iteration 0.0608683583925466\n",
      "current loss at 44071 iteration 0.060868341626647414\n",
      "current loss at 44081 iteration 0.060868324868662955\n",
      "current loss at 44091 iteration 0.06086830811858765\n",
      "current loss at 44101 iteration 0.06086829137641598\n",
      "current loss at 44111 iteration 0.06086827464214236\n",
      "current loss at 44121 iteration 0.060868257915761226\n",
      "current loss at 44131 iteration 0.06086824119726705\n",
      "current loss at 44141 iteration 0.0608682244866543\n",
      "current loss at 44151 iteration 0.06086820778391741\n",
      "current loss at 44161 iteration 0.06086819108905087\n",
      "current loss at 44171 iteration 0.060868174402049155\n",
      "current loss at 44181 iteration 0.06086815772290674\n",
      "current loss at 44191 iteration 0.06086814105161811\n",
      "current loss at 44201 iteration 0.06086812438817774\n",
      "current loss at 44211 iteration 0.06086810773258015\n",
      "current loss at 44221 iteration 0.060868091084819845\n",
      "current loss at 44231 iteration 0.0608680744448913\n",
      "current loss at 44241 iteration 0.06086805781278902\n",
      "current loss at 44251 iteration 0.060868041188507555\n",
      "current loss at 44261 iteration 0.06086802457204141\n",
      "current loss at 44271 iteration 0.060868007963385096\n",
      "current loss at 44281 iteration 0.060867991362533166\n",
      "current loss at 44291 iteration 0.060867974769480165\n",
      "current loss at 44301 iteration 0.060867958184220576\n",
      "current loss at 44311 iteration 0.06086794160674902\n",
      "current loss at 44321 iteration 0.060867925037060004\n",
      "current loss at 44331 iteration 0.06086790847514808\n",
      "current loss at 44341 iteration 0.06086789192100783\n",
      "current loss at 44351 iteration 0.06086787537463382\n",
      "current loss at 44361 iteration 0.06086785883602059\n",
      "current loss at 44371 iteration 0.06086784230516276\n",
      "current loss at 44381 iteration 0.06086782578205488\n",
      "current loss at 44391 iteration 0.06086780926669155\n",
      "current loss at 44401 iteration 0.060867792759067334\n",
      "current loss at 44411 iteration 0.06086777625917688\n",
      "current loss at 44421 iteration 0.06086775976701475\n",
      "current loss at 44431 iteration 0.060867743282575555\n",
      "current loss at 44441 iteration 0.06086772680585392\n",
      "current loss at 44451 iteration 0.06086771033684446\n",
      "current loss at 44461 iteration 0.06086769387554177\n",
      "current loss at 44471 iteration 0.06086767742194052\n",
      "current loss at 44481 iteration 0.06086766097603529\n",
      "current loss at 44491 iteration 0.06086764453782075\n",
      "current loss at 44501 iteration 0.06086762810729155\n",
      "current loss at 44511 iteration 0.06086761168444231\n",
      "current loss at 44521 iteration 0.06086759526926769\n",
      "current loss at 44531 iteration 0.06086757886176234\n",
      "current loss at 44541 iteration 0.060867562461920946\n",
      "current loss at 44551 iteration 0.06086754606973815\n",
      "current loss at 44561 iteration 0.060867529685208616\n",
      "current loss at 44571 iteration 0.06086751330832702\n",
      "current loss at 44581 iteration 0.06086749693908808\n",
      "current loss at 44591 iteration 0.06086748057748645\n",
      "current loss at 44601 iteration 0.060867464223516826\n",
      "current loss at 44611 iteration 0.060867447877173894\n",
      "current loss at 44621 iteration 0.060867431538452386\n",
      "current loss at 44631 iteration 0.06086741520734697\n",
      "current loss at 44641 iteration 0.06086739888385236\n",
      "current loss at 44651 iteration 0.06086738256796331\n",
      "current loss at 44661 iteration 0.060867366259674496\n",
      "current loss at 44671 iteration 0.06086734995898065\n",
      "current loss at 44681 iteration 0.060867333665876516\n",
      "current loss at 44691 iteration 0.06086731738035683\n",
      "current loss at 44701 iteration 0.0608673011024163\n",
      "current loss at 44711 iteration 0.060867284832049715\n",
      "current loss at 44721 iteration 0.060867268569251794\n",
      "current loss at 44731 iteration 0.060867252314017316\n",
      "current loss at 44741 iteration 0.060867236066341\n",
      "current loss at 44751 iteration 0.06086721982621764\n",
      "current loss at 44761 iteration 0.060867203593641984\n",
      "current loss at 44771 iteration 0.06086718736860883\n",
      "current loss at 44781 iteration 0.06086717115111294\n",
      "current loss at 44791 iteration 0.0608671549411491\n",
      "current loss at 44801 iteration 0.06086713873871209\n",
      "current loss at 44811 iteration 0.06086712254379672\n",
      "current loss at 44821 iteration 0.060867106356397756\n",
      "current loss at 44831 iteration 0.06086709017651003\n",
      "current loss at 44841 iteration 0.06086707400412834\n",
      "current loss at 44851 iteration 0.06086705783924749\n",
      "current loss at 44861 iteration 0.0608670416818623\n",
      "current loss at 44871 iteration 0.06086702553196761\n",
      "current loss at 44881 iteration 0.0608670093895582\n",
      "current loss at 44891 iteration 0.06086699325462894\n",
      "current loss at 44901 iteration 0.06086697712717464\n",
      "current loss at 44911 iteration 0.06086696100719017\n",
      "current loss at 44921 iteration 0.060866944894670344\n",
      "current loss at 44931 iteration 0.06086692878961004\n",
      "current loss at 44941 iteration 0.060866912692004074\n",
      "current loss at 44951 iteration 0.060866896601847346\n",
      "current loss at 44961 iteration 0.06086688051913469\n",
      "current loss at 44971 iteration 0.060866864443860975\n",
      "current loss at 44981 iteration 0.06086684837602109\n",
      "current loss at 44991 iteration 0.0608668323156099\n",
      "current loss at 45001 iteration 0.06086681626262231\n",
      "current loss at 45011 iteration 0.060866800217053166\n",
      "current loss at 45021 iteration 0.0608667841788974\n",
      "current loss at 45031 iteration 0.060866768148149876\n",
      "current loss at 45041 iteration 0.06086675212480552\n",
      "current loss at 45051 iteration 0.060866736108859225\n",
      "current loss at 45061 iteration 0.06086672010030589\n",
      "current loss at 45071 iteration 0.060866704099140446\n",
      "current loss at 45081 iteration 0.06086668810535783\n",
      "current loss at 45091 iteration 0.060866672118952914\n",
      "current loss at 45101 iteration 0.06086665613992068\n",
      "current loss at 45111 iteration 0.06086664016825602\n",
      "current loss at 45121 iteration 0.06086662420395391\n",
      "current loss at 45131 iteration 0.06086660824700926\n",
      "current loss at 45141 iteration 0.06086659229741704\n",
      "current loss at 45151 iteration 0.060866576355172196\n",
      "current loss at 45161 iteration 0.06086656042026966\n",
      "current loss at 45171 iteration 0.06086654449270443\n",
      "current loss at 45181 iteration 0.06086652857247145\n",
      "current loss at 45191 iteration 0.06086651265956569\n",
      "current loss at 45201 iteration 0.06086649675398215\n",
      "current loss at 45211 iteration 0.06086648085571576\n",
      "current loss at 45221 iteration 0.06086646496476156\n",
      "current loss at 45231 iteration 0.06086644908111448\n",
      "current loss at 45241 iteration 0.060866433204769574\n",
      "current loss at 45251 iteration 0.06086641733572178\n",
      "current loss at 45261 iteration 0.06086640147396616\n",
      "current loss at 45271 iteration 0.06086638561949769\n",
      "current loss at 45281 iteration 0.060866369772311364\n",
      "current loss at 45291 iteration 0.06086635393240222\n",
      "current loss at 45301 iteration 0.060866338099765285\n",
      "current loss at 45311 iteration 0.06086632227439556\n",
      "current loss at 45321 iteration 0.0608663064562881\n",
      "current loss at 45331 iteration 0.060866290645437915\n",
      "current loss at 45341 iteration 0.06086627484184007\n",
      "current loss at 45351 iteration 0.06086625904548956\n",
      "current loss at 45361 iteration 0.06086624325638149\n",
      "current loss at 45371 iteration 0.060866227474510884\n",
      "current loss at 45381 iteration 0.060866211699872795\n",
      "current loss at 45391 iteration 0.06086619593246228\n",
      "current loss at 45401 iteration 0.06086618017227444\n",
      "current loss at 45411 iteration 0.06086616441930429\n",
      "current loss at 45421 iteration 0.06086614867354694\n",
      "current loss at 45431 iteration 0.06086613293499748\n",
      "current loss at 45441 iteration 0.060866117203650955\n",
      "current loss at 45451 iteration 0.06086610147950248\n",
      "current loss at 45461 iteration 0.06086608576254712\n",
      "current loss at 45471 iteration 0.060866070052780025\n",
      "current loss at 45481 iteration 0.060866054350196236\n",
      "current loss at 45491 iteration 0.06086603865479087\n",
      "current loss at 45501 iteration 0.06086602296655909\n",
      "current loss at 45511 iteration 0.060866007285495946\n",
      "current loss at 45521 iteration 0.06086599161159657\n",
      "current loss at 45531 iteration 0.06086597594485613\n",
      "current loss at 45541 iteration 0.060865960285269693\n",
      "current loss at 45551 iteration 0.060865944632832424\n",
      "current loss at 45561 iteration 0.060865928987539446\n",
      "current loss at 45571 iteration 0.06086591334938591\n",
      "current loss at 45581 iteration 0.06086589771836696\n",
      "current loss at 45591 iteration 0.06086588209447774\n",
      "current loss at 45601 iteration 0.060865866477713415\n",
      "current loss at 45611 iteration 0.060865850868069137\n",
      "current loss at 45621 iteration 0.06086583526554005\n",
      "current loss at 45631 iteration 0.06086581967012134\n",
      "current loss at 45641 iteration 0.06086580408180818\n",
      "current loss at 45651 iteration 0.060865788500595736\n",
      "current loss at 45661 iteration 0.0608657729264792\n",
      "current loss at 45671 iteration 0.06086575735945375\n",
      "current loss at 45681 iteration 0.06086574179951457\n",
      "current loss at 45691 iteration 0.06086572624665684\n",
      "current loss at 45701 iteration 0.0608657107008758\n",
      "current loss at 45711 iteration 0.060865695162166604\n",
      "current loss at 45721 iteration 0.0608656796305245\n",
      "current loss at 45731 iteration 0.06086566410594466\n",
      "current loss at 45741 iteration 0.06086564858842233\n",
      "current loss at 45751 iteration 0.06086563307795271\n",
      "current loss at 45761 iteration 0.06086561757453102\n",
      "current loss at 45771 iteration 0.06086560207815249\n",
      "current loss at 45781 iteration 0.06086558658881239\n",
      "current loss at 45791 iteration 0.060865571106505895\n",
      "current loss at 45801 iteration 0.06086555563122827\n",
      "current loss at 45811 iteration 0.060865540162974775\n",
      "current loss at 45821 iteration 0.06086552470174064\n",
      "current loss at 45831 iteration 0.06086550924752115\n",
      "current loss at 45841 iteration 0.06086549380031151\n",
      "current loss at 45851 iteration 0.060865478360107014\n",
      "current loss at 45861 iteration 0.06086546292690293\n",
      "current loss at 45871 iteration 0.060865447500694524\n",
      "current loss at 45881 iteration 0.060865432081477065\n",
      "current loss at 45891 iteration 0.06086541666924583\n",
      "current loss at 45901 iteration 0.0608654012639961\n",
      "current loss at 45911 iteration 0.060865385865723175\n",
      "current loss at 45921 iteration 0.060865370474422344\n",
      "current loss at 45931 iteration 0.0608653550900889\n",
      "current loss at 45941 iteration 0.060865339712718125\n",
      "current loss at 45951 iteration 0.060865324342305356\n",
      "current loss at 45961 iteration 0.06086530897884587\n",
      "current loss at 45971 iteration 0.060865293622335\n",
      "current loss at 45981 iteration 0.06086527827276806\n",
      "current loss at 45991 iteration 0.06086526293014037\n",
      "current loss at 46001 iteration 0.06086524759444724\n",
      "current loss at 46011 iteration 0.06086523226568403\n",
      "current loss at 46021 iteration 0.06086521694384603\n",
      "current loss at 46031 iteration 0.06086520162892861\n",
      "current loss at 46041 iteration 0.060865186320927106\n",
      "current loss at 46051 iteration 0.06086517101983686\n",
      "current loss at 46061 iteration 0.06086515572565322\n",
      "current loss at 46071 iteration 0.060865140438371546\n",
      "current loss at 46081 iteration 0.06086512515798718\n",
      "current loss at 46091 iteration 0.06086510988449551\n",
      "current loss at 46101 iteration 0.0608650946178919\n",
      "current loss at 46111 iteration 0.060865079358171696\n",
      "current loss at 46121 iteration 0.0608650641053303\n",
      "current loss at 46131 iteration 0.06086504885936307\n",
      "current loss at 46141 iteration 0.060865033620265394\n",
      "current loss at 46151 iteration 0.060865018388032675\n",
      "current loss at 46161 iteration 0.06086500316266029\n",
      "current loss at 46171 iteration 0.06086498794414362\n",
      "current loss at 46181 iteration 0.06086497273247809\n",
      "current loss at 46191 iteration 0.06086495752765911\n",
      "current loss at 46201 iteration 0.06086494232968206\n",
      "current loss at 46211 iteration 0.060864927138542364\n",
      "current loss at 46221 iteration 0.06086491195423543\n",
      "current loss at 46231 iteration 0.060864896776756695\n",
      "current loss at 46241 iteration 0.06086488160610158\n",
      "current loss at 46251 iteration 0.06086486644226549\n",
      "current loss at 46261 iteration 0.06086485128524388\n",
      "current loss at 46271 iteration 0.06086483613503219\n",
      "current loss at 46281 iteration 0.06086482099162586\n",
      "current loss at 46291 iteration 0.06086480585502031\n",
      "current loss at 46301 iteration 0.060864790725211004\n",
      "current loss at 46311 iteration 0.06086477560219339\n",
      "current loss at 46321 iteration 0.06086476048596293\n",
      "current loss at 46331 iteration 0.060864745376515095\n",
      "current loss at 46341 iteration 0.06086473027384533\n",
      "current loss at 46351 iteration 0.0608647151779491\n",
      "current loss at 46361 iteration 0.060864700088821895\n",
      "current loss at 46371 iteration 0.0608646850064592\n",
      "current loss at 46381 iteration 0.06086466993085647\n",
      "current loss at 46391 iteration 0.06086465486200919\n",
      "current loss at 46401 iteration 0.06086463979991287\n",
      "current loss at 46411 iteration 0.060864624744563\n",
      "current loss at 46421 iteration 0.06086460969595505\n",
      "current loss at 46431 iteration 0.060864594654084545\n",
      "current loss at 46441 iteration 0.06086457961894699\n",
      "current loss at 46451 iteration 0.06086456459053789\n",
      "current loss at 46461 iteration 0.060864549568852754\n",
      "current loss at 46471 iteration 0.06086453455388709\n",
      "current loss at 46481 iteration 0.06086451954563642\n",
      "current loss at 46491 iteration 0.06086450454409628\n",
      "current loss at 46501 iteration 0.06086448954926222\n",
      "current loss at 46511 iteration 0.06086447456112973\n",
      "current loss at 46521 iteration 0.06086445957969436\n",
      "current loss at 46531 iteration 0.06086444460495165\n",
      "current loss at 46541 iteration 0.060864429636897165\n",
      "current loss at 46551 iteration 0.06086441467552642\n",
      "current loss at 46561 iteration 0.06086439972083499\n",
      "current loss at 46571 iteration 0.06086438477281843\n",
      "current loss at 46581 iteration 0.06086436983147228\n",
      "current loss at 46591 iteration 0.06086435489679213\n",
      "current loss at 46601 iteration 0.06086433996877353\n",
      "current loss at 46611 iteration 0.06086432504741205\n",
      "current loss at 46621 iteration 0.06086431013270329\n",
      "current loss at 46631 iteration 0.06086429522464281\n",
      "current loss at 46641 iteration 0.06086428032322619\n",
      "current loss at 46651 iteration 0.06086426542844901\n",
      "current loss at 46661 iteration 0.06086425054030691\n",
      "current loss at 46671 iteration 0.06086423565879543\n",
      "current loss at 46681 iteration 0.06086422078391018\n",
      "current loss at 46691 iteration 0.060864205915646785\n",
      "current loss at 46701 iteration 0.06086419105400084\n",
      "current loss at 46711 iteration 0.06086417619896794\n",
      "current loss at 46721 iteration 0.06086416135054373\n",
      "current loss at 46731 iteration 0.0608641465087238\n",
      "current loss at 46741 iteration 0.060864131673503796\n",
      "current loss at 46751 iteration 0.06086411684487932\n",
      "current loss at 46761 iteration 0.06086410202284601\n",
      "current loss at 46771 iteration 0.06086408720739952\n",
      "current loss at 46781 iteration 0.06086407239853545\n",
      "current loss at 46791 iteration 0.06086405759624948\n",
      "current loss at 46801 iteration 0.06086404280053724\n",
      "current loss at 46811 iteration 0.06086402801139438\n",
      "current loss at 46821 iteration 0.06086401322881653\n",
      "current loss at 46831 iteration 0.06086399845279938\n",
      "current loss at 46841 iteration 0.06086398368333858\n",
      "current loss at 46851 iteration 0.06086396892042978\n",
      "current loss at 46861 iteration 0.06086395416406867\n",
      "current loss at 46871 iteration 0.06086393941425091\n",
      "current loss at 46881 iteration 0.06086392467097216\n",
      "current loss at 46891 iteration 0.06086390993422814\n",
      "current loss at 46901 iteration 0.06086389520401451\n",
      "current loss at 46911 iteration 0.06086388048032695\n",
      "current loss at 46921 iteration 0.06086386576316115\n",
      "current loss at 46931 iteration 0.06086385105251283\n",
      "current loss at 46941 iteration 0.06086383634837765\n",
      "current loss at 46951 iteration 0.060863821650751354\n",
      "current loss at 46961 iteration 0.06086380695962961\n",
      "current loss at 46971 iteration 0.06086379227500817\n",
      "current loss at 46981 iteration 0.0608637775968827\n",
      "current loss at 46991 iteration 0.06086376292524895\n",
      "current loss at 47001 iteration 0.06086374826010263\n",
      "current loss at 47011 iteration 0.06086373360143947\n",
      "current loss at 47021 iteration 0.06086371894925519\n",
      "current loss at 47031 iteration 0.06086370430354552\n",
      "current loss at 47041 iteration 0.060863689664306206\n",
      "current loss at 47051 iteration 0.060863675031533004\n",
      "current loss at 47061 iteration 0.06086366040522162\n",
      "current loss at 47071 iteration 0.06086364578536782\n",
      "current loss at 47081 iteration 0.06086363117196736\n",
      "current loss at 47091 iteration 0.06086361656501597\n",
      "current loss at 47101 iteration 0.06086360196450945\n",
      "current loss at 47111 iteration 0.06086358737044352\n",
      "current loss at 47121 iteration 0.06086357278281397\n",
      "current loss at 47131 iteration 0.06086355820161657\n",
      "current loss at 47141 iteration 0.06086354362684708\n",
      "current loss at 47151 iteration 0.06086352905850128\n",
      "current loss at 47161 iteration 0.060863514496574976\n",
      "current loss at 47171 iteration 0.06086349994106391\n",
      "current loss at 47181 iteration 0.060863485391963894\n",
      "current loss at 47191 iteration 0.06086347084927072\n",
      "current loss at 47201 iteration 0.060863456312980166\n",
      "current loss at 47211 iteration 0.06086344178308805\n",
      "current loss at 47221 iteration 0.060863427259590167\n",
      "current loss at 47231 iteration 0.06086341274248233\n",
      "current loss at 47241 iteration 0.06086339823176033\n",
      "current loss at 47251 iteration 0.06086338372742001\n",
      "current loss at 47261 iteration 0.060863369229457155\n",
      "current loss at 47271 iteration 0.06086335473786762\n",
      "current loss at 47281 iteration 0.0608633402526472\n",
      "current loss at 47291 iteration 0.06086332577379172\n",
      "current loss at 47301 iteration 0.06086331130129704\n",
      "current loss at 47311 iteration 0.060863296835158974\n",
      "current loss at 47321 iteration 0.06086328237537337\n",
      "current loss at 47331 iteration 0.06086326792193607\n",
      "current loss at 47341 iteration 0.06086325347484291\n",
      "current loss at 47351 iteration 0.06086323903408976\n",
      "current loss at 47361 iteration 0.060863224599672444\n",
      "current loss at 47371 iteration 0.060863210171586836\n",
      "current loss at 47381 iteration 0.060863195749828805\n",
      "current loss at 47391 iteration 0.0608631813343942\n",
      "current loss at 47401 iteration 0.0608631669252789\n",
      "current loss at 47411 iteration 0.06086315252247877\n",
      "current loss at 47421 iteration 0.06086313812598968\n",
      "current loss at 47431 iteration 0.060863123735807524\n",
      "current loss at 47441 iteration 0.060863109351928166\n",
      "current loss at 47451 iteration 0.0608630949743475\n",
      "current loss at 47461 iteration 0.06086308060306141\n",
      "current loss at 47471 iteration 0.0608630662380658\n",
      "current loss at 47481 iteration 0.06086305187935656\n",
      "current loss at 47491 iteration 0.06086303752692958\n",
      "current loss at 47501 iteration 0.0608630231807808\n",
      "current loss at 47511 iteration 0.06086300884090607\n",
      "current loss at 47521 iteration 0.060862994507301336\n",
      "current loss at 47531 iteration 0.06086298017996253\n",
      "current loss at 47541 iteration 0.06086296585888551\n",
      "current loss at 47551 iteration 0.060862951544066256\n",
      "current loss at 47561 iteration 0.06086293723550065\n",
      "current loss at 47571 iteration 0.06086292293318466\n",
      "current loss at 47581 iteration 0.0608629086371142\n",
      "current loss at 47591 iteration 0.060862894347285174\n",
      "current loss at 47601 iteration 0.060862880063693586\n",
      "current loss at 47611 iteration 0.06086286578633531\n",
      "current loss at 47621 iteration 0.06086285151520633\n",
      "current loss at 47631 iteration 0.0608628372503026\n",
      "current loss at 47641 iteration 0.06086282299162006\n",
      "current loss at 47651 iteration 0.060862808739154645\n",
      "current loss at 47661 iteration 0.060862794492902356\n",
      "current loss at 47671 iteration 0.06086278025285912\n",
      "current loss at 47681 iteration 0.060862766019020936\n",
      "current loss at 47691 iteration 0.06086275179138376\n",
      "current loss at 47701 iteration 0.060862737569943565\n",
      "current loss at 47711 iteration 0.06086272335469632\n",
      "current loss at 47721 iteration 0.06086270914563802\n",
      "current loss at 47731 iteration 0.06086269494276461\n",
      "current loss at 47741 iteration 0.06086268074607214\n",
      "current loss at 47751 iteration 0.060862666555556576\n",
      "current loss at 47761 iteration 0.06086265237121388\n",
      "current loss at 47771 iteration 0.06086263819304009\n",
      "current loss at 47781 iteration 0.06086262402103119\n",
      "current loss at 47791 iteration 0.060862609855183186\n",
      "current loss at 47801 iteration 0.06086259569549209\n",
      "current loss at 47811 iteration 0.060862581541953925\n",
      "current loss at 47821 iteration 0.06086256739456467\n",
      "current loss at 47831 iteration 0.06086255325332038\n",
      "current loss at 47841 iteration 0.060862539118217054\n",
      "current loss at 47851 iteration 0.060862524989250724\n",
      "current loss at 47861 iteration 0.06086251086641744\n",
      "current loss at 47871 iteration 0.060862496749713196\n",
      "current loss at 47881 iteration 0.06086248263913405\n",
      "current loss at 47891 iteration 0.06086246853467604\n",
      "current loss at 47901 iteration 0.0608624544363352\n",
      "current loss at 47911 iteration 0.06086244034410759\n",
      "current loss at 47921 iteration 0.06086242625798924\n",
      "current loss at 47931 iteration 0.06086241217797623\n",
      "current loss at 47941 iteration 0.06086239810406459\n",
      "current loss at 47951 iteration 0.060862384036250376\n",
      "current loss at 47961 iteration 0.060862369974529665\n",
      "current loss at 47971 iteration 0.06086235591889852\n",
      "current loss at 47981 iteration 0.060862341869353034\n",
      "current loss at 47991 iteration 0.06086232782588923\n",
      "current loss at 48001 iteration 0.06086231378850323\n",
      "current loss at 48011 iteration 0.06086229975719109\n",
      "current loss at 48021 iteration 0.060862285731948886\n",
      "current loss at 48031 iteration 0.060862271712772725\n",
      "current loss at 48041 iteration 0.06086225769965868\n",
      "current loss at 48051 iteration 0.06086224369260286\n",
      "current loss at 48061 iteration 0.06086222969160134\n",
      "current loss at 48071 iteration 0.06086221569665024\n",
      "current loss at 48081 iteration 0.06086220170774565\n",
      "current loss at 48091 iteration 0.060862187724883686\n",
      "current loss at 48101 iteration 0.06086217374806044\n",
      "current loss at 48111 iteration 0.06086215977727205\n",
      "current loss at 48121 iteration 0.060862145812514624\n",
      "current loss at 48131 iteration 0.06086213185378427\n",
      "current loss at 48141 iteration 0.06086211790107712\n",
      "current loss at 48151 iteration 0.06086210395438929\n",
      "current loss at 48161 iteration 0.060862090013716924\n",
      "current loss at 48171 iteration 0.06086207607905615\n",
      "current loss at 48181 iteration 0.0608620621504031\n",
      "current loss at 48191 iteration 0.0608620482277539\n",
      "current loss at 48201 iteration 0.060862034311104735\n",
      "current loss at 48211 iteration 0.06086202040045171\n",
      "current loss at 48221 iteration 0.06086200649579099\n",
      "current loss at 48231 iteration 0.060861992597118726\n",
      "current loss at 48241 iteration 0.06086197870443107\n",
      "current loss at 48251 iteration 0.060861964817724185\n",
      "current loss at 48261 iteration 0.06086195093699424\n",
      "current loss at 48271 iteration 0.06086193706223739\n",
      "current loss at 48281 iteration 0.060861923193449824\n",
      "current loss at 48291 iteration 0.06086190933062768\n",
      "current loss at 48301 iteration 0.06086189547376716\n",
      "current loss at 48311 iteration 0.06086188162286442\n",
      "current loss at 48321 iteration 0.06086186777791566\n",
      "current loss at 48331 iteration 0.060861853938917077\n",
      "current loss at 48341 iteration 0.06086184010586483\n",
      "current loss at 48351 iteration 0.06086182627875514\n",
      "current loss at 48361 iteration 0.060861812457584166\n",
      "current loss at 48371 iteration 0.060861798642348135\n",
      "current loss at 48381 iteration 0.06086178483304324\n",
      "current loss at 48391 iteration 0.06086177102966569\n",
      "current loss at 48401 iteration 0.06086175723221168\n",
      "current loss at 48411 iteration 0.060861743440677435\n",
      "current loss at 48421 iteration 0.060861729655059156\n",
      "current loss at 48431 iteration 0.06086171587535308\n",
      "current loss at 48441 iteration 0.06086170210155541\n",
      "current loss at 48451 iteration 0.06086168833366236\n",
      "current loss at 48461 iteration 0.060861674571670195\n",
      "current loss at 48471 iteration 0.06086166081557512\n",
      "current loss at 48481 iteration 0.060861647065373364\n",
      "current loss at 48491 iteration 0.060861633321061176\n",
      "current loss at 48501 iteration 0.06086161958263479\n",
      "current loss at 48511 iteration 0.06086160585009045\n",
      "current loss at 48521 iteration 0.060861592123424395\n",
      "current loss at 48531 iteration 0.060861578402632906\n",
      "current loss at 48541 iteration 0.06086156468771217\n",
      "current loss at 48551 iteration 0.060861550978658516\n",
      "current loss at 48561 iteration 0.06086153727546817\n",
      "current loss at 48571 iteration 0.06086152357813739\n",
      "current loss at 48581 iteration 0.06086150988666245\n",
      "current loss at 48591 iteration 0.06086149620103962\n",
      "current loss at 48601 iteration 0.06086148252126516\n",
      "current loss at 48611 iteration 0.06086146884733536\n",
      "current loss at 48621 iteration 0.060861455179246474\n",
      "current loss at 48631 iteration 0.060861441516994796\n",
      "current loss at 48641 iteration 0.06086142786057661\n",
      "current loss at 48651 iteration 0.060861414209988224\n",
      "current loss at 48661 iteration 0.060861400565225904\n",
      "current loss at 48671 iteration 0.06086138692628597\n",
      "current loss at 48681 iteration 0.06086137329316468\n",
      "current loss at 48691 iteration 0.060861359665858344\n",
      "current loss at 48701 iteration 0.060861346044363294\n",
      "current loss at 48711 iteration 0.0608613324286758\n",
      "current loss at 48721 iteration 0.060861318818792184\n",
      "current loss at 48731 iteration 0.06086130521470877\n",
      "current loss at 48741 iteration 0.06086129161642187\n",
      "current loss at 48751 iteration 0.060861278023927795\n",
      "current loss at 48761 iteration 0.060861264437222874\n",
      "current loss at 48771 iteration 0.06086125085630342\n",
      "current loss at 48781 iteration 0.06086123728116577\n",
      "current loss at 48791 iteration 0.06086122371180626\n",
      "current loss at 48801 iteration 0.0608612101482212\n",
      "current loss at 48811 iteration 0.06086119659040696\n",
      "current loss at 48821 iteration 0.060861183038359856\n",
      "current loss at 48831 iteration 0.06086116949207624\n",
      "current loss at 48841 iteration 0.06086115595155247\n",
      "current loss at 48851 iteration 0.060861142416784864\n",
      "current loss at 48861 iteration 0.06086112888776981\n",
      "current loss at 48871 iteration 0.06086111536450364\n",
      "current loss at 48881 iteration 0.06086110184698273\n",
      "current loss at 48891 iteration 0.06086108833520343\n",
      "current loss at 48901 iteration 0.06086107482916211\n",
      "current loss at 48911 iteration 0.060861061328855125\n",
      "current loss at 48921 iteration 0.06086104783427885\n",
      "current loss at 48931 iteration 0.06086103434542968\n",
      "current loss at 48941 iteration 0.06086102086230397\n",
      "current loss at 48951 iteration 0.060861007384898115\n",
      "current loss at 48961 iteration 0.06086099391320849\n",
      "current loss at 48971 iteration 0.060860980447231476\n",
      "current loss at 48981 iteration 0.060860966986963465\n",
      "current loss at 48991 iteration 0.060860953532400855\n",
      "current loss at 49001 iteration 0.06086094008354004\n",
      "current loss at 49011 iteration 0.06086092664037741\n",
      "current loss at 49021 iteration 0.06086091320290938\n",
      "current loss at 49031 iteration 0.06086089977113233\n",
      "current loss at 49041 iteration 0.060860886345042706\n",
      "current loss at 49051 iteration 0.060860872924636884\n",
      "current loss at 49061 iteration 0.060860859509911304\n",
      "current loss at 49071 iteration 0.06086084610086235\n",
      "current loss at 49081 iteration 0.06086083269748647\n",
      "current loss at 49091 iteration 0.06086081929978008\n",
      "current loss at 49101 iteration 0.06086080590773958\n",
      "current loss at 49111 iteration 0.06086079252136144\n",
      "current loss at 49121 iteration 0.060860779140642064\n",
      "current loss at 49131 iteration 0.0608607657655779\n",
      "current loss at 49141 iteration 0.06086075239616537\n",
      "current loss at 49151 iteration 0.06086073903240092\n",
      "current loss at 49161 iteration 0.06086072567428099\n",
      "current loss at 49171 iteration 0.06086071232180206\n",
      "current loss at 49181 iteration 0.06086069897496053\n",
      "current loss at 49191 iteration 0.06086068563375288\n",
      "current loss at 49201 iteration 0.06086067229817555\n",
      "current loss at 49211 iteration 0.06086065896822501\n",
      "current loss at 49221 iteration 0.06086064564389772\n",
      "current loss at 49231 iteration 0.06086063232519014\n",
      "current loss at 49241 iteration 0.06086061901209874\n",
      "current loss at 49251 iteration 0.06086060570461999\n",
      "current loss at 49261 iteration 0.06086059240275036\n",
      "current loss at 49271 iteration 0.06086057910648632\n",
      "current loss at 49281 iteration 0.060860565815824365\n",
      "current loss at 49291 iteration 0.06086055253076098\n",
      "current loss at 49301 iteration 0.06086053925129262\n",
      "current loss at 49311 iteration 0.06086052597741579\n",
      "current loss at 49321 iteration 0.060860512709126985\n",
      "current loss at 49331 iteration 0.0608604994464227\n",
      "current loss at 49341 iteration 0.060860486189299415\n",
      "current loss at 49351 iteration 0.060860472937753624\n",
      "current loss at 49361 iteration 0.060860459691781874\n",
      "current loss at 49371 iteration 0.06086044645138063\n",
      "current loss at 49381 iteration 0.060860433216546404\n",
      "current loss at 49391 iteration 0.06086041998727571\n",
      "current loss at 49401 iteration 0.06086040676356508\n",
      "current loss at 49411 iteration 0.060860393545411\n",
      "current loss at 49421 iteration 0.06086038033281002\n",
      "current loss at 49431 iteration 0.060860367125758645\n",
      "current loss at 49441 iteration 0.06086035392425339\n",
      "current loss at 49451 iteration 0.060860340728290815\n",
      "current loss at 49461 iteration 0.06086032753786742\n",
      "current loss at 49471 iteration 0.06086031435297975\n",
      "current loss at 49481 iteration 0.060860301173624336\n",
      "current loss at 49491 iteration 0.06086028799979774\n",
      "current loss at 49501 iteration 0.06086027483149649\n",
      "current loss at 49511 iteration 0.060860261668717115\n",
      "current loss at 49521 iteration 0.0608602485114562\n",
      "current loss at 49531 iteration 0.06086023535971027\n",
      "current loss at 49541 iteration 0.06086022221347588\n",
      "current loss at 49551 iteration 0.060860209072749596\n",
      "current loss at 49561 iteration 0.06086019593752797\n",
      "current loss at 49571 iteration 0.06086018280780757\n",
      "current loss at 49581 iteration 0.06086016968358496\n",
      "current loss at 49591 iteration 0.06086015656485671\n",
      "current loss at 49601 iteration 0.0608601434516194\n",
      "current loss at 49611 iteration 0.06086013034386958\n",
      "current loss at 49621 iteration 0.06086011724160383\n",
      "current loss at 49631 iteration 0.06086010414481876\n",
      "current loss at 49641 iteration 0.06086009105351093\n",
      "current loss at 49651 iteration 0.06086007796767693\n",
      "current loss at 49661 iteration 0.06086006488731334\n",
      "current loss at 49671 iteration 0.06086005181241675\n",
      "current loss at 49681 iteration 0.060860038742983784\n",
      "current loss at 49691 iteration 0.060860025679011005\n",
      "current loss at 49701 iteration 0.06086001262049502\n",
      "current loss at 49711 iteration 0.06085999956743244\n",
      "current loss at 49721 iteration 0.060859986519819866\n",
      "current loss at 49731 iteration 0.06085997347765389\n",
      "current loss at 49741 iteration 0.06085996044093115\n",
      "current loss at 49751 iteration 0.06085994740964826\n",
      "current loss at 49761 iteration 0.06085993438380181\n",
      "current loss at 49771 iteration 0.06085992136338842\n",
      "current loss at 49781 iteration 0.06085990834840476\n",
      "current loss at 49791 iteration 0.06085989533884738\n",
      "current loss at 49801 iteration 0.060859882334712984\n",
      "current loss at 49811 iteration 0.060859869335998136\n",
      "current loss at 49821 iteration 0.06085985634269951\n",
      "current loss at 49831 iteration 0.06085984335481373\n",
      "current loss at 49841 iteration 0.06085983037233744\n",
      "current loss at 49851 iteration 0.060859817395267274\n",
      "current loss at 49861 iteration 0.06085980442359987\n",
      "current loss at 49871 iteration 0.060859791457331905\n",
      "current loss at 49881 iteration 0.06085977849645999\n",
      "current loss at 49891 iteration 0.06085976554098079\n",
      "current loss at 49901 iteration 0.060859752590890964\n",
      "current loss at 49911 iteration 0.06085973964618718\n",
      "current loss at 49921 iteration 0.06085972670686609\n",
      "current loss at 49931 iteration 0.06085971377292435\n",
      "current loss at 49941 iteration 0.06085970084435864\n",
      "current loss at 49951 iteration 0.06085968792116562\n",
      "current loss at 49961 iteration 0.06085967500334197\n",
      "current loss at 49971 iteration 0.06085966209088435\n",
      "current loss at 49981 iteration 0.06085964918378945\n",
      "current loss at 49991 iteration 0.060859636282053944\n",
      "current loss at 50001 iteration 0.060859623385674494\n",
      "current loss at 50011 iteration 0.060859610494647845\n",
      "current loss at 50021 iteration 0.060859597608970625\n",
      "current loss at 50031 iteration 0.06085958472863954\n",
      "current loss at 50041 iteration 0.06085957185365132\n",
      "current loss at 50051 iteration 0.06085955898400261\n",
      "current loss at 50061 iteration 0.06085954611969013\n",
      "current loss at 50071 iteration 0.0608595332607106\n",
      "current loss at 50081 iteration 0.06085952040706069\n",
      "current loss at 50091 iteration 0.06085950755873713\n",
      "current loss at 50101 iteration 0.06085949471573663\n",
      "current loss at 50111 iteration 0.060859481878055885\n",
      "current loss at 50121 iteration 0.06085946904569163\n",
      "current loss at 50131 iteration 0.06085945621864056\n",
      "current loss at 50141 iteration 0.06085944339689943\n",
      "current loss at 50151 iteration 0.06085943058046492\n",
      "current loss at 50161 iteration 0.060859417769333804\n",
      "current loss at 50171 iteration 0.06085940496350276\n",
      "current loss at 50181 iteration 0.06085939216296857\n",
      "current loss at 50191 iteration 0.06085937936772793\n",
      "current loss at 50201 iteration 0.06085936657777759\n",
      "current loss at 50211 iteration 0.060859353793114294\n",
      "current loss at 50221 iteration 0.060859341013734775\n",
      "current loss at 50231 iteration 0.060859328239635774\n",
      "current loss at 50241 iteration 0.06085931547081405\n",
      "current loss at 50251 iteration 0.06085930270726636\n",
      "current loss at 50261 iteration 0.060859289948989446\n",
      "current loss at 50271 iteration 0.060859277195980055\n",
      "current loss at 50281 iteration 0.060859264448234945\n",
      "current loss at 50291 iteration 0.06085925170575089\n",
      "current loss at 50301 iteration 0.060859238968524645\n",
      "current loss at 50311 iteration 0.060859226236552995\n",
      "current loss at 50321 iteration 0.060859213509832664\n",
      "current loss at 50331 iteration 0.060859200788360476\n",
      "current loss at 50341 iteration 0.06085918807213316\n",
      "current loss at 50351 iteration 0.06085917536114752\n",
      "current loss at 50361 iteration 0.06085916265540034\n",
      "current loss at 50371 iteration 0.06085914995488838\n",
      "current loss at 50381 iteration 0.060859137259608426\n",
      "current loss at 50391 iteration 0.06085912456955727\n",
      "current loss at 50401 iteration 0.060859111884731726\n",
      "current loss at 50411 iteration 0.06085909920512854\n",
      "current loss at 50421 iteration 0.06085908653074454\n",
      "current loss at 50431 iteration 0.06085907386157653\n",
      "current loss at 50441 iteration 0.06085906119762127\n",
      "current loss at 50451 iteration 0.060859048538875596\n",
      "current loss at 50461 iteration 0.06085903588533632\n",
      "current loss at 50471 iteration 0.06085902323700021\n",
      "current loss at 50481 iteration 0.0608590105938641\n",
      "current loss at 50491 iteration 0.060858997955924816\n",
      "current loss at 50501 iteration 0.060858985323179154\n",
      "current loss at 50511 iteration 0.06085897269562395\n",
      "current loss at 50521 iteration 0.060858960073256\n",
      "current loss at 50531 iteration 0.06085894745607215\n",
      "current loss at 50541 iteration 0.06085893484406921\n",
      "current loss at 50551 iteration 0.06085892223724402\n",
      "current loss at 50561 iteration 0.06085890963559341\n",
      "current loss at 50571 iteration 0.06085889703911421\n",
      "current loss at 50581 iteration 0.06085888444780325\n",
      "current loss at 50591 iteration 0.06085887186165738\n",
      "current loss at 50601 iteration 0.06085885928067344\n",
      "current loss at 50611 iteration 0.06085884670484828\n",
      "current loss at 50621 iteration 0.06085883413417871\n",
      "current loss at 50631 iteration 0.06085882156866164\n",
      "current loss at 50641 iteration 0.06085880900829385\n",
      "current loss at 50651 iteration 0.060858796453072274\n",
      "current loss at 50661 iteration 0.06085878390299371\n",
      "current loss at 50671 iteration 0.060858771358055036\n",
      "current loss at 50681 iteration 0.0608587588182531\n",
      "current loss at 50691 iteration 0.0608587462835848\n",
      "current loss at 50701 iteration 0.06085873375404696\n",
      "current loss at 50711 iteration 0.06085872122963649\n",
      "current loss at 50721 iteration 0.06085870871035023\n",
      "current loss at 50731 iteration 0.06085869619618508\n",
      "current loss at 50741 iteration 0.060858683687137906\n",
      "current loss at 50751 iteration 0.06085867118320557\n",
      "current loss at 50761 iteration 0.060858658684384975\n",
      "current loss at 50771 iteration 0.060858646190673014\n",
      "current loss at 50781 iteration 0.06085863370206657\n",
      "current loss at 50791 iteration 0.06085862121856251\n",
      "current loss at 50801 iteration 0.06085860874015773\n",
      "current loss at 50811 iteration 0.06085859626684915\n",
      "current loss at 50821 iteration 0.06085858379863366\n",
      "current loss at 50831 iteration 0.06085857133550814\n",
      "current loss at 50841 iteration 0.06085855887746951\n",
      "current loss at 50851 iteration 0.060858546424514666\n",
      "current loss at 50861 iteration 0.06085853397664051\n",
      "current loss at 50871 iteration 0.06085852153384397\n",
      "current loss at 50881 iteration 0.06085850909612197\n",
      "current loss at 50891 iteration 0.06085849666347138\n",
      "current loss at 50901 iteration 0.06085848423588913\n",
      "current loss at 50911 iteration 0.060858471813372166\n",
      "current loss at 50921 iteration 0.0608584593959174\n",
      "current loss at 50931 iteration 0.06085844698352173\n",
      "current loss at 50941 iteration 0.06085843457618213\n",
      "current loss at 50951 iteration 0.06085842217389549\n",
      "current loss at 50961 iteration 0.06085840977665876\n",
      "current loss at 50971 iteration 0.06085839738446887\n",
      "current loss at 50981 iteration 0.06085838499732277\n",
      "current loss at 50991 iteration 0.060858372615217364\n",
      "current loss at 51001 iteration 0.06085836023814964\n",
      "current loss at 51011 iteration 0.06085834786611651\n",
      "current loss at 51021 iteration 0.060858335499114936\n",
      "current loss at 51031 iteration 0.06085832313714185\n",
      "current loss at 51041 iteration 0.06085831078019424\n",
      "current loss at 51051 iteration 0.060858298428269016\n",
      "current loss at 51061 iteration 0.06085828608136316\n",
      "current loss at 51071 iteration 0.060858273739473626\n",
      "current loss at 51081 iteration 0.060858261402597376\n",
      "current loss at 51091 iteration 0.06085824907073139\n",
      "current loss at 51101 iteration 0.06085823674387261\n",
      "current loss at 51111 iteration 0.06085822442201801\n",
      "current loss at 51121 iteration 0.06085821210516456\n",
      "current loss at 51131 iteration 0.06085819979330926\n",
      "current loss at 51141 iteration 0.06085818748644905\n",
      "current loss at 51151 iteration 0.060858175184580934\n",
      "current loss at 51161 iteration 0.060858162887701864\n",
      "current loss at 51171 iteration 0.06085815059580885\n",
      "current loss at 51181 iteration 0.060858138308898885\n",
      "current loss at 51191 iteration 0.060858126026968926\n",
      "current loss at 51201 iteration 0.06085811375001599\n",
      "current loss at 51211 iteration 0.060858101478037066\n",
      "current loss at 51221 iteration 0.06085808921102915\n",
      "current loss at 51231 iteration 0.060858076948989225\n",
      "current loss at 51241 iteration 0.060858064691914286\n",
      "current loss at 51251 iteration 0.060858052439801386\n",
      "current loss at 51261 iteration 0.060858040192647476\n",
      "current loss at 51271 iteration 0.06085802795044959\n",
      "current loss at 51281 iteration 0.06085801571320472\n",
      "current loss at 51291 iteration 0.060858003480909896\n",
      "current loss at 51301 iteration 0.06085799125356214\n",
      "current loss at 51311 iteration 0.06085797903115845\n",
      "current loss at 51321 iteration 0.060857966813695846\n",
      "current loss at 51331 iteration 0.06085795460117135\n",
      "current loss at 51341 iteration 0.06085794239358201\n",
      "current loss at 51351 iteration 0.06085793019092483\n",
      "current loss at 51361 iteration 0.06085791799319684\n",
      "current loss at 51371 iteration 0.060857905800395073\n",
      "current loss at 51381 iteration 0.060857893612516575\n",
      "current loss at 51391 iteration 0.060857881429558364\n",
      "current loss at 51401 iteration 0.060857869251517496\n",
      "current loss at 51411 iteration 0.060857857078391\n",
      "current loss at 51421 iteration 0.06085784491017592\n",
      "current loss at 51431 iteration 0.06085783274686932\n",
      "current loss at 51441 iteration 0.06085782058846821\n",
      "current loss at 51451 iteration 0.06085780843496967\n",
      "current loss at 51461 iteration 0.06085779628637075\n",
      "current loss at 51471 iteration 0.0608577841426685\n",
      "current loss at 51481 iteration 0.06085777200386\n",
      "current loss at 51491 iteration 0.06085775986994227\n",
      "current loss at 51501 iteration 0.0608577477409124\n",
      "current loss at 51511 iteration 0.060857735616767425\n",
      "current loss at 51521 iteration 0.06085772349750445\n",
      "current loss at 51531 iteration 0.06085771138312052\n",
      "current loss at 51541 iteration 0.06085769927361273\n",
      "current loss at 51551 iteration 0.06085768716897812\n",
      "current loss at 51561 iteration 0.06085767506921379\n",
      "current loss at 51571 iteration 0.0608576629743168\n",
      "current loss at 51581 iteration 0.06085765088428426\n",
      "current loss at 51591 iteration 0.060857638799113246\n",
      "current loss at 51601 iteration 0.06085762671880082\n",
      "current loss at 51611 iteration 0.06085761464334408\n",
      "current loss at 51621 iteration 0.06085760257274012\n",
      "current loss at 51631 iteration 0.06085759050698605\n",
      "current loss at 51641 iteration 0.060857578446078926\n",
      "current loss at 51651 iteration 0.060857566390015876\n",
      "current loss at 51661 iteration 0.06085755433879398\n",
      "current loss at 51671 iteration 0.060857542292410374\n",
      "current loss at 51681 iteration 0.06085753025086213\n",
      "current loss at 51691 iteration 0.06085751821414636\n",
      "current loss at 51701 iteration 0.06085750618226016\n",
      "current loss at 51711 iteration 0.06085749415520067\n",
      "current loss at 51721 iteration 0.060857482132964984\n",
      "current loss at 51731 iteration 0.06085747011555024\n",
      "current loss at 51741 iteration 0.060857458102953524\n",
      "current loss at 51751 iteration 0.06085744609517198\n",
      "current loss at 51761 iteration 0.060857434092202706\n",
      "current loss at 51771 iteration 0.060857422094042844\n",
      "current loss at 51781 iteration 0.060857410100689534\n",
      "current loss at 51791 iteration 0.060857398112139874\n",
      "current loss at 51801 iteration 0.060857386128391026\n",
      "current loss at 51811 iteration 0.0608573741494401\n",
      "current loss at 51821 iteration 0.06085736217528423\n",
      "current loss at 51831 iteration 0.060857350205920584\n",
      "current loss at 51841 iteration 0.06085733824134629\n",
      "current loss at 51851 iteration 0.060857326281558474\n",
      "current loss at 51861 iteration 0.06085731432655429\n",
      "current loss at 51871 iteration 0.060857302376330885\n",
      "current loss at 51881 iteration 0.060857290430885426\n",
      "current loss at 51891 iteration 0.06085727849021505\n",
      "current loss at 51901 iteration 0.060857266554316905\n",
      "current loss at 51911 iteration 0.06085725462318817\n",
      "current loss at 51921 iteration 0.06085724269682597\n",
      "current loss at 51931 iteration 0.060857230775227485\n",
      "current loss at 51941 iteration 0.06085721885838987\n",
      "current loss at 51951 iteration 0.06085720694631031\n",
      "current loss at 51961 iteration 0.06085719503898596\n",
      "current loss at 51971 iteration 0.060857183136413986\n",
      "current loss at 51981 iteration 0.060857171238591565\n",
      "current loss at 51991 iteration 0.06085715934551587\n",
      "current loss at 52001 iteration 0.06085714745718408\n",
      "current loss at 52011 iteration 0.06085713557359337\n",
      "current loss at 52021 iteration 0.06085712369474093\n",
      "current loss at 52031 iteration 0.060857111820623935\n",
      "current loss at 52041 iteration 0.06085709995123957\n",
      "current loss at 52051 iteration 0.060857088086585014\n",
      "current loss at 52061 iteration 0.06085707622665748\n",
      "current loss at 52071 iteration 0.06085706437145414\n",
      "current loss at 52081 iteration 0.06085705252097221\n",
      "current loss at 52091 iteration 0.06085704067520887\n",
      "current loss at 52101 iteration 0.0608570288341613\n",
      "current loss at 52111 iteration 0.06085701699782675\n",
      "current loss at 52121 iteration 0.06085700516620238\n",
      "current loss at 52131 iteration 0.06085699333928542\n",
      "current loss at 52141 iteration 0.060856981517073055\n",
      "current loss at 52151 iteration 0.06085696969956252\n",
      "current loss at 52161 iteration 0.060856957886751016\n",
      "current loss at 52171 iteration 0.06085694607863576\n",
      "current loss at 52181 iteration 0.06085693427521396\n",
      "current loss at 52191 iteration 0.06085692247648284\n",
      "current loss at 52201 iteration 0.06085691068243962\n",
      "current loss at 52211 iteration 0.06085689889308152\n",
      "current loss at 52221 iteration 0.06085688710840578\n",
      "current loss at 52231 iteration 0.06085687532840962\n",
      "current loss at 52241 iteration 0.06085686355309025\n",
      "current loss at 52251 iteration 0.06085685178244492\n",
      "current loss at 52261 iteration 0.060856840016470876\n",
      "current loss at 52271 iteration 0.06085682825516533\n",
      "current loss at 52281 iteration 0.06085681649852553\n",
      "current loss at 52291 iteration 0.06085680474654872\n",
      "current loss at 52301 iteration 0.06085679299923213\n",
      "current loss at 52311 iteration 0.06085678125657301\n",
      "current loss at 52321 iteration 0.06085676951856863\n",
      "current loss at 52331 iteration 0.060856757785216216\n",
      "current loss at 52341 iteration 0.060856746056513006\n",
      "current loss at 52351 iteration 0.06085673433245628\n",
      "current loss at 52361 iteration 0.060856722613043285\n",
      "current loss at 52371 iteration 0.06085671089827127\n",
      "current loss at 52381 iteration 0.060856699188137506\n",
      "current loss at 52391 iteration 0.060856687482639255\n",
      "current loss at 52401 iteration 0.060856675781773784\n",
      "current loss at 52411 iteration 0.060856664085538345\n",
      "current loss at 52421 iteration 0.06085665239393022\n",
      "current loss at 52431 iteration 0.060856640706946664\n",
      "current loss at 52441 iteration 0.06085662902458496\n",
      "current loss at 52451 iteration 0.0608566173468424\n",
      "current loss at 52461 iteration 0.060856605673716216\n",
      "current loss at 52471 iteration 0.060856594005203726\n",
      "current loss at 52481 iteration 0.0608565823413022\n",
      "current loss at 52491 iteration 0.06085657068200892\n",
      "current loss at 52501 iteration 0.06085655902732117\n",
      "current loss at 52511 iteration 0.06085654737723625\n",
      "current loss at 52521 iteration 0.06085653573175144\n",
      "current loss at 52531 iteration 0.06085652409086401\n",
      "current loss at 52541 iteration 0.06085651245457129\n",
      "current loss at 52551 iteration 0.06085650082287058\n",
      "current loss at 52561 iteration 0.06085648919575913\n",
      "current loss at 52571 iteration 0.060856477573234286\n",
      "current loss at 52581 iteration 0.060856465955293336\n",
      "current loss at 52591 iteration 0.060856454341933576\n",
      "current loss at 52601 iteration 0.06085644273315232\n",
      "current loss at 52611 iteration 0.06085643112894688\n",
      "current loss at 52621 iteration 0.06085641952931456\n",
      "current loss at 52631 iteration 0.06085640793425268\n",
      "current loss at 52641 iteration 0.060856396343758536\n",
      "current loss at 52651 iteration 0.06085638475782947\n",
      "current loss at 52661 iteration 0.060856373176462815\n",
      "current loss at 52671 iteration 0.06085636159965583\n",
      "current loss at 52681 iteration 0.060856350027405894\n",
      "current loss at 52691 iteration 0.060856338459710314\n",
      "current loss at 52701 iteration 0.06085632689656643\n",
      "current loss at 52711 iteration 0.06085631533797156\n",
      "current loss at 52721 iteration 0.060856303783923024\n",
      "current loss at 52731 iteration 0.06085629223441818\n",
      "current loss at 52741 iteration 0.06085628068945435\n",
      "current loss at 52751 iteration 0.06085626914902886\n",
      "current loss at 52761 iteration 0.06085625761313908\n",
      "current loss at 52771 iteration 0.060856246081782334\n",
      "current loss at 52781 iteration 0.06085623455495597\n",
      "current loss at 52791 iteration 0.06085622303265733\n",
      "current loss at 52801 iteration 0.06085621151488377\n",
      "current loss at 52811 iteration 0.060856200001632624\n",
      "current loss at 52821 iteration 0.060856188492901274\n",
      "current loss at 52831 iteration 0.06085617698868703\n",
      "current loss at 52841 iteration 0.0608561654889873\n",
      "current loss at 52851 iteration 0.0608561539937994\n",
      "current loss at 52861 iteration 0.060856142503120735\n",
      "current loss at 52871 iteration 0.06085613101694861\n",
      "current loss at 52881 iteration 0.06085611953528044\n",
      "current loss at 52891 iteration 0.06085610805811356\n",
      "current loss at 52901 iteration 0.060856096585445364\n",
      "current loss at 52911 iteration 0.0608560851172732\n",
      "current loss at 52921 iteration 0.06085607365359445\n",
      "current loss at 52931 iteration 0.0608560621944065\n",
      "current loss at 52941 iteration 0.060856050739706714\n",
      "current loss at 52951 iteration 0.06085603928949247\n",
      "current loss at 52961 iteration 0.06085602784376116\n",
      "current loss at 52971 iteration 0.06085601640251016\n",
      "current loss at 52981 iteration 0.06085600496573686\n",
      "current loss at 52991 iteration 0.060855993533438624\n",
      "current loss at 53001 iteration 0.060855982105612876\n",
      "current loss at 53011 iteration 0.06085597068225698\n",
      "current loss at 53021 iteration 0.06085595926336835\n",
      "current loss at 53031 iteration 0.06085594784894436\n",
      "current loss at 53041 iteration 0.060855936438982426\n",
      "current loss at 53051 iteration 0.060855925033479946\n",
      "current loss at 53061 iteration 0.06085591363243432\n",
      "current loss at 53071 iteration 0.06085590223584293\n",
      "current loss at 53081 iteration 0.06085589084370321\n",
      "current loss at 53091 iteration 0.060855879456012535\n",
      "current loss at 53101 iteration 0.06085586807276836\n",
      "current loss at 53111 iteration 0.060855856693968056\n",
      "current loss at 53121 iteration 0.06085584531960907\n",
      "current loss at 53131 iteration 0.06085583394968878\n",
      "current loss at 53141 iteration 0.06085582258420462\n",
      "current loss at 53151 iteration 0.06085581122315401\n",
      "current loss at 53161 iteration 0.06085579986653438\n",
      "current loss at 53171 iteration 0.060855788514343145\n",
      "current loss at 53181 iteration 0.06085577716657774\n",
      "current loss at 53191 iteration 0.06085576582323558\n",
      "current loss at 53201 iteration 0.0608557544843141\n",
      "current loss at 53211 iteration 0.060855743149810716\n",
      "current loss at 53221 iteration 0.060855731819722876\n",
      "current loss at 53231 iteration 0.06085572049404802\n",
      "current loss at 53241 iteration 0.06085570917278357\n",
      "current loss at 53251 iteration 0.06085569785592699\n",
      "current loss at 53261 iteration 0.060855686543475694\n",
      "current loss at 53271 iteration 0.06085567523542712\n",
      "current loss at 53281 iteration 0.06085566393177877\n",
      "current loss at 53291 iteration 0.06085565263252801\n",
      "current loss at 53301 iteration 0.06085564133767236\n",
      "current loss at 53311 iteration 0.060855630047209223\n",
      "current loss at 53321 iteration 0.06085561876113607\n",
      "current loss at 53331 iteration 0.06085560747945037\n",
      "current loss at 53341 iteration 0.060855596202149545\n",
      "current loss at 53351 iteration 0.06085558492923108\n",
      "current loss at 53361 iteration 0.060855573660692434\n",
      "current loss at 53371 iteration 0.06085556239653105\n",
      "current loss at 53381 iteration 0.06085555113674443\n",
      "current loss at 53391 iteration 0.060855539881330016\n",
      "current loss at 53401 iteration 0.06085552863028527\n",
      "current loss at 53411 iteration 0.06085551738360768\n",
      "current loss at 53421 iteration 0.06085550614129471\n",
      "current loss at 53431 iteration 0.06085549490334382\n",
      "current loss at 53441 iteration 0.06085548366975251\n",
      "current loss at 53451 iteration 0.060855472440518255\n",
      "current loss at 53461 iteration 0.060855461215638514\n",
      "current loss at 53471 iteration 0.060855449995110805\n",
      "current loss at 53481 iteration 0.06085543877893259\n",
      "current loss at 53491 iteration 0.06085542756710135\n",
      "current loss at 53501 iteration 0.060855416359614584\n",
      "current loss at 53511 iteration 0.060855405156469776\n",
      "current loss at 53521 iteration 0.06085539395766441\n",
      "current loss at 53531 iteration 0.06085538276319599\n",
      "current loss at 53541 iteration 0.06085537157306203\n",
      "current loss at 53551 iteration 0.06085536038725999\n",
      "current loss at 53561 iteration 0.060855349205787404\n",
      "current loss at 53571 iteration 0.06085533802864175\n",
      "current loss at 53581 iteration 0.06085532685582054\n",
      "current loss at 53591 iteration 0.060855315687321276\n",
      "current loss at 53601 iteration 0.060855304523141474\n",
      "current loss at 53611 iteration 0.060855293363278634\n",
      "current loss at 53621 iteration 0.06085528220773026\n",
      "current loss at 53631 iteration 0.06085527105649388\n",
      "current loss at 53641 iteration 0.06085525990956701\n",
      "current loss at 53651 iteration 0.06085524876694716\n",
      "current loss at 53661 iteration 0.06085523762863184\n",
      "current loss at 53671 iteration 0.060855226494618594\n",
      "current loss at 53681 iteration 0.060855215364904916\n",
      "current loss at 53691 iteration 0.06085520423948834\n",
      "current loss at 53701 iteration 0.060855193118366416\n",
      "current loss at 53711 iteration 0.06085518200153664\n",
      "current loss at 53721 iteration 0.06085517088899655\n",
      "current loss at 53731 iteration 0.06085515978074369\n",
      "current loss at 53741 iteration 0.06085514867677558\n",
      "current loss at 53751 iteration 0.06085513757708976\n",
      "current loss at 53761 iteration 0.060855126481683786\n",
      "current loss at 53771 iteration 0.06085511539055515\n",
      "current loss at 53781 iteration 0.060855104303701446\n",
      "current loss at 53791 iteration 0.0608550932211202\n",
      "current loss at 53801 iteration 0.06085508214280894\n",
      "current loss at 53811 iteration 0.060855071068765226\n",
      "current loss at 53821 iteration 0.0608550599989866\n",
      "current loss at 53831 iteration 0.06085504893347063\n",
      "current loss at 53841 iteration 0.06085503787221485\n",
      "current loss at 53851 iteration 0.06085502681521682\n",
      "current loss at 53861 iteration 0.060855015762474096\n",
      "current loss at 53871 iteration 0.06085500471398423\n",
      "current loss at 53881 iteration 0.06085499366974481\n",
      "current loss at 53891 iteration 0.06085498262975337\n",
      "current loss at 53901 iteration 0.06085497159400747\n",
      "current loss at 53911 iteration 0.060854960562504695\n",
      "current loss at 53921 iteration 0.060854949535242606\n",
      "current loss at 53931 iteration 0.060854938512218765\n",
      "current loss at 53941 iteration 0.06085492749343075\n",
      "current loss at 53951 iteration 0.060854916478876135\n",
      "current loss at 53961 iteration 0.06085490546855248\n",
      "current loss at 53971 iteration 0.0608548944624574\n",
      "current loss at 53981 iteration 0.06085488346058841\n",
      "current loss at 53991 iteration 0.06085487246294316\n",
      "current loss at 54001 iteration 0.06085486146951918\n",
      "current loss at 54011 iteration 0.060854850480314075\n",
      "current loss at 54021 iteration 0.060854839495325455\n",
      "current loss at 54031 iteration 0.060854828514550846\n",
      "current loss at 54041 iteration 0.0608548175379879\n",
      "current loss at 54051 iteration 0.06085480656563416\n",
      "current loss at 54061 iteration 0.06085479559748725\n",
      "current loss at 54071 iteration 0.06085478463354477\n",
      "current loss at 54081 iteration 0.060854773673804284\n",
      "current loss at 54091 iteration 0.060854762718263414\n",
      "current loss at 54101 iteration 0.060854751766919755\n",
      "current loss at 54111 iteration 0.06085474081977091\n",
      "current loss at 54121 iteration 0.06085472987681449\n",
      "current loss at 54131 iteration 0.060854718938048086\n",
      "current loss at 54141 iteration 0.06085470800346932\n",
      "current loss at 54151 iteration 0.060854697073075795\n",
      "current loss at 54161 iteration 0.06085468614686513\n",
      "current loss at 54171 iteration 0.06085467522483491\n",
      "current loss at 54181 iteration 0.06085466430698278\n",
      "current loss at 54191 iteration 0.06085465339330636\n",
      "current loss at 54201 iteration 0.06085464248380325\n",
      "current loss at 54211 iteration 0.060854631578471076\n",
      "current loss at 54221 iteration 0.06085462067730745\n",
      "current loss at 54231 iteration 0.06085460978031002\n",
      "current loss at 54241 iteration 0.0608545988874764\n",
      "current loss at 54251 iteration 0.0608545879988042\n",
      "current loss at 54261 iteration 0.060854577114291084\n",
      "current loss at 54271 iteration 0.06085456623393467\n",
      "current loss at 54281 iteration 0.06085455535773258\n",
      "current loss at 54291 iteration 0.06085454448568244\n",
      "current loss at 54301 iteration 0.060854533617781925\n",
      "current loss at 54311 iteration 0.06085452275402863\n",
      "current loss at 54321 iteration 0.060854511894420225\n",
      "current loss at 54331 iteration 0.06085450103895434\n",
      "current loss at 54341 iteration 0.060854490187628625\n",
      "current loss at 54351 iteration 0.06085447934044071\n",
      "current loss at 54361 iteration 0.06085446849738826\n",
      "current loss at 54371 iteration 0.06085445765846891\n",
      "current loss at 54381 iteration 0.06085444682368032\n",
      "current loss at 54391 iteration 0.06085443599302016\n",
      "current loss at 54401 iteration 0.060854425166486044\n",
      "current loss at 54411 iteration 0.060854414344075645\n",
      "current loss at 54421 iteration 0.06085440352578664\n",
      "current loss at 54431 iteration 0.06085439271161667\n",
      "current loss at 54441 iteration 0.0608543819015634\n",
      "current loss at 54451 iteration 0.06085437109562448\n",
      "current loss at 54461 iteration 0.060854360293797614\n",
      "current loss at 54471 iteration 0.06085434949608042\n",
      "current loss at 54481 iteration 0.0608543387024706\n",
      "current loss at 54491 iteration 0.06085432791296582\n",
      "current loss at 54501 iteration 0.06085431712756374\n",
      "current loss at 54511 iteration 0.06085430634626204\n",
      "current loss at 54521 iteration 0.060854295569058386\n",
      "current loss at 54531 iteration 0.06085428479595048\n",
      "current loss at 54541 iteration 0.06085427402693597\n",
      "current loss at 54551 iteration 0.06085426326201255\n",
      "current loss at 54561 iteration 0.060854252501177916\n",
      "current loss at 54571 iteration 0.06085424174442974\n",
      "current loss at 54581 iteration 0.06085423099176571\n",
      "current loss at 54591 iteration 0.06085422024318351\n",
      "current loss at 54601 iteration 0.060854209498680825\n",
      "current loss at 54611 iteration 0.060854198758255346\n",
      "current loss at 54621 iteration 0.0608541880219048\n",
      "current loss at 54631 iteration 0.06085417728962683\n",
      "current loss at 54641 iteration 0.060854166561419176\n",
      "current loss at 54651 iteration 0.0608541558372795\n",
      "current loss at 54661 iteration 0.06085414511720552\n",
      "current loss at 54671 iteration 0.06085413440119495\n",
      "current loss at 54681 iteration 0.06085412368924546\n",
      "current loss at 54691 iteration 0.060854112981354805\n",
      "current loss at 54701 iteration 0.060854102277520625\n",
      "current loss at 54711 iteration 0.0608540915777407\n",
      "current loss at 54721 iteration 0.06085408088201268\n",
      "current loss at 54731 iteration 0.060854070190334314\n",
      "current loss at 54741 iteration 0.0608540595027033\n",
      "current loss at 54751 iteration 0.060854048819117344\n",
      "current loss at 54761 iteration 0.06085403813957419\n",
      "current loss at 54771 iteration 0.06085402746407155\n",
      "current loss at 54781 iteration 0.06085401679260713\n",
      "current loss at 54791 iteration 0.06085400612517865\n",
      "current loss at 54801 iteration 0.060853995461783855\n",
      "current loss at 54811 iteration 0.06085398480242046\n",
      "current loss at 54821 iteration 0.06085397414708618\n",
      "current loss at 54831 iteration 0.06085396349577876\n",
      "current loss at 54841 iteration 0.06085395284849593\n",
      "current loss at 54851 iteration 0.06085394220523542\n",
      "current loss at 54861 iteration 0.06085393156599496\n",
      "current loss at 54871 iteration 0.06085392093077228\n",
      "current loss at 54881 iteration 0.06085391029956514\n",
      "current loss at 54891 iteration 0.06085389967237125\n",
      "current loss at 54901 iteration 0.06085388904918838\n",
      "current loss at 54911 iteration 0.06085387843001425\n",
      "current loss at 54921 iteration 0.06085386781484661\n",
      "current loss at 54931 iteration 0.06085385720368319\n",
      "current loss at 54941 iteration 0.06085384659652179\n",
      "current loss at 54951 iteration 0.06085383599336011\n",
      "current loss at 54961 iteration 0.060853825394195926\n",
      "current loss at 54971 iteration 0.06085381479902696\n",
      "current loss at 54981 iteration 0.06085380420785101\n",
      "current loss at 54991 iteration 0.060853793620665814\n",
      "current loss at 55001 iteration 0.060853783037469106\n",
      "current loss at 55011 iteration 0.060853772458258674\n",
      "current loss at 55021 iteration 0.06085376188303225\n",
      "current loss at 55031 iteration 0.06085375131178764\n",
      "current loss at 55041 iteration 0.06085374074452258\n",
      "current loss at 55051 iteration 0.06085373018123483\n",
      "current loss at 55061 iteration 0.060853719621922174\n",
      "current loss at 55071 iteration 0.06085370906658237\n",
      "current loss at 55081 iteration 0.060853698515213196\n",
      "current loss at 55091 iteration 0.06085368796781243\n",
      "current loss at 55101 iteration 0.06085367742437783\n",
      "current loss at 55111 iteration 0.0608536668849072\n",
      "current loss at 55121 iteration 0.06085365634939827\n",
      "current loss at 55131 iteration 0.06085364581784888\n",
      "current loss at 55141 iteration 0.06085363529025675\n",
      "current loss at 55151 iteration 0.06085362476661971\n",
      "current loss at 55161 iteration 0.060853614246935535\n",
      "current loss at 55171 iteration 0.06085360373120198\n",
      "current loss at 55181 iteration 0.06085359321941687\n",
      "current loss at 55191 iteration 0.06085358271157797\n",
      "current loss at 55201 iteration 0.060853572207683095\n",
      "current loss at 55211 iteration 0.06085356170773001\n",
      "current loss at 55221 iteration 0.06085355121171651\n",
      "current loss at 55231 iteration 0.060853540719640414\n",
      "current loss at 55241 iteration 0.06085353023149952\n",
      "current loss at 55251 iteration 0.06085351974729161\n",
      "current loss at 55261 iteration 0.06085350926701448\n",
      "current loss at 55271 iteration 0.06085349879066594\n",
      "current loss at 55281 iteration 0.060853488318243804\n",
      "current loss at 55291 iteration 0.06085347784974585\n",
      "current loss at 55301 iteration 0.06085346738516993\n",
      "current loss at 55311 iteration 0.06085345692451383\n",
      "current loss at 55321 iteration 0.06085344646777534\n",
      "current loss at 55331 iteration 0.0608534360149523\n",
      "current loss at 55341 iteration 0.0608534255660425\n",
      "current loss at 55351 iteration 0.06085341512104378\n",
      "current loss at 55361 iteration 0.060853404679953954\n",
      "current loss at 55371 iteration 0.06085339424277082\n",
      "current loss at 55381 iteration 0.060853383809492206\n",
      "current loss at 55391 iteration 0.060853373380115955\n",
      "current loss at 55401 iteration 0.06085336295463987\n",
      "current loss at 55411 iteration 0.06085335253306176\n",
      "current loss at 55421 iteration 0.0608533421153795\n",
      "current loss at 55431 iteration 0.06085333170159088\n",
      "current loss at 55441 iteration 0.06085332129169374\n",
      "current loss at 55451 iteration 0.06085331088568589\n",
      "current loss at 55461 iteration 0.06085330048356522\n",
      "current loss at 55471 iteration 0.060853290085329505\n",
      "current loss at 55481 iteration 0.06085327969097662\n",
      "current loss at 55491 iteration 0.06085326930050439\n",
      "current loss at 55501 iteration 0.06085325891391064\n",
      "current loss at 55511 iteration 0.06085324853119323\n",
      "current loss at 55521 iteration 0.060853238152349984\n",
      "current loss at 55531 iteration 0.06085322777737877\n",
      "current loss at 55541 iteration 0.06085321740627741\n",
      "current loss at 55551 iteration 0.060853207039043795\n",
      "current loss at 55561 iteration 0.060853196675675715\n",
      "current loss at 55571 iteration 0.06085318631617106\n",
      "current loss at 55581 iteration 0.06085317596052767\n",
      "current loss at 55591 iteration 0.06085316560874338\n",
      "current loss at 55601 iteration 0.060853155260816104\n",
      "current loss at 55611 iteration 0.06085314491674364\n",
      "current loss at 55621 iteration 0.06085313457652386\n",
      "current loss at 55631 iteration 0.06085312424015463\n",
      "current loss at 55641 iteration 0.060853113907633816\n",
      "current loss at 55651 iteration 0.060853103578959296\n",
      "current loss at 55661 iteration 0.060853093254128895\n",
      "current loss at 55671 iteration 0.06085308293314051\n",
      "current loss at 55681 iteration 0.060853072615991995\n",
      "current loss at 55691 iteration 0.06085306230268122\n",
      "current loss at 55701 iteration 0.06085305199320607\n",
      "current loss at 55711 iteration 0.06085304168756441\n",
      "current loss at 55721 iteration 0.06085303138575412\n",
      "current loss at 55731 iteration 0.060853021087773046\n",
      "current loss at 55741 iteration 0.06085301079361911\n",
      "current loss at 55751 iteration 0.06085300050329016\n",
      "current loss at 55761 iteration 0.06085299021678409\n",
      "current loss at 55771 iteration 0.06085297993409877\n",
      "current loss at 55781 iteration 0.0608529696552321\n",
      "current loss at 55791 iteration 0.06085295938018195\n",
      "current loss at 55801 iteration 0.06085294910894622\n",
      "current loss at 55811 iteration 0.06085293884152279\n",
      "current loss at 55821 iteration 0.06085292857790955\n",
      "current loss at 55831 iteration 0.06085291831810438\n",
      "current loss at 55841 iteration 0.06085290806210521\n",
      "current loss at 55851 iteration 0.06085289780990989\n",
      "current loss at 55861 iteration 0.06085288756151635\n",
      "current loss at 55871 iteration 0.060852877316922474\n",
      "current loss at 55881 iteration 0.060852867076126166\n",
      "current loss at 55891 iteration 0.0608528568391253\n",
      "current loss at 55901 iteration 0.06085284660591783\n",
      "current loss at 55911 iteration 0.0608528363765016\n",
      "current loss at 55921 iteration 0.06085282615087457\n",
      "current loss at 55931 iteration 0.06085281592903461\n",
      "current loss at 55941 iteration 0.06085280571097965\n",
      "current loss at 55951 iteration 0.060852795496707586\n",
      "current loss at 55961 iteration 0.06085278528621633\n",
      "current loss at 55971 iteration 0.06085277507950382\n",
      "current loss at 55981 iteration 0.06085276487656791\n",
      "current loss at 55991 iteration 0.060852754677406584\n",
      "current loss at 56001 iteration 0.060852744482017745\n",
      "current loss at 56011 iteration 0.060852734290399266\n",
      "current loss at 56021 iteration 0.060852724102549126\n",
      "current loss at 56031 iteration 0.060852713918465216\n",
      "current loss at 56041 iteration 0.060852703738145456\n",
      "current loss at 56051 iteration 0.06085269356158778\n",
      "current loss at 56061 iteration 0.060852683388790116\n",
      "current loss at 56071 iteration 0.06085267321975041\n",
      "current loss at 56081 iteration 0.06085266305446656\n",
      "current loss at 56091 iteration 0.06085265289293651\n",
      "current loss at 56101 iteration 0.0608526427351582\n",
      "current loss at 56111 iteration 0.060852632581129544\n",
      "current loss at 56121 iteration 0.0608526224308485\n",
      "current loss at 56131 iteration 0.060852612284313\n",
      "current loss at 56141 iteration 0.06085260214152097\n",
      "current loss at 56151 iteration 0.06085259200247036\n",
      "current loss at 56161 iteration 0.06085258186715911\n",
      "current loss at 56171 iteration 0.06085257173558518\n",
      "current loss at 56181 iteration 0.060852561607746485\n",
      "current loss at 56191 iteration 0.06085255148364101\n",
      "current loss at 56201 iteration 0.06085254136326666\n",
      "current loss at 56211 iteration 0.060852531246621405\n",
      "current loss at 56221 iteration 0.06085252113370318\n",
      "current loss at 56231 iteration 0.060852511024509975\n",
      "current loss at 56241 iteration 0.06085250091903971\n",
      "current loss at 56251 iteration 0.060852490817290356\n",
      "current loss at 56261 iteration 0.06085248071925986\n",
      "current loss at 56271 iteration 0.06085247062494619\n",
      "current loss at 56281 iteration 0.06085246053434729\n",
      "current loss at 56291 iteration 0.060852450447461154\n",
      "current loss at 56301 iteration 0.060852440364285706\n",
      "current loss at 56311 iteration 0.06085243028481893\n",
      "current loss at 56321 iteration 0.06085242020905879\n",
      "current loss at 56331 iteration 0.06085241013700325\n",
      "current loss at 56341 iteration 0.060852400068650274\n",
      "current loss at 56351 iteration 0.060852390003997864\n",
      "current loss at 56361 iteration 0.06085237994304393\n",
      "current loss at 56371 iteration 0.06085236988578651\n",
      "current loss at 56381 iteration 0.06085235983222354\n",
      "current loss at 56391 iteration 0.06085234978235302\n",
      "current loss at 56401 iteration 0.060852339736172906\n",
      "current loss at 56411 iteration 0.06085232969368118\n",
      "current loss at 56421 iteration 0.060852319654875836\n",
      "current loss at 56431 iteration 0.060852309619754824\n",
      "current loss at 56441 iteration 0.06085229958831617\n",
      "current loss at 56451 iteration 0.06085228956055783\n",
      "current loss at 56461 iteration 0.06085227953647782\n",
      "current loss at 56471 iteration 0.060852269516074094\n",
      "current loss at 56481 iteration 0.06085225949934467\n",
      "current loss at 56491 iteration 0.06085224948628751\n",
      "current loss at 56501 iteration 0.060852239476900626\n",
      "current loss at 56511 iteration 0.060852229471182\n",
      "current loss at 56521 iteration 0.06085221946912964\n",
      "Traning is compelet after 56523 itration\n"
     ]
    }
   ],
   "source": [
    "ploss=0\n",
    "flag=0\n",
    "cov =1e-9\n",
    "for i in range(100000):\n",
    "    ycap = prediction(X.dot(W))\n",
    "    e = Y-ycap\n",
    "    closs = loss(Y,ycap)\n",
    "    if i%10==0:\n",
    "        print(\"current loss at\", i+1,\"iteration\", closs)\n",
    "    if abs(ploss-closs)<=cov:\n",
    "        print(\"Traning is compelet after\",i+1,\"itration\")\n",
    "        flag=1\n",
    "        break\n",
    "    delta = e*derivative(ycap)\n",
    "    W+= X.T.dot(delta)\n",
    "    ploss=closs\n",
    "if flag==0:\n",
    "    print(\"traning is not completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = prediction(X.dot(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.97691318e-01 3.61755011e-41 1.00357994e-02]\n",
      " [1.00939977e-06 7.41636224e-16 8.55468975e-01]\n",
      " [6.08721571e-05 3.70832236e-21 4.90637806e-01]\n",
      " [9.99997388e-01 1.46887635e-40 2.34808055e-03]\n",
      " [1.00000000e+00 4.96221481e-51 1.05775274e-04]\n",
      " [5.90865042e-06 1.75064761e-17 7.86053322e-01]\n",
      " [3.15193781e-03 5.17771712e-10 4.63398299e-01]\n",
      " [9.28038231e-06 9.82728309e-01 6.80192269e-01]\n",
      " [4.85436109e-03 9.75720174e-01 3.52080340e-03]\n",
      " [2.62306734e-08 2.20435258e-02 6.38576751e-01]\n",
      " [9.94707623e-01 2.24238454e-02 5.02205027e-04]\n",
      " [1.00000000e+00 8.56434275e-14 8.50655088e-06]]\n"
     ]
    }
   ],
   "source": [
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform probabilites into binary array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "probs[probs>0.5]=1\n",
    "probs[probs<0.5]=0\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert binary array into numerical label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-568eb4788489>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mYc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-568eb4788489>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mYc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "Yc = [int(np.where(arr==1)[0]) for arr in probs] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Yc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btn = np.where(probs==1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(btn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.c_[Y,Yc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y,ycap):\n",
    "    c= y==ycap\n",
    "    return c[c==True].size/y.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(y,Yc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
